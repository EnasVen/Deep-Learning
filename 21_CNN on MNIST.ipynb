{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST(TensorFlow版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.312980\n",
      "batch 1: loss 12.441854\n",
      "batch 2: loss 5.605434\n",
      "batch 3: loss 2.307137\n",
      "batch 4: loss 2.292812\n",
      "batch 5: loss 2.307375\n",
      "batch 6: loss 2.299499\n",
      "batch 7: loss 2.317246\n",
      "batch 8: loss 2.296153\n",
      "batch 9: loss 2.298928\n",
      "batch 10: loss 2.305989\n",
      "batch 11: loss 2.309182\n",
      "batch 12: loss 2.274323\n",
      "batch 13: loss 2.327966\n",
      "batch 14: loss 2.311240\n",
      "batch 15: loss 2.302894\n",
      "batch 16: loss 2.363893\n",
      "batch 17: loss 2.344068\n",
      "batch 18: loss 2.324596\n",
      "batch 19: loss 2.280692\n",
      "batch 20: loss 2.314594\n",
      "batch 21: loss 2.302614\n",
      "batch 22: loss 2.314267\n",
      "batch 23: loss 2.312832\n",
      "batch 24: loss 2.309758\n",
      "batch 25: loss 2.297209\n",
      "batch 26: loss 2.307161\n",
      "batch 27: loss 2.312736\n",
      "batch 28: loss 2.290920\n",
      "batch 29: loss 2.308201\n",
      "batch 30: loss 2.313276\n",
      "batch 31: loss 2.304058\n",
      "batch 32: loss 2.297906\n",
      "batch 33: loss 2.298638\n",
      "batch 34: loss 2.305308\n",
      "batch 35: loss 2.296111\n",
      "batch 36: loss 2.292198\n",
      "batch 37: loss 2.285738\n",
      "batch 38: loss 2.285405\n",
      "batch 39: loss 2.300639\n",
      "batch 40: loss 2.291445\n",
      "batch 41: loss 2.317694\n",
      "batch 42: loss 2.306731\n",
      "batch 43: loss 2.276161\n",
      "batch 44: loss 2.329943\n",
      "batch 45: loss 2.309300\n",
      "batch 46: loss 2.312361\n",
      "batch 47: loss 2.309477\n",
      "batch 48: loss 2.303117\n",
      "batch 49: loss 2.290031\n",
      "batch 50: loss 2.323225\n",
      "batch 51: loss 2.310289\n",
      "batch 52: loss 2.303426\n",
      "batch 53: loss 2.302315\n",
      "batch 54: loss 2.307976\n",
      "batch 55: loss 2.293922\n",
      "batch 56: loss 2.297741\n",
      "batch 57: loss 2.291784\n",
      "batch 58: loss 2.300416\n",
      "batch 59: loss 2.310629\n",
      "batch 60: loss 2.299554\n",
      "batch 61: loss 2.316117\n",
      "batch 62: loss 2.316935\n",
      "batch 63: loss 2.313802\n",
      "batch 64: loss 2.306867\n",
      "batch 65: loss 2.299617\n",
      "batch 66: loss 2.322095\n",
      "batch 67: loss 2.305069\n",
      "batch 68: loss 2.306059\n",
      "batch 69: loss 2.302706\n",
      "batch 70: loss 2.308499\n",
      "batch 71: loss 2.304944\n",
      "batch 72: loss 2.297441\n",
      "batch 73: loss 2.295722\n",
      "batch 74: loss 2.299531\n",
      "batch 75: loss 2.305383\n",
      "batch 76: loss 2.301666\n",
      "batch 77: loss 2.303212\n",
      "batch 78: loss 2.298977\n",
      "batch 79: loss 2.299922\n",
      "batch 80: loss 2.302927\n",
      "batch 81: loss 2.303492\n",
      "batch 82: loss 2.298108\n",
      "batch 83: loss 2.303713\n",
      "batch 84: loss 2.310200\n",
      "batch 85: loss 2.297397\n",
      "batch 86: loss 2.301272\n",
      "batch 87: loss 2.303908\n",
      "batch 88: loss 2.305336\n",
      "batch 89: loss 2.308717\n",
      "batch 90: loss 2.300179\n",
      "batch 91: loss 2.305808\n",
      "batch 92: loss 2.298272\n",
      "batch 93: loss 2.295306\n",
      "batch 94: loss 2.307023\n",
      "batch 95: loss 2.294516\n",
      "batch 96: loss 2.302215\n",
      "batch 97: loss 2.304726\n",
      "batch 98: loss 2.301990\n",
      "batch 99: loss 2.295769\n",
      "batch 100: loss 2.299854\n",
      "batch 101: loss 2.301362\n",
      "batch 102: loss 2.304882\n",
      "batch 103: loss 2.299863\n",
      "batch 104: loss 2.304961\n",
      "batch 105: loss 2.309092\n",
      "batch 106: loss 2.305359\n",
      "batch 107: loss 2.305783\n",
      "batch 108: loss 2.307328\n",
      "batch 109: loss 2.308967\n",
      "batch 110: loss 2.315386\n",
      "batch 111: loss 2.300891\n",
      "batch 112: loss 2.305646\n",
      "batch 113: loss 2.303387\n",
      "batch 114: loss 2.296829\n",
      "batch 115: loss 2.311593\n",
      "batch 116: loss 2.308835\n",
      "batch 117: loss 2.302377\n",
      "batch 118: loss 2.304834\n",
      "batch 119: loss 2.291067\n",
      "batch 120: loss 2.309782\n",
      "batch 121: loss 2.299187\n",
      "batch 122: loss 2.299280\n",
      "batch 123: loss 2.300652\n",
      "batch 124: loss 2.291909\n",
      "batch 125: loss 2.320670\n",
      "batch 126: loss 2.297369\n",
      "batch 127: loss 2.301614\n",
      "batch 128: loss 2.295392\n",
      "batch 129: loss 2.305930\n",
      "batch 130: loss 2.299527\n",
      "batch 131: loss 2.295180\n",
      "batch 132: loss 2.302600\n",
      "batch 133: loss 2.304085\n",
      "batch 134: loss 2.297854\n",
      "batch 135: loss 2.284113\n",
      "batch 136: loss 2.297016\n",
      "batch 137: loss 2.306459\n",
      "batch 138: loss 2.299373\n",
      "batch 139: loss 2.312073\n",
      "batch 140: loss 2.293355\n",
      "batch 141: loss 2.293414\n",
      "batch 142: loss 2.291337\n",
      "batch 143: loss 2.312095\n",
      "batch 144: loss 2.294297\n",
      "batch 145: loss 2.309313\n",
      "batch 146: loss 2.300307\n",
      "batch 147: loss 2.305880\n",
      "batch 148: loss 2.325471\n",
      "batch 149: loss 2.303296\n",
      "batch 150: loss 2.305418\n",
      "batch 151: loss 2.291428\n",
      "batch 152: loss 2.288856\n",
      "batch 153: loss 2.288142\n",
      "batch 154: loss 2.315910\n",
      "batch 155: loss 2.304847\n",
      "batch 156: loss 2.285178\n",
      "batch 157: loss 2.306832\n",
      "batch 158: loss 2.310667\n",
      "batch 159: loss 2.323218\n",
      "batch 160: loss 2.305048\n",
      "batch 161: loss 2.307680\n",
      "batch 162: loss 2.288072\n",
      "batch 163: loss 2.299015\n",
      "batch 164: loss 2.315712\n",
      "batch 165: loss 2.298968\n",
      "batch 166: loss 2.310143\n",
      "batch 167: loss 2.298783\n",
      "batch 168: loss 2.303083\n",
      "batch 169: loss 2.299857\n",
      "batch 170: loss 2.299606\n",
      "batch 171: loss 2.292628\n",
      "batch 172: loss 2.299695\n",
      "batch 173: loss 2.297704\n",
      "batch 174: loss 2.307351\n",
      "batch 175: loss 2.292326\n",
      "batch 176: loss 2.292367\n",
      "batch 177: loss 2.306986\n",
      "batch 178: loss 2.304901\n",
      "batch 179: loss 2.291414\n",
      "batch 180: loss 2.295612\n",
      "batch 181: loss 2.298955\n",
      "batch 182: loss 2.301819\n",
      "batch 183: loss 2.296658\n",
      "batch 184: loss 2.289668\n",
      "batch 185: loss 2.317429\n",
      "batch 186: loss 2.283540\n",
      "batch 187: loss 2.292244\n",
      "batch 188: loss 2.285416\n",
      "batch 189: loss 2.294504\n",
      "batch 190: loss 2.322170\n",
      "batch 191: loss 2.305131\n",
      "batch 192: loss 2.295902\n",
      "batch 193: loss 2.310687\n",
      "batch 194: loss 2.298445\n",
      "batch 195: loss 2.292259\n",
      "batch 196: loss 2.305356\n",
      "batch 197: loss 2.301811\n",
      "batch 198: loss 2.307997\n",
      "batch 199: loss 2.313418\n",
      "batch 200: loss 2.288825\n",
      "batch 201: loss 2.321335\n",
      "batch 202: loss 2.319701\n",
      "batch 203: loss 2.308904\n",
      "batch 204: loss 2.283419\n",
      "batch 205: loss 2.310907\n",
      "batch 206: loss 2.309361\n",
      "batch 207: loss 2.297234\n",
      "batch 208: loss 2.304929\n",
      "batch 209: loss 2.295972\n",
      "batch 210: loss 2.296431\n",
      "batch 211: loss 2.317867\n",
      "batch 212: loss 2.309300\n",
      "batch 213: loss 2.283910\n",
      "batch 214: loss 2.311985\n",
      "batch 215: loss 2.309534\n",
      "batch 216: loss 2.309859\n",
      "batch 217: loss 2.282799\n",
      "batch 218: loss 2.308031\n",
      "batch 219: loss 2.298035\n",
      "batch 220: loss 2.296925\n",
      "batch 221: loss 2.301481\n",
      "batch 222: loss 2.292596\n",
      "batch 223: loss 2.302648\n",
      "batch 224: loss 2.300972\n",
      "batch 225: loss 2.319914\n",
      "batch 226: loss 2.309353\n",
      "batch 227: loss 2.298789\n",
      "batch 228: loss 2.298534\n",
      "batch 229: loss 2.296388\n",
      "batch 230: loss 2.305971\n",
      "batch 231: loss 2.311453\n",
      "batch 232: loss 2.297333\n",
      "batch 233: loss 2.302536\n",
      "batch 234: loss 2.296058\n",
      "batch 235: loss 2.283489\n",
      "batch 236: loss 2.320006\n",
      "batch 237: loss 2.315431\n",
      "batch 238: loss 2.295209\n",
      "batch 239: loss 2.289499\n",
      "batch 240: loss 2.306756\n",
      "batch 241: loss 2.309761\n",
      "batch 242: loss 2.294932\n",
      "batch 243: loss 2.306973\n",
      "batch 244: loss 2.308418\n",
      "batch 245: loss 2.304160\n",
      "batch 246: loss 2.301563\n",
      "batch 247: loss 2.311930\n",
      "batch 248: loss 2.304984\n",
      "batch 249: loss 2.296987\n",
      "batch 250: loss 2.297462\n",
      "batch 251: loss 2.301839\n",
      "batch 252: loss 2.298224\n",
      "batch 253: loss 2.311708\n",
      "batch 254: loss 2.308213\n",
      "batch 255: loss 2.306679\n",
      "batch 256: loss 2.311512\n",
      "batch 257: loss 2.293751\n",
      "batch 258: loss 2.304234\n",
      "batch 259: loss 2.309269\n",
      "batch 260: loss 2.318134\n",
      "batch 261: loss 2.302820\n",
      "batch 262: loss 2.314065\n",
      "batch 263: loss 2.302122\n",
      "batch 264: loss 2.297339\n",
      "batch 265: loss 2.294262\n",
      "batch 266: loss 2.310872\n",
      "batch 267: loss 2.311677\n",
      "batch 268: loss 2.296005\n",
      "batch 269: loss 2.304468\n",
      "batch 270: loss 2.310769\n",
      "batch 271: loss 2.322211\n",
      "batch 272: loss 2.299620\n",
      "batch 273: loss 2.288793\n",
      "batch 274: loss 2.316302\n",
      "batch 275: loss 2.305999\n",
      "batch 276: loss 2.302409\n",
      "batch 277: loss 2.301422\n",
      "batch 278: loss 2.293670\n",
      "batch 279: loss 2.306801\n",
      "batch 280: loss 2.307875\n",
      "batch 281: loss 2.304977\n",
      "batch 282: loss 2.307963\n",
      "batch 283: loss 2.306922\n",
      "batch 284: loss 2.295541\n",
      "batch 285: loss 2.299482\n",
      "batch 286: loss 2.296751\n",
      "batch 287: loss 2.299964\n",
      "batch 288: loss 2.291733\n",
      "batch 289: loss 2.313586\n",
      "batch 290: loss 2.287252\n",
      "batch 291: loss 2.307227\n",
      "batch 292: loss 2.318588\n",
      "batch 293: loss 2.307373\n",
      "batch 294: loss 2.293203\n",
      "batch 295: loss 2.308066\n",
      "batch 296: loss 2.291967\n",
      "batch 297: loss 2.300273\n",
      "batch 298: loss 2.316540\n",
      "batch 299: loss 2.308940\n",
      "batch 300: loss 2.296430\n",
      "batch 301: loss 2.286235\n",
      "batch 302: loss 2.307875\n",
      "batch 303: loss 2.297568\n",
      "batch 304: loss 2.295955\n",
      "batch 305: loss 2.303302\n",
      "batch 306: loss 2.304977\n",
      "batch 307: loss 2.300101\n",
      "batch 308: loss 2.307347\n",
      "batch 309: loss 2.300424\n",
      "batch 310: loss 2.305251\n",
      "batch 311: loss 2.303766\n",
      "batch 312: loss 2.304010\n",
      "batch 313: loss 2.300865\n",
      "batch 314: loss 2.295071\n",
      "batch 315: loss 2.310267\n",
      "batch 316: loss 2.304361\n",
      "batch 317: loss 2.296633\n",
      "batch 318: loss 2.294576\n",
      "batch 319: loss 2.294733\n",
      "batch 320: loss 2.298561\n",
      "batch 321: loss 2.299448\n",
      "batch 322: loss 2.292282\n",
      "batch 323: loss 2.305519\n",
      "batch 324: loss 2.286574\n",
      "batch 325: loss 2.295935\n",
      "batch 326: loss 2.310081\n",
      "batch 327: loss 2.305185\n",
      "batch 328: loss 2.303969\n",
      "batch 329: loss 2.313752\n",
      "batch 330: loss 2.301220\n",
      "batch 331: loss 2.309209\n",
      "batch 332: loss 2.296794\n",
      "batch 333: loss 2.315977\n",
      "batch 334: loss 2.309579\n",
      "batch 335: loss 2.322003\n",
      "batch 336: loss 2.315553\n",
      "batch 337: loss 2.294319\n",
      "batch 338: loss 2.308572\n",
      "batch 339: loss 2.302259\n",
      "batch 340: loss 2.284498\n",
      "batch 341: loss 2.297341\n",
      "batch 342: loss 2.324283\n",
      "batch 343: loss 2.314473\n",
      "batch 344: loss 2.297507\n",
      "batch 345: loss 2.290338\n",
      "batch 346: loss 2.303773\n",
      "batch 347: loss 2.312750\n",
      "batch 348: loss 2.308553\n",
      "batch 349: loss 2.300876\n",
      "batch 350: loss 2.302845\n",
      "batch 351: loss 2.309248\n",
      "batch 352: loss 2.302601\n",
      "batch 353: loss 2.300030\n",
      "batch 354: loss 2.307136\n",
      "batch 355: loss 2.304498\n",
      "batch 356: loss 2.310578\n",
      "batch 357: loss 2.298499\n",
      "batch 358: loss 2.305234\n",
      "batch 359: loss 2.306706\n",
      "batch 360: loss 2.300898\n",
      "batch 361: loss 2.295490\n",
      "batch 362: loss 2.301708\n",
      "batch 363: loss 2.302393\n",
      "batch 364: loss 2.310124\n",
      "batch 365: loss 2.301769\n",
      "batch 366: loss 2.303477\n",
      "batch 367: loss 2.308356\n",
      "batch 368: loss 2.298510\n",
      "batch 369: loss 2.299802\n",
      "batch 370: loss 2.309816\n",
      "batch 371: loss 2.299066\n",
      "batch 372: loss 2.302305\n",
      "batch 373: loss 2.292070\n",
      "batch 374: loss 2.297529\n",
      "batch 375: loss 2.300979\n",
      "batch 376: loss 2.298817\n",
      "batch 377: loss 2.299667\n",
      "batch 378: loss 2.291584\n",
      "batch 379: loss 2.313475\n",
      "batch 380: loss 2.294330\n",
      "batch 381: loss 2.302056\n",
      "batch 382: loss 2.307729\n",
      "batch 383: loss 2.303367\n",
      "batch 384: loss 2.305925\n",
      "batch 385: loss 2.300429\n",
      "batch 386: loss 2.294600\n",
      "batch 387: loss 2.298691\n",
      "batch 388: loss 2.304230\n",
      "batch 389: loss 2.298690\n",
      "batch 390: loss 2.290263\n",
      "batch 391: loss 2.312658\n",
      "batch 392: loss 2.302597\n",
      "batch 393: loss 2.301983\n",
      "batch 394: loss 2.307370\n",
      "batch 395: loss 2.298152\n",
      "batch 396: loss 2.298625\n",
      "batch 397: loss 2.306152\n",
      "batch 398: loss 2.301820\n",
      "batch 399: loss 2.293555\n",
      "batch 400: loss 2.291914\n",
      "batch 401: loss 2.306906\n",
      "batch 402: loss 2.289866\n",
      "batch 403: loss 2.293116\n",
      "batch 404: loss 2.318987\n",
      "batch 405: loss 2.305457\n",
      "batch 406: loss 2.298863\n",
      "batch 407: loss 2.292717\n",
      "batch 408: loss 2.306201\n",
      "batch 409: loss 2.304414\n",
      "batch 410: loss 2.292717\n",
      "batch 411: loss 2.314837\n",
      "batch 412: loss 2.299271\n",
      "batch 413: loss 2.297581\n",
      "batch 414: loss 2.301312\n",
      "batch 415: loss 2.313420\n",
      "batch 416: loss 2.309795\n",
      "batch 417: loss 2.286598\n",
      "batch 418: loss 2.316860\n",
      "batch 419: loss 2.321016\n",
      "batch 420: loss 2.301906\n",
      "batch 421: loss 2.296049\n",
      "batch 422: loss 2.301403\n",
      "batch 423: loss 2.304435\n",
      "batch 424: loss 2.303253\n",
      "batch 425: loss 2.296172\n",
      "batch 426: loss 2.310115\n",
      "batch 427: loss 2.294479\n",
      "batch 428: loss 2.304023\n",
      "batch 429: loss 2.275414\n",
      "batch 430: loss 2.296888\n",
      "batch 431: loss 2.305108\n",
      "batch 432: loss 2.291352\n",
      "batch 433: loss 2.302403\n",
      "batch 434: loss 2.304819\n",
      "batch 435: loss 2.308239\n",
      "batch 436: loss 2.297821\n",
      "batch 437: loss 2.307436\n",
      "batch 438: loss 2.302672\n",
      "batch 439: loss 2.318448\n",
      "batch 440: loss 2.308476\n",
      "batch 441: loss 2.293464\n",
      "batch 442: loss 2.299382\n",
      "batch 443: loss 2.315926\n",
      "batch 444: loss 2.290827\n",
      "batch 445: loss 2.314186\n",
      "batch 446: loss 2.309305\n",
      "batch 447: loss 2.275569\n",
      "batch 448: loss 2.287336\n",
      "batch 449: loss 2.284384\n",
      "batch 450: loss 2.291741\n",
      "batch 451: loss 2.313127\n",
      "batch 452: loss 2.292977\n",
      "batch 453: loss 2.286433\n",
      "batch 454: loss 2.313577\n",
      "batch 455: loss 2.308387\n",
      "batch 456: loss 2.287376\n",
      "batch 457: loss 2.304662\n",
      "batch 458: loss 2.307337\n",
      "batch 459: loss 2.305840\n",
      "batch 460: loss 2.278128\n",
      "batch 461: loss 2.312000\n",
      "batch 462: loss 2.306089\n",
      "batch 463: loss 2.306806\n",
      "batch 464: loss 2.303098\n",
      "batch 465: loss 2.299587\n",
      "batch 466: loss 2.293358\n",
      "batch 467: loss 2.283858\n",
      "batch 468: loss 2.305528\n",
      "batch 469: loss 2.294797\n",
      "batch 470: loss 2.301719\n",
      "batch 471: loss 2.298985\n",
      "batch 472: loss 2.299882\n",
      "batch 473: loss 2.299144\n",
      "batch 474: loss 2.283900\n",
      "batch 475: loss 2.316905\n",
      "batch 476: loss 2.315651\n",
      "batch 477: loss 2.286937\n",
      "batch 478: loss 2.301247\n",
      "batch 479: loss 2.292922\n",
      "batch 480: loss 2.294607\n",
      "batch 481: loss 2.276523\n",
      "batch 482: loss 2.292431\n",
      "batch 483: loss 2.273718\n",
      "batch 484: loss 2.312727\n",
      "batch 485: loss 2.332089\n",
      "batch 486: loss 2.300024\n",
      "batch 487: loss 2.317966\n",
      "batch 488: loss 2.297012\n",
      "batch 489: loss 2.310791\n",
      "batch 490: loss 2.290639\n",
      "batch 491: loss 2.293727\n",
      "batch 492: loss 2.290831\n",
      "batch 493: loss 2.304087\n",
      "batch 494: loss 2.299685\n",
      "batch 495: loss 2.298996\n",
      "batch 496: loss 2.288972\n",
      "batch 497: loss 2.333405\n",
      "batch 498: loss 2.316505\n",
      "batch 499: loss 2.293403\n",
      "batch 500: loss 2.279379\n",
      "batch 501: loss 2.287112\n",
      "batch 502: loss 2.321217\n",
      "batch 503: loss 2.290785\n",
      "batch 504: loss 2.317693\n",
      "batch 505: loss 2.292289\n",
      "batch 506: loss 2.305763\n",
      "batch 507: loss 2.296348\n",
      "batch 508: loss 2.307533\n",
      "batch 509: loss 2.291735\n",
      "batch 510: loss 2.286523\n",
      "batch 511: loss 2.304870\n",
      "batch 512: loss 2.299444\n",
      "batch 513: loss 2.296277\n",
      "batch 514: loss 2.298937\n",
      "batch 515: loss 2.341373\n",
      "batch 516: loss 2.287077\n",
      "batch 517: loss 2.295208\n",
      "batch 518: loss 2.286127\n",
      "batch 519: loss 2.296496\n",
      "batch 520: loss 2.317285\n",
      "batch 521: loss 2.298450\n",
      "batch 522: loss 2.288258\n",
      "batch 523: loss 2.277328\n",
      "batch 524: loss 2.304944\n",
      "batch 525: loss 2.306382\n",
      "batch 526: loss 2.299947\n",
      "batch 527: loss 2.302048\n",
      "batch 528: loss 2.324052\n",
      "batch 529: loss 2.294983\n",
      "batch 530: loss 2.306356\n",
      "batch 531: loss 2.318120\n",
      "batch 532: loss 2.321631\n",
      "batch 533: loss 2.299253\n",
      "batch 534: loss 2.311845\n",
      "batch 535: loss 2.313013\n",
      "batch 536: loss 2.301862\n",
      "batch 537: loss 2.298818\n",
      "batch 538: loss 2.288306\n",
      "batch 539: loss 2.278971\n",
      "batch 540: loss 2.305758\n",
      "batch 541: loss 2.306253\n",
      "batch 542: loss 2.283187\n",
      "batch 543: loss 2.318163\n",
      "batch 544: loss 2.303666\n",
      "batch 545: loss 2.281474\n",
      "batch 546: loss 2.321902\n",
      "batch 547: loss 2.291520\n",
      "batch 548: loss 2.315280\n",
      "batch 549: loss 2.294605\n",
      "batch 550: loss 2.304946\n",
      "batch 551: loss 2.300733\n",
      "batch 552: loss 2.288808\n",
      "batch 553: loss 2.284348\n",
      "batch 554: loss 2.292161\n",
      "batch 555: loss 2.305640\n",
      "batch 556: loss 2.287883\n",
      "batch 557: loss 2.300918\n",
      "batch 558: loss 2.293036\n",
      "batch 559: loss 2.302007\n",
      "batch 560: loss 2.286830\n",
      "batch 561: loss 2.280209\n",
      "batch 562: loss 2.305726\n",
      "batch 563: loss 2.327013\n",
      "batch 564: loss 2.313254\n",
      "batch 565: loss 2.289547\n",
      "batch 566: loss 2.297298\n",
      "batch 567: loss 2.294246\n",
      "batch 568: loss 2.296178\n",
      "batch 569: loss 2.283529\n",
      "batch 570: loss 2.312197\n",
      "batch 571: loss 2.310184\n",
      "batch 572: loss 2.307153\n",
      "batch 573: loss 2.271825\n",
      "batch 574: loss 2.308918\n",
      "batch 575: loss 2.296367\n",
      "batch 576: loss 2.331522\n",
      "batch 577: loss 2.283574\n",
      "batch 578: loss 2.329556\n",
      "batch 579: loss 2.348409\n",
      "batch 580: loss 2.329696\n",
      "batch 581: loss 2.286898\n",
      "batch 582: loss 2.304707\n",
      "batch 583: loss 2.288983\n",
      "batch 584: loss 2.293747\n",
      "batch 585: loss 2.323231\n",
      "batch 586: loss 2.317573\n",
      "batch 587: loss 2.306195\n",
      "batch 588: loss 2.294637\n",
      "batch 589: loss 2.289021\n",
      "batch 590: loss 2.312696\n",
      "batch 591: loss 2.311783\n",
      "batch 592: loss 2.303348\n",
      "batch 593: loss 2.307370\n",
      "batch 594: loss 2.313901\n",
      "batch 595: loss 2.333780\n",
      "batch 596: loss 2.296657\n",
      "batch 597: loss 2.310238\n",
      "batch 598: loss 2.312495\n",
      "batch 599: loss 2.295850\n",
      "batch 600: loss 2.300636\n",
      "batch 601: loss 2.289665\n",
      "batch 602: loss 2.299050\n",
      "batch 603: loss 2.314781\n",
      "batch 604: loss 2.305248\n",
      "batch 605: loss 2.300595\n",
      "batch 606: loss 2.302252\n",
      "batch 607: loss 2.320987\n",
      "batch 608: loss 2.288546\n",
      "batch 609: loss 2.313128\n",
      "batch 610: loss 2.303492\n",
      "batch 611: loss 2.304485\n",
      "batch 612: loss 2.291574\n",
      "batch 613: loss 2.304497\n",
      "batch 614: loss 2.294375\n",
      "batch 615: loss 2.292773\n",
      "batch 616: loss 2.295233\n",
      "batch 617: loss 2.300717\n",
      "batch 618: loss 2.309467\n",
      "batch 619: loss 2.292352\n",
      "batch 620: loss 2.289098\n",
      "batch 621: loss 2.307828\n",
      "batch 622: loss 2.309012\n",
      "batch 623: loss 2.291737\n",
      "batch 624: loss 2.309075\n",
      "batch 625: loss 2.312351\n",
      "batch 626: loss 2.287460\n",
      "batch 627: loss 2.290618\n",
      "batch 628: loss 2.288367\n",
      "batch 629: loss 2.289990\n",
      "batch 630: loss 2.302076\n",
      "batch 631: loss 2.299337\n",
      "batch 632: loss 2.285856\n",
      "batch 633: loss 2.309459\n",
      "batch 634: loss 2.306962\n",
      "batch 635: loss 2.300778\n",
      "batch 636: loss 2.297776\n",
      "batch 637: loss 2.296327\n",
      "batch 638: loss 2.305801\n",
      "batch 639: loss 2.303397\n",
      "batch 640: loss 2.282643\n",
      "batch 641: loss 2.302672\n",
      "batch 642: loss 2.282104\n",
      "batch 643: loss 2.313255\n",
      "batch 644: loss 2.311167\n",
      "batch 645: loss 2.306239\n",
      "batch 646: loss 2.313182\n",
      "batch 647: loss 2.289885\n",
      "batch 648: loss 2.297033\n",
      "batch 649: loss 2.301013\n",
      "batch 650: loss 2.279388\n",
      "batch 651: loss 2.300887\n",
      "batch 652: loss 2.325984\n",
      "batch 653: loss 2.319173\n",
      "batch 654: loss 2.305814\n",
      "batch 655: loss 2.287040\n",
      "batch 656: loss 2.321102\n",
      "batch 657: loss 2.304490\n",
      "batch 658: loss 2.285171\n",
      "batch 659: loss 2.298962\n",
      "batch 660: loss 2.312963\n",
      "batch 661: loss 2.318166\n",
      "batch 662: loss 2.338758\n",
      "batch 663: loss 2.312925\n",
      "batch 664: loss 2.287487\n",
      "batch 665: loss 2.294291\n",
      "batch 666: loss 2.291855\n",
      "batch 667: loss 2.296386\n",
      "batch 668: loss 2.314549\n",
      "batch 669: loss 2.300035\n",
      "batch 670: loss 2.295532\n",
      "batch 671: loss 2.298466\n",
      "batch 672: loss 2.308077\n",
      "batch 673: loss 2.306504\n",
      "batch 674: loss 2.300860\n",
      "batch 675: loss 2.307540\n",
      "batch 676: loss 2.298017\n",
      "batch 677: loss 2.311444\n",
      "batch 678: loss 2.314451\n",
      "batch 679: loss 2.292100\n",
      "batch 680: loss 2.304700\n",
      "batch 681: loss 2.318786\n",
      "batch 682: loss 2.316450\n",
      "batch 683: loss 2.300984\n",
      "batch 684: loss 2.301202\n",
      "batch 685: loss 2.295706\n",
      "batch 686: loss 2.309391\n",
      "batch 687: loss 2.320556\n",
      "batch 688: loss 2.303075\n",
      "batch 689: loss 2.301282\n",
      "batch 690: loss 2.280772\n",
      "batch 691: loss 2.292425\n",
      "batch 692: loss 2.307338\n",
      "batch 693: loss 2.298398\n",
      "batch 694: loss 2.299164\n",
      "batch 695: loss 2.304988\n",
      "batch 696: loss 2.312587\n",
      "batch 697: loss 2.302540\n",
      "batch 698: loss 2.298120\n",
      "batch 699: loss 2.290573\n",
      "batch 700: loss 2.301218\n",
      "batch 701: loss 2.311639\n",
      "batch 702: loss 2.308314\n",
      "batch 703: loss 2.315352\n",
      "batch 704: loss 2.304588\n",
      "batch 705: loss 2.301246\n",
      "batch 706: loss 2.293172\n",
      "batch 707: loss 2.290246\n",
      "batch 708: loss 2.300941\n",
      "batch 709: loss 2.299899\n",
      "batch 710: loss 2.274630\n",
      "batch 711: loss 2.314858\n",
      "batch 712: loss 2.299076\n",
      "batch 713: loss 2.314698\n",
      "batch 714: loss 2.289633\n",
      "batch 715: loss 2.299014\n",
      "batch 716: loss 2.324531\n",
      "batch 717: loss 2.299228\n",
      "batch 718: loss 2.313820\n",
      "batch 719: loss 2.300697\n",
      "batch 720: loss 2.303898\n",
      "batch 721: loss 2.295368\n",
      "batch 722: loss 2.291738\n",
      "batch 723: loss 2.311138\n",
      "batch 724: loss 2.303663\n",
      "batch 725: loss 2.295533\n",
      "batch 726: loss 2.313463\n",
      "batch 727: loss 2.321063\n",
      "batch 728: loss 2.321293\n",
      "batch 729: loss 2.314440\n",
      "batch 730: loss 2.291093\n",
      "batch 731: loss 2.284374\n",
      "batch 732: loss 2.297363\n",
      "batch 733: loss 2.290864\n",
      "batch 734: loss 2.301726\n",
      "batch 735: loss 2.304339\n",
      "batch 736: loss 2.311492\n",
      "batch 737: loss 2.291418\n",
      "batch 738: loss 2.311560\n",
      "batch 739: loss 2.311431\n",
      "batch 740: loss 2.303343\n",
      "batch 741: loss 2.296249\n",
      "batch 742: loss 2.299887\n",
      "batch 743: loss 2.301822\n",
      "batch 744: loss 2.309049\n",
      "batch 745: loss 2.300383\n",
      "batch 746: loss 2.302952\n",
      "batch 747: loss 2.309108\n",
      "batch 748: loss 2.316010\n",
      "batch 749: loss 2.294509\n",
      "batch 750: loss 2.294666\n",
      "batch 751: loss 2.303893\n",
      "batch 752: loss 2.306971\n",
      "batch 753: loss 2.307380\n",
      "batch 754: loss 2.289113\n",
      "batch 755: loss 2.302035\n",
      "batch 756: loss 2.308754\n",
      "batch 757: loss 2.304632\n",
      "batch 758: loss 2.307197\n",
      "batch 759: loss 2.303319\n",
      "batch 760: loss 2.318851\n",
      "batch 761: loss 2.300624\n",
      "batch 762: loss 2.283449\n",
      "batch 763: loss 2.302390\n",
      "batch 764: loss 2.299086\n",
      "batch 765: loss 2.301418\n",
      "batch 766: loss 2.305503\n",
      "batch 767: loss 2.300480\n",
      "batch 768: loss 2.303719\n",
      "batch 769: loss 2.301764\n",
      "batch 770: loss 2.307247\n",
      "batch 771: loss 2.302901\n",
      "batch 772: loss 2.293614\n",
      "batch 773: loss 2.297845\n",
      "batch 774: loss 2.293813\n",
      "batch 775: loss 2.292520\n",
      "batch 776: loss 2.302077\n",
      "batch 777: loss 2.300460\n",
      "batch 778: loss 2.306195\n",
      "batch 779: loss 2.286742\n",
      "batch 780: loss 2.317235\n",
      "batch 781: loss 2.313005\n",
      "batch 782: loss 2.309866\n",
      "batch 783: loss 2.297781\n",
      "batch 784: loss 2.300812\n",
      "batch 785: loss 2.284685\n",
      "batch 786: loss 2.301766\n",
      "batch 787: loss 2.294505\n",
      "batch 788: loss 2.306428\n",
      "batch 789: loss 2.305183\n",
      "batch 790: loss 2.302807\n",
      "batch 791: loss 2.301749\n",
      "batch 792: loss 2.288982\n",
      "batch 793: loss 2.298622\n",
      "batch 794: loss 2.318089\n",
      "batch 795: loss 2.287694\n",
      "batch 796: loss 2.318259\n",
      "batch 797: loss 2.295841\n",
      "batch 798: loss 2.296351\n",
      "batch 799: loss 2.299101\n",
      "batch 800: loss 2.301458\n",
      "batch 801: loss 2.285412\n",
      "batch 802: loss 2.296694\n",
      "batch 803: loss 2.305308\n",
      "batch 804: loss 2.302606\n",
      "batch 805: loss 2.322654\n",
      "batch 806: loss 2.303384\n",
      "batch 807: loss 2.303428\n",
      "batch 808: loss 2.315891\n",
      "batch 809: loss 2.316573\n",
      "batch 810: loss 2.323342\n",
      "batch 811: loss 2.294986\n",
      "batch 812: loss 2.280875\n",
      "batch 813: loss 2.303458\n",
      "batch 814: loss 2.320176\n",
      "batch 815: loss 2.295216\n",
      "batch 816: loss 2.290016\n",
      "batch 817: loss 2.289257\n",
      "batch 818: loss 2.290414\n",
      "batch 819: loss 2.297154\n",
      "batch 820: loss 2.301280\n",
      "batch 821: loss 2.300868\n",
      "batch 822: loss 2.300613\n",
      "batch 823: loss 2.300783\n",
      "batch 824: loss 2.305656\n",
      "batch 825: loss 2.303106\n",
      "batch 826: loss 2.305197\n",
      "batch 827: loss 2.302595\n",
      "batch 828: loss 2.306258\n",
      "batch 829: loss 2.303472\n",
      "batch 830: loss 2.298109\n",
      "batch 831: loss 2.293009\n",
      "batch 832: loss 2.308590\n",
      "batch 833: loss 2.299545\n",
      "batch 834: loss 2.298953\n",
      "batch 835: loss 2.298219\n",
      "batch 836: loss 2.326753\n",
      "batch 837: loss 2.312054\n",
      "batch 838: loss 2.305625\n",
      "batch 839: loss 2.294987\n",
      "batch 840: loss 2.312617\n",
      "batch 841: loss 2.303947\n",
      "batch 842: loss 2.306223\n",
      "batch 843: loss 2.313027\n",
      "batch 844: loss 2.306810\n",
      "batch 845: loss 2.314130\n",
      "batch 846: loss 2.289194\n",
      "batch 847: loss 2.294010\n",
      "batch 848: loss 2.306366\n",
      "batch 849: loss 2.308691\n",
      "batch 850: loss 2.305936\n",
      "batch 851: loss 2.294657\n",
      "batch 852: loss 2.297183\n",
      "batch 853: loss 2.306351\n",
      "batch 854: loss 2.304343\n",
      "batch 855: loss 2.294353\n",
      "batch 856: loss 2.301296\n",
      "batch 857: loss 2.299631\n",
      "batch 858: loss 2.298993\n",
      "batch 859: loss 2.310473\n",
      "batch 860: loss 2.296123\n",
      "batch 861: loss 2.313912\n",
      "batch 862: loss 2.314430\n",
      "batch 863: loss 2.310113\n",
      "batch 864: loss 2.293295\n",
      "batch 865: loss 2.311175\n",
      "batch 866: loss 2.299634\n",
      "batch 867: loss 2.296579\n",
      "batch 868: loss 2.288723\n",
      "batch 869: loss 2.294167\n",
      "batch 870: loss 2.298327\n",
      "batch 871: loss 2.295200\n",
      "batch 872: loss 2.297170\n",
      "batch 873: loss 2.297784\n",
      "batch 874: loss 2.315613\n",
      "batch 875: loss 2.311520\n",
      "batch 876: loss 2.310759\n",
      "batch 877: loss 2.294184\n",
      "batch 878: loss 2.307653\n",
      "batch 879: loss 2.307303\n",
      "batch 880: loss 2.311713\n",
      "batch 881: loss 2.306376\n",
      "batch 882: loss 2.299008\n",
      "batch 883: loss 2.303197\n",
      "batch 884: loss 2.310091\n",
      "batch 885: loss 2.314347\n",
      "batch 886: loss 2.301657\n",
      "batch 887: loss 2.306761\n",
      "batch 888: loss 2.297750\n",
      "batch 889: loss 2.300994\n",
      "batch 890: loss 2.308720\n",
      "batch 891: loss 2.306790\n",
      "batch 892: loss 2.301928\n",
      "batch 893: loss 2.310487\n",
      "batch 894: loss 2.299809\n",
      "batch 895: loss 2.302163\n",
      "batch 896: loss 2.298671\n",
      "batch 897: loss 2.284113\n",
      "batch 898: loss 2.299431\n",
      "batch 899: loss 2.298537\n",
      "batch 900: loss 2.295828\n",
      "batch 901: loss 2.314086\n",
      "batch 902: loss 2.294694\n",
      "batch 903: loss 2.297622\n",
      "batch 904: loss 2.294574\n",
      "batch 905: loss 2.304880\n",
      "batch 906: loss 2.297895\n",
      "batch 907: loss 2.303804\n",
      "batch 908: loss 2.301416\n",
      "batch 909: loss 2.314119\n",
      "batch 910: loss 2.303628\n",
      "batch 911: loss 2.281272\n",
      "batch 912: loss 2.297310\n",
      "batch 913: loss 2.294690\n",
      "batch 914: loss 2.291628\n",
      "batch 915: loss 2.310603\n",
      "batch 916: loss 2.307409\n",
      "batch 917: loss 2.286461\n",
      "batch 918: loss 2.296670\n",
      "batch 919: loss 2.290170\n",
      "batch 920: loss 2.302148\n",
      "batch 921: loss 2.303900\n",
      "batch 922: loss 2.302399\n",
      "batch 923: loss 2.289010\n",
      "batch 924: loss 2.294391\n",
      "batch 925: loss 2.309268\n",
      "batch 926: loss 2.311318\n",
      "batch 927: loss 2.292923\n",
      "batch 928: loss 2.314018\n",
      "batch 929: loss 2.289364\n",
      "batch 930: loss 2.315242\n",
      "batch 931: loss 2.293456\n",
      "batch 932: loss 2.303466\n",
      "batch 933: loss 2.289803\n",
      "batch 934: loss 2.305641\n",
      "batch 935: loss 2.303961\n",
      "batch 936: loss 2.312144\n",
      "batch 937: loss 2.287508\n",
      "batch 938: loss 2.293655\n",
      "batch 939: loss 2.306304\n",
      "batch 940: loss 2.296268\n",
      "batch 941: loss 2.277630\n",
      "batch 942: loss 2.298938\n",
      "batch 943: loss 2.305989\n",
      "batch 944: loss 2.301741\n",
      "batch 945: loss 2.320622\n",
      "batch 946: loss 2.309962\n",
      "batch 947: loss 2.302365\n",
      "batch 948: loss 2.290423\n",
      "batch 949: loss 2.313825\n",
      "batch 950: loss 2.303916\n",
      "batch 951: loss 2.298346\n",
      "batch 952: loss 2.315017\n",
      "batch 953: loss 2.293103\n",
      "batch 954: loss 2.300798\n",
      "batch 955: loss 2.306365\n",
      "batch 956: loss 2.310783\n",
      "batch 957: loss 2.299129\n",
      "batch 958: loss 2.298005\n",
      "batch 959: loss 2.291041\n",
      "batch 960: loss 2.297788\n",
      "batch 961: loss 2.328270\n",
      "batch 962: loss 2.305603\n",
      "batch 963: loss 2.291443\n",
      "batch 964: loss 2.275518\n",
      "batch 965: loss 2.311430\n",
      "batch 966: loss 2.297642\n",
      "batch 967: loss 2.300563\n",
      "batch 968: loss 2.275247\n",
      "batch 969: loss 2.305123\n",
      "batch 970: loss 2.311183\n",
      "batch 971: loss 2.299043\n",
      "batch 972: loss 2.289830\n",
      "batch 973: loss 2.289460\n",
      "batch 974: loss 2.288491\n",
      "batch 975: loss 2.299512\n",
      "batch 976: loss 2.300689\n",
      "batch 977: loss 2.284365\n",
      "batch 978: loss 2.313085\n",
      "batch 979: loss 2.290606\n",
      "batch 980: loss 2.295475\n",
      "batch 981: loss 2.319719\n",
      "batch 982: loss 2.307731\n",
      "batch 983: loss 2.284313\n",
      "batch 984: loss 2.306134\n",
      "batch 985: loss 2.304168\n",
      "batch 986: loss 2.302871\n",
      "batch 987: loss 2.306648\n",
      "batch 988: loss 2.314674\n",
      "batch 989: loss 2.307683\n",
      "batch 990: loss 2.304834\n",
      "batch 991: loss 2.293648\n",
      "batch 992: loss 2.317427\n",
      "batch 993: loss 2.301139\n",
      "batch 994: loss 2.297541\n",
      "batch 995: loss 2.298669\n",
      "batch 996: loss 2.317411\n",
      "batch 997: loss 2.292418\n",
      "batch 998: loss 2.320472\n",
      "batch 999: loss 2.287758\n",
      "batch 1000: loss 2.292754\n",
      "batch 1001: loss 2.300748\n",
      "batch 1002: loss 2.301476\n",
      "batch 1003: loss 2.303324\n",
      "batch 1004: loss 2.295285\n",
      "batch 1005: loss 2.289343\n",
      "batch 1006: loss 2.309799\n",
      "batch 1007: loss 2.319407\n",
      "batch 1008: loss 2.297280\n",
      "batch 1009: loss 2.281196\n",
      "batch 1010: loss 2.298137\n",
      "batch 1011: loss 2.294902\n",
      "batch 1012: loss 2.303916\n",
      "batch 1013: loss 2.324973\n",
      "batch 1014: loss 2.291454\n",
      "batch 1015: loss 2.327370\n",
      "batch 1016: loss 2.303643\n",
      "batch 1017: loss 2.307122\n",
      "batch 1018: loss 2.313303\n",
      "batch 1019: loss 2.296599\n",
      "batch 1020: loss 2.304784\n",
      "batch 1021: loss 2.302008\n",
      "batch 1022: loss 2.313477\n",
      "batch 1023: loss 2.302480\n",
      "batch 1024: loss 2.295572\n",
      "batch 1025: loss 2.290119\n",
      "batch 1026: loss 2.303644\n",
      "batch 1027: loss 2.298268\n",
      "batch 1028: loss 2.304826\n",
      "batch 1029: loss 2.290760\n",
      "batch 1030: loss 2.284642\n",
      "batch 1031: loss 2.299848\n",
      "batch 1032: loss 2.309548\n",
      "batch 1033: loss 2.312366\n",
      "batch 1034: loss 2.307162\n",
      "batch 1035: loss 2.312083\n",
      "batch 1036: loss 2.312474\n",
      "batch 1037: loss 2.304498\n",
      "batch 1038: loss 2.299835\n",
      "batch 1039: loss 2.289161\n",
      "batch 1040: loss 2.299781\n",
      "batch 1041: loss 2.313381\n",
      "batch 1042: loss 2.290878\n",
      "batch 1043: loss 2.295290\n",
      "batch 1044: loss 2.299139\n",
      "batch 1045: loss 2.297011\n",
      "batch 1046: loss 2.276836\n",
      "batch 1047: loss 2.313022\n",
      "batch 1048: loss 2.309459\n",
      "batch 1049: loss 2.300149\n",
      "batch 1050: loss 2.327190\n",
      "batch 1051: loss 2.301473\n",
      "batch 1052: loss 2.302584\n",
      "batch 1053: loss 2.305830\n",
      "batch 1054: loss 2.301438\n",
      "batch 1055: loss 2.303917\n",
      "batch 1056: loss 2.301833\n",
      "batch 1057: loss 2.308934\n",
      "batch 1058: loss 2.300957\n",
      "batch 1059: loss 2.304572\n",
      "batch 1060: loss 2.284171\n",
      "batch 1061: loss 2.313162\n",
      "batch 1062: loss 2.304569\n",
      "batch 1063: loss 2.300771\n",
      "batch 1064: loss 2.300771\n",
      "batch 1065: loss 2.299067\n",
      "batch 1066: loss 2.300147\n",
      "batch 1067: loss 2.276389\n",
      "batch 1068: loss 2.301572\n",
      "batch 1069: loss 2.299348\n",
      "batch 1070: loss 2.303481\n",
      "batch 1071: loss 2.303195\n",
      "batch 1072: loss 2.313924\n",
      "batch 1073: loss 2.292856\n",
      "batch 1074: loss 2.299535\n",
      "batch 1075: loss 2.294949\n",
      "batch 1076: loss 2.301817\n",
      "batch 1077: loss 2.307246\n",
      "batch 1078: loss 2.317183\n",
      "batch 1079: loss 2.312649\n",
      "batch 1080: loss 2.290030\n",
      "batch 1081: loss 2.306285\n",
      "batch 1082: loss 2.301884\n",
      "batch 1083: loss 2.292177\n",
      "batch 1084: loss 2.306289\n",
      "batch 1085: loss 2.314780\n",
      "batch 1086: loss 2.305100\n",
      "batch 1087: loss 2.299298\n",
      "batch 1088: loss 2.288961\n",
      "batch 1089: loss 2.313914\n",
      "batch 1090: loss 2.301614\n",
      "batch 1091: loss 2.283919\n",
      "batch 1092: loss 2.310296\n",
      "batch 1093: loss 2.304717\n",
      "batch 1094: loss 2.295079\n",
      "batch 1095: loss 2.322487\n",
      "batch 1096: loss 2.307500\n",
      "batch 1097: loss 2.310768\n",
      "batch 1098: loss 2.304498\n",
      "batch 1099: loss 2.294032\n",
      "batch 1100: loss 2.294322\n",
      "batch 1101: loss 2.309103\n",
      "batch 1102: loss 2.311859\n",
      "batch 1103: loss 2.285509\n",
      "batch 1104: loss 2.292827\n",
      "batch 1105: loss 2.301092\n",
      "batch 1106: loss 2.309766\n",
      "batch 1107: loss 2.304870\n",
      "batch 1108: loss 2.298151\n",
      "batch 1109: loss 2.304796\n",
      "batch 1110: loss 2.292982\n",
      "batch 1111: loss 2.317429\n",
      "batch 1112: loss 2.299562\n",
      "batch 1113: loss 2.306587\n",
      "batch 1114: loss 2.299595\n",
      "batch 1115: loss 2.299118\n",
      "batch 1116: loss 2.309659\n",
      "batch 1117: loss 2.273837\n",
      "batch 1118: loss 2.292492\n",
      "batch 1119: loss 2.301388\n",
      "batch 1120: loss 2.307265\n",
      "batch 1121: loss 2.302423\n",
      "batch 1122: loss 2.302931\n",
      "batch 1123: loss 2.309597\n",
      "batch 1124: loss 2.313796\n",
      "batch 1125: loss 2.299679\n",
      "batch 1126: loss 2.285540\n",
      "batch 1127: loss 2.289433\n",
      "batch 1128: loss 2.292048\n",
      "batch 1129: loss 2.286504\n",
      "batch 1130: loss 2.302641\n",
      "batch 1131: loss 2.298551\n",
      "batch 1132: loss 2.328655\n",
      "batch 1133: loss 2.291379\n",
      "batch 1134: loss 2.282690\n",
      "batch 1135: loss 2.285179\n",
      "batch 1136: loss 2.280903\n",
      "batch 1137: loss 2.307617\n",
      "batch 1138: loss 2.280843\n",
      "batch 1139: loss 2.284898\n",
      "batch 1140: loss 2.302022\n",
      "batch 1141: loss 2.287977\n",
      "batch 1142: loss 2.295237\n",
      "batch 1143: loss 2.280004\n",
      "batch 1144: loss 2.296827\n",
      "batch 1145: loss 2.300135\n",
      "batch 1146: loss 2.322165\n",
      "batch 1147: loss 2.298511\n",
      "batch 1148: loss 2.333014\n",
      "batch 1149: loss 2.316130\n",
      "batch 1150: loss 2.303283\n",
      "batch 1151: loss 2.288960\n",
      "batch 1152: loss 2.308540\n",
      "batch 1153: loss 2.270520\n",
      "batch 1154: loss 2.300949\n",
      "batch 1155: loss 2.309689\n",
      "batch 1156: loss 2.302406\n",
      "batch 1157: loss 2.324762\n",
      "batch 1158: loss 2.318711\n",
      "batch 1159: loss 2.295355\n",
      "batch 1160: loss 2.298237\n",
      "batch 1161: loss 2.288159\n",
      "batch 1162: loss 2.301601\n",
      "batch 1163: loss 2.302123\n",
      "batch 1164: loss 2.290971\n",
      "batch 1165: loss 2.307919\n",
      "batch 1166: loss 2.286938\n",
      "batch 1167: loss 2.292660\n",
      "batch 1168: loss 2.318953\n",
      "batch 1169: loss 2.299564\n",
      "batch 1170: loss 2.302220\n",
      "batch 1171: loss 2.304136\n",
      "batch 1172: loss 2.315069\n",
      "batch 1173: loss 2.295313\n",
      "batch 1174: loss 2.296861\n",
      "batch 1175: loss 2.310191\n",
      "batch 1176: loss 2.292832\n",
      "batch 1177: loss 2.304714\n",
      "batch 1178: loss 2.304821\n",
      "batch 1179: loss 2.304626\n",
      "batch 1180: loss 2.311481\n",
      "batch 1181: loss 2.296527\n",
      "batch 1182: loss 2.308553\n",
      "batch 1183: loss 2.285708\n",
      "batch 1184: loss 2.288395\n",
      "batch 1185: loss 2.297791\n",
      "batch 1186: loss 2.311793\n",
      "batch 1187: loss 2.291413\n",
      "batch 1188: loss 2.290583\n",
      "batch 1189: loss 2.312391\n",
      "batch 1190: loss 2.315324\n",
      "batch 1191: loss 2.310631\n",
      "batch 1192: loss 2.330024\n",
      "batch 1193: loss 2.300261\n",
      "batch 1194: loss 2.268967\n",
      "batch 1195: loss 2.296755\n",
      "batch 1196: loss 2.323889\n",
      "batch 1197: loss 2.292474\n",
      "batch 1198: loss 2.293499\n",
      "batch 1199: loss 2.299940\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "\n",
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷積層神經元（卷積核）數目\n",
    "            kernel_size=[5, 5],     # 接受區的大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 1]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 1\n",
    "batch_size = 50\n",
    "learning_rate = 0.01\n",
    "\n",
    "data_loader = MNISTLoader()\n",
    "model = CNN()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#print(model.summary())\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.102800\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST(Keras版本)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "\n",
    "\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))) # params = 32*9+32\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu')) # params = 320*9+64\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1480 - accuracy: 0.9545\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0490 - accuracy: 0.9851\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0353 - accuracy: 0.9887\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0262 - accuracy: 0.9918\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 13s 7ms/step - loss: 0.0220 - accuracy: 0.9927\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x24145d96d30>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0293 - accuracy: 0.9909\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
