{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d8f914a-295a-4885-a03c-6e36104be13c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Gradient  \n",
    "使用with + GradientTape方法產生梯度向量，並且使用梯度向量物件內的gradient方法實作偏微分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9885ae76-eff4-4b7d-a45f-93819023768b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([7.389056], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.constant([2.0]))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    loss = tf.math.exp(w)\n",
    "#     loss = w * w\n",
    "grad = t.gradient(loss,w)\n",
    "print(grad)\n",
    "\n",
    "#grad = t.gradient(loss,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e78ca147-81a2-4453-957c-2957009b70a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([8.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([81.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(tf.constant([2.0]))\n",
    "b = tf.Variable(tf.constant([3.0]))\n",
    "\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    loss = 2*a**2 + 3*b**3\n",
    "grad = t.gradient(loss,[a,b])\n",
    "for i in grad:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ffabd52-4617-4e2a-a39f-e62593a02c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([22.18071], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.constant([2.0]))\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    loss = 4**w\n",
    "grad = t.gradient(loss,w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0d5bd3b-32d7-4af2-b855-9298913adf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.18070977791825"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(4)*4**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7c5f205b-9c9c-4e36-acdd-8e3146111a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n",
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.constant([2.0]))\n",
    "\n",
    "# 加了persistent=True才能重複使用這個t物件，否則用完即丟\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    loss = w * w\n",
    "grad = t.gradient(loss,w)\n",
    "print(grad)\n",
    "grad = t.gradient(loss,w)\n",
    "print(grad)\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "948b4aa5-b881-409b-ad57-3a493bf8b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "w = tf.constant([2.0])\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(w) # 非tf.variable的話就要加上 watch才能微分\n",
    "    loss = w * w\n",
    "grad = t.gradient(loss,w)\n",
    "print(grad)\n",
    "\n",
    "#grad = t.gradient(loss,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fad5d8-8bb8-4eff-bdf4-a933a2a0032e",
   "metadata": {},
   "source": [
    "# Optimization - method 1\n",
    "兩種方法:\n",
    "- 共同處:\n",
    "1. 使用keras的optimizers定義優化方法\n",
    "2. for迴圈設定調整參數次數\n",
    "- 不同處:\n",
    "1. keras內的minimize方法進行疊代\n",
    "2. processed_grads以list儲存偏微分的項目 + apply_gradients方法對梯度和x的pair值(zip)實作梯度下降"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3fbb16-cc6b-4004-a924-367da242164f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.24999982118606567\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def fu_minimzie():\n",
    "    return 2*x*x + x + 3 \n",
    "\n",
    "x = tf.Variable(1.0) \n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "#opt = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#opt = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "#opt = tf.keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# 把x調整1000次\n",
    "for i in range(1000):\n",
    "    opt.minimize(fu_minimzie, var_list=[x])\n",
    "print('x: {}'.format(x.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fe5603-c5f8-449e-9cf9-c8e8402b1de3",
   "metadata": {},
   "source": [
    "# Optimization - method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4c63b9fb-2252-4377-8a1f-3b0f32090d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: -0.2499999850988388\n",
      "y: 2.875\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(1.0)\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y = 2*x*x + x + 3 \n",
    "    grads = tape.gradient(y, [x])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [x])\n",
    "#    print('y = {}, x = {}, grads = {} '.format(y.numpy(), x.numpy(), grads[0].numpy()))\n",
    "    \n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "print('x: {}'.format(x.numpy()))\n",
    "print('y: {}'.format(y.numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72448e3d-041a-4b79-b2d3-b545315f40db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 4.89378601604716e-38\n",
      "b: 1.930686042851918e-38\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0) \n",
    "b = tf.Variable(1.0) \n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "for i in range(1000):\n",
    "    \n",
    "    # 多變數調參\n",
    "    with tf.GradientTape() as tape:\n",
    "        z = a*a + 2*b*b \n",
    "    grads = tape.gradient(z, [a, b])\n",
    "    processed_grads = [g for g in grads]\n",
    "    grads_and_vars = zip(processed_grads, [a,b])\n",
    "#    print('y = {}, x = {}, grads = {} '.format(y.numpy(), x.numpy(), grads[0].numpy()))\n",
    "    \n",
    "    opt.apply_gradients(grads_and_vars)\n",
    "print('a: {}'.format(a.numpy()))\n",
    "print('b: {}'.format(b.numpy()))\n",
    "\n",
    "#print('y: {}'.format(y.numpy()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
