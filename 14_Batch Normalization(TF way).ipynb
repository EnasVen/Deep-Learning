{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization(TensorFlow版本)\n",
    "透過將每一個batch的資料正規化來縮短模型訓練時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.395871\n",
      "batch 1: loss 2.302521\n",
      "batch 2: loss 2.223227\n",
      "batch 3: loss 2.127664\n",
      "batch 4: loss 2.082026\n",
      "batch 5: loss 2.010781\n",
      "batch 6: loss 1.953032\n",
      "batch 7: loss 1.892682\n",
      "batch 8: loss 1.830975\n",
      "batch 9: loss 1.719758\n",
      "batch 10: loss 1.677893\n",
      "batch 11: loss 1.581999\n",
      "batch 12: loss 1.517661\n",
      "batch 13: loss 1.482351\n",
      "batch 14: loss 1.403175\n",
      "batch 15: loss 1.358136\n",
      "batch 16: loss 1.222202\n",
      "batch 17: loss 1.243408\n",
      "batch 18: loss 1.194086\n",
      "batch 19: loss 1.110593\n",
      "batch 20: loss 1.021773\n",
      "batch 21: loss 0.942443\n",
      "batch 22: loss 0.911492\n",
      "batch 23: loss 0.880218\n",
      "batch 24: loss 0.804583\n",
      "batch 25: loss 0.768039\n",
      "batch 26: loss 0.785902\n",
      "batch 27: loss 0.724358\n",
      "batch 28: loss 0.699926\n",
      "batch 29: loss 0.649268\n",
      "batch 30: loss 0.610541\n",
      "batch 31: loss 0.590848\n",
      "batch 32: loss 0.602088\n",
      "batch 33: loss 0.553420\n",
      "batch 34: loss 0.570544\n",
      "batch 35: loss 0.587716\n",
      "batch 36: loss 0.547053\n",
      "batch 37: loss 0.570801\n",
      "batch 38: loss 0.541263\n",
      "batch 39: loss 0.543303\n",
      "batch 40: loss 0.511966\n",
      "batch 41: loss 0.480443\n",
      "batch 42: loss 0.459338\n",
      "batch 43: loss 0.456228\n",
      "batch 44: loss 0.493893\n",
      "batch 45: loss 0.478905\n",
      "batch 46: loss 0.407915\n",
      "batch 47: loss 0.460229\n",
      "batch 48: loss 0.452885\n",
      "batch 49: loss 0.443677\n",
      "batch 50: loss 0.431070\n",
      "batch 51: loss 0.402176\n",
      "batch 52: loss 0.460926\n",
      "batch 53: loss 0.427145\n",
      "batch 54: loss 0.420839\n",
      "batch 55: loss 0.423288\n",
      "batch 56: loss 0.392523\n",
      "batch 57: loss 0.375156\n",
      "batch 58: loss 0.393117\n",
      "batch 59: loss 0.384371\n",
      "batch 60: loss 0.371423\n",
      "batch 61: loss 0.398796\n",
      "batch 62: loss 0.386292\n",
      "batch 63: loss 0.396710\n",
      "batch 64: loss 0.386143\n",
      "batch 65: loss 0.417796\n",
      "batch 66: loss 0.344399\n",
      "batch 67: loss 0.384053\n",
      "batch 68: loss 0.348121\n",
      "batch 69: loss 0.373084\n",
      "batch 70: loss 0.407333\n",
      "batch 71: loss 0.354201\n",
      "batch 72: loss 0.349270\n",
      "batch 73: loss 0.325483\n",
      "batch 74: loss 0.350455\n",
      "batch 75: loss 0.383837\n",
      "batch 76: loss 0.322372\n",
      "batch 77: loss 0.345053\n",
      "batch 78: loss 0.333755\n",
      "batch 79: loss 0.300634\n",
      "batch 80: loss 0.364523\n",
      "batch 81: loss 0.301152\n",
      "batch 82: loss 0.373795\n",
      "batch 83: loss 0.307607\n",
      "batch 84: loss 0.336133\n",
      "batch 85: loss 0.321614\n",
      "batch 86: loss 0.354595\n",
      "batch 87: loss 0.371244\n",
      "batch 88: loss 0.340039\n",
      "batch 89: loss 0.326758\n",
      "batch 90: loss 0.319692\n",
      "batch 91: loss 0.318233\n",
      "batch 92: loss 0.290001\n",
      "batch 93: loss 0.287864\n",
      "batch 94: loss 0.297477\n",
      "batch 95: loss 0.318509\n",
      "batch 96: loss 0.328466\n",
      "batch 97: loss 0.315088\n",
      "batch 98: loss 0.301933\n",
      "batch 99: loss 0.322340\n",
      "batch 100: loss 0.294147\n",
      "batch 101: loss 0.329137\n",
      "batch 102: loss 0.318309\n",
      "batch 103: loss 0.295231\n",
      "batch 104: loss 0.301700\n",
      "batch 105: loss 0.335583\n",
      "batch 106: loss 0.309289\n",
      "batch 107: loss 0.338678\n",
      "batch 108: loss 0.334796\n",
      "batch 109: loss 0.315106\n",
      "batch 110: loss 0.306316\n",
      "batch 111: loss 0.319466\n",
      "batch 112: loss 0.282578\n",
      "batch 113: loss 0.307298\n",
      "batch 114: loss 0.304969\n",
      "batch 115: loss 0.293954\n",
      "batch 116: loss 0.311902\n",
      "batch 117: loss 0.290350\n",
      "batch 118: loss 0.265605\n",
      "batch 119: loss 0.273305\n",
      "batch 120: loss 0.277005\n",
      "batch 121: loss 0.303520\n",
      "batch 122: loss 0.291574\n",
      "batch 123: loss 0.269237\n",
      "batch 124: loss 0.267254\n",
      "batch 125: loss 0.262400\n",
      "batch 126: loss 0.295292\n",
      "batch 127: loss 0.305832\n",
      "batch 128: loss 0.316331\n",
      "batch 129: loss 0.254420\n",
      "batch 130: loss 0.274555\n",
      "batch 131: loss 0.270258\n",
      "batch 132: loss 0.301042\n",
      "batch 133: loss 0.290342\n",
      "batch 134: loss 0.287707\n",
      "batch 135: loss 0.305810\n",
      "batch 136: loss 0.283544\n",
      "batch 137: loss 0.261826\n",
      "batch 138: loss 0.307725\n",
      "batch 139: loss 0.285658\n",
      "batch 140: loss 0.303536\n",
      "batch 141: loss 0.278576\n",
      "batch 142: loss 0.278145\n",
      "batch 143: loss 0.299287\n",
      "batch 144: loss 0.260034\n",
      "batch 145: loss 0.277047\n",
      "batch 146: loss 0.313596\n",
      "batch 147: loss 0.262001\n",
      "batch 148: loss 0.243808\n",
      "batch 149: loss 0.290044\n",
      "batch 150: loss 0.285346\n",
      "batch 151: loss 0.234701\n",
      "batch 152: loss 0.252644\n",
      "batch 153: loss 0.248686\n",
      "batch 154: loss 0.283697\n",
      "batch 155: loss 0.322151\n",
      "batch 156: loss 0.246379\n",
      "batch 157: loss 0.271421\n",
      "batch 158: loss 0.275010\n",
      "batch 159: loss 0.224847\n",
      "batch 160: loss 0.274099\n",
      "batch 161: loss 0.236618\n",
      "batch 162: loss 0.289178\n",
      "batch 163: loss 0.309952\n",
      "batch 164: loss 0.274812\n",
      "batch 165: loss 0.276938\n",
      "batch 166: loss 0.271889\n",
      "batch 167: loss 0.277662\n",
      "batch 168: loss 0.255136\n",
      "batch 169: loss 0.254084\n",
      "batch 170: loss 0.308528\n",
      "batch 171: loss 0.291556\n",
      "batch 172: loss 0.269903\n",
      "batch 173: loss 0.238076\n",
      "batch 174: loss 0.252666\n",
      "batch 175: loss 0.245389\n",
      "batch 176: loss 0.240502\n",
      "batch 177: loss 0.201639\n",
      "batch 178: loss 0.242383\n",
      "batch 179: loss 0.263700\n",
      "batch 180: loss 0.262498\n",
      "batch 181: loss 0.236075\n",
      "batch 182: loss 0.235341\n",
      "batch 183: loss 0.230550\n",
      "batch 184: loss 0.226118\n",
      "batch 185: loss 0.279357\n",
      "batch 186: loss 0.246040\n",
      "batch 187: loss 0.251954\n",
      "batch 188: loss 0.249165\n",
      "batch 189: loss 0.249130\n",
      "batch 190: loss 0.296653\n",
      "batch 191: loss 0.262396\n",
      "batch 192: loss 0.211352\n",
      "batch 193: loss 0.245191\n",
      "batch 194: loss 0.252495\n",
      "batch 195: loss 0.240762\n",
      "batch 196: loss 0.271061\n",
      "batch 197: loss 0.256170\n",
      "batch 198: loss 0.270625\n",
      "batch 199: loss 0.262907\n",
      "batch 200: loss 0.245638\n",
      "batch 201: loss 0.260787\n",
      "batch 202: loss 0.242953\n",
      "batch 203: loss 0.281707\n",
      "batch 204: loss 0.246451\n",
      "batch 205: loss 0.217752\n",
      "batch 206: loss 0.236371\n",
      "batch 207: loss 0.278844\n",
      "batch 208: loss 0.220445\n",
      "batch 209: loss 0.256022\n",
      "batch 210: loss 0.238423\n",
      "batch 211: loss 0.255439\n",
      "batch 212: loss 0.241087\n",
      "batch 213: loss 0.285587\n",
      "batch 214: loss 0.230529\n",
      "batch 215: loss 0.204795\n",
      "batch 216: loss 0.259563\n",
      "batch 217: loss 0.266391\n",
      "batch 218: loss 0.228562\n",
      "batch 219: loss 0.265632\n",
      "batch 220: loss 0.246009\n",
      "batch 221: loss 0.212537\n",
      "batch 222: loss 0.238537\n",
      "batch 223: loss 0.267754\n",
      "batch 224: loss 0.216679\n",
      "batch 225: loss 0.283271\n",
      "batch 226: loss 0.247846\n",
      "batch 227: loss 0.204072\n",
      "batch 228: loss 0.229214\n",
      "batch 229: loss 0.204013\n",
      "batch 230: loss 0.249167\n",
      "batch 231: loss 0.266510\n",
      "batch 232: loss 0.235478\n",
      "batch 233: loss 0.227869\n",
      "batch 234: loss 0.217476\n",
      "batch 235: loss 0.229067\n",
      "batch 236: loss 0.254162\n",
      "batch 237: loss 0.235742\n",
      "batch 238: loss 0.236314\n",
      "batch 239: loss 0.246631\n",
      "batch 240: loss 0.214194\n",
      "batch 241: loss 0.223150\n",
      "batch 242: loss 0.233451\n",
      "batch 243: loss 0.244585\n",
      "batch 244: loss 0.193062\n",
      "batch 245: loss 0.215817\n",
      "batch 246: loss 0.217867\n",
      "batch 247: loss 0.202764\n",
      "batch 248: loss 0.265996\n",
      "batch 249: loss 0.212988\n",
      "batch 250: loss 0.234969\n",
      "batch 251: loss 0.241901\n",
      "batch 252: loss 0.271046\n",
      "batch 253: loss 0.230895\n",
      "batch 254: loss 0.275338\n",
      "batch 255: loss 0.288774\n",
      "batch 256: loss 0.222712\n",
      "batch 257: loss 0.201342\n",
      "batch 258: loss 0.233245\n",
      "batch 259: loss 0.221035\n",
      "batch 260: loss 0.222363\n",
      "batch 261: loss 0.187967\n",
      "batch 262: loss 0.242912\n",
      "batch 263: loss 0.220077\n",
      "batch 264: loss 0.236692\n",
      "batch 265: loss 0.233128\n",
      "batch 266: loss 0.227481\n",
      "batch 267: loss 0.170328\n",
      "batch 268: loss 0.210498\n",
      "batch 269: loss 0.182006\n",
      "batch 270: loss 0.234633\n",
      "batch 271: loss 0.232164\n",
      "batch 272: loss 0.188884\n",
      "batch 273: loss 0.181317\n",
      "batch 274: loss 0.233423\n",
      "batch 275: loss 0.169424\n",
      "batch 276: loss 0.234360\n",
      "batch 277: loss 0.194796\n",
      "batch 278: loss 0.211834\n",
      "batch 279: loss 0.214875\n",
      "batch 280: loss 0.169863\n",
      "batch 281: loss 0.249782\n",
      "batch 282: loss 0.194659\n",
      "batch 283: loss 0.233114\n",
      "batch 284: loss 0.241509\n",
      "batch 285: loss 0.202995\n",
      "batch 286: loss 0.209868\n",
      "batch 287: loss 0.221284\n",
      "batch 288: loss 0.220094\n",
      "batch 289: loss 0.196181\n",
      "batch 290: loss 0.226332\n",
      "batch 291: loss 0.183376\n",
      "batch 292: loss 0.237367\n",
      "batch 293: loss 0.191565\n",
      "batch 294: loss 0.181346\n",
      "batch 295: loss 0.222528\n",
      "batch 296: loss 0.219471\n",
      "batch 297: loss 0.206688\n",
      "batch 298: loss 0.198317\n",
      "batch 299: loss 0.192314\n",
      "test accuracy: 0.938400\n"
     ]
    }
   ],
   "source": [
    "# 增加訓練速度 (丟進激活函數之前先做Batch Normalization，簡稱BN層)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.bn1(x)\n",
    "        x = self.ac1(x)\n",
    "        \n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.bn2(x)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
