{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Penalty Restriction(TensorFlow版本)\n",
    "透過L1 & L2 Regularization來防止類神經網路overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.435484\n",
      "batch 1: loss 2.235003\n",
      "batch 2: loss 2.219408\n",
      "batch 3: loss 2.103619\n",
      "batch 4: loss 2.093702\n",
      "batch 5: loss 2.011176\n",
      "batch 6: loss 1.976617\n",
      "batch 7: loss 1.986470\n",
      "batch 8: loss 1.821554\n",
      "batch 9: loss 1.747742\n",
      "batch 10: loss 1.700303\n",
      "batch 11: loss 1.721067\n",
      "batch 12: loss 1.631144\n",
      "batch 13: loss 1.618835\n",
      "batch 14: loss 1.594871\n",
      "batch 15: loss 1.520527\n",
      "batch 16: loss 1.482371\n",
      "batch 17: loss 1.382504\n",
      "batch 18: loss 1.166363\n",
      "batch 19: loss 1.341701\n",
      "batch 20: loss 1.364029\n",
      "batch 21: loss 1.141860\n",
      "batch 22: loss 1.105238\n",
      "batch 23: loss 1.218396\n",
      "batch 24: loss 1.143119\n",
      "batch 25: loss 0.932499\n",
      "batch 26: loss 1.122879\n",
      "batch 27: loss 0.927348\n",
      "batch 28: loss 0.918405\n",
      "batch 29: loss 0.759680\n",
      "batch 30: loss 0.909123\n",
      "batch 31: loss 0.793296\n",
      "batch 32: loss 0.776943\n",
      "batch 33: loss 0.889736\n",
      "batch 34: loss 0.835688\n",
      "batch 35: loss 0.936955\n",
      "batch 36: loss 0.720742\n",
      "batch 37: loss 0.964241\n",
      "batch 38: loss 0.769888\n",
      "batch 39: loss 0.833201\n",
      "batch 40: loss 0.672999\n",
      "batch 41: loss 0.874691\n",
      "batch 42: loss 0.705780\n",
      "batch 43: loss 0.691137\n",
      "batch 44: loss 0.524506\n",
      "batch 45: loss 0.620781\n",
      "batch 46: loss 0.889927\n",
      "batch 47: loss 0.736543\n",
      "batch 48: loss 0.655127\n",
      "batch 49: loss 0.694714\n",
      "batch 50: loss 0.596407\n",
      "batch 51: loss 0.669339\n",
      "batch 52: loss 0.602160\n",
      "batch 53: loss 0.501325\n",
      "batch 54: loss 0.584653\n",
      "batch 55: loss 0.719615\n",
      "batch 56: loss 0.832600\n",
      "batch 57: loss 0.791332\n",
      "batch 58: loss 0.643914\n",
      "batch 59: loss 0.550853\n",
      "batch 60: loss 0.799639\n",
      "batch 61: loss 0.615529\n",
      "batch 62: loss 0.556809\n",
      "batch 63: loss 0.606895\n",
      "batch 64: loss 0.491099\n",
      "batch 65: loss 0.449175\n",
      "batch 66: loss 0.728122\n",
      "batch 67: loss 0.535114\n",
      "batch 68: loss 0.437929\n",
      "batch 69: loss 0.707891\n",
      "batch 70: loss 0.504810\n",
      "batch 71: loss 0.496701\n",
      "batch 72: loss 0.403119\n",
      "batch 73: loss 0.652816\n",
      "batch 74: loss 0.456083\n",
      "batch 75: loss 0.486037\n",
      "batch 76: loss 0.487516\n",
      "batch 77: loss 0.470300\n",
      "batch 78: loss 0.680592\n",
      "batch 79: loss 0.532582\n",
      "batch 80: loss 0.578205\n",
      "batch 81: loss 0.390047\n",
      "batch 82: loss 0.485612\n",
      "batch 83: loss 0.580472\n",
      "batch 84: loss 0.445409\n",
      "batch 85: loss 0.464635\n",
      "batch 86: loss 0.411538\n",
      "batch 87: loss 0.401426\n",
      "batch 88: loss 0.343833\n",
      "batch 89: loss 0.394789\n",
      "batch 90: loss 0.475418\n",
      "batch 91: loss 0.324739\n",
      "batch 92: loss 0.545702\n",
      "batch 93: loss 0.637712\n",
      "batch 94: loss 0.414330\n",
      "batch 95: loss 0.533487\n",
      "batch 96: loss 0.602629\n",
      "batch 97: loss 0.668057\n",
      "batch 98: loss 0.584377\n",
      "batch 99: loss 0.697336\n",
      "batch 100: loss 0.392546\n",
      "batch 101: loss 0.309475\n",
      "batch 102: loss 0.319978\n",
      "batch 103: loss 0.336374\n",
      "batch 104: loss 0.381196\n",
      "batch 105: loss 0.510990\n",
      "batch 106: loss 0.462259\n",
      "batch 107: loss 0.453555\n",
      "batch 108: loss 0.471546\n",
      "batch 109: loss 0.439962\n",
      "batch 110: loss 0.543279\n",
      "batch 111: loss 0.293016\n",
      "batch 112: loss 0.449405\n",
      "batch 113: loss 0.508910\n",
      "batch 114: loss 0.289504\n",
      "batch 115: loss 0.358246\n",
      "batch 116: loss 0.358058\n",
      "batch 117: loss 0.405899\n",
      "batch 118: loss 0.504811\n",
      "batch 119: loss 0.332851\n",
      "batch 120: loss 0.556040\n",
      "batch 121: loss 0.691181\n",
      "batch 122: loss 0.496662\n",
      "batch 123: loss 0.443116\n",
      "batch 124: loss 0.394209\n",
      "batch 125: loss 0.244469\n",
      "batch 126: loss 0.348119\n",
      "batch 127: loss 0.309034\n",
      "batch 128: loss 0.411929\n",
      "batch 129: loss 0.392534\n",
      "batch 130: loss 0.352779\n",
      "batch 131: loss 0.267708\n",
      "batch 132: loss 0.459417\n",
      "batch 133: loss 0.399191\n",
      "batch 134: loss 0.269851\n",
      "batch 135: loss 0.268051\n",
      "batch 136: loss 0.262997\n",
      "batch 137: loss 0.257387\n",
      "batch 138: loss 0.419670\n",
      "batch 139: loss 0.351392\n",
      "batch 140: loss 0.699660\n",
      "batch 141: loss 0.241033\n",
      "batch 142: loss 0.239647\n",
      "batch 143: loss 0.353206\n",
      "batch 144: loss 0.428226\n",
      "batch 145: loss 0.444964\n",
      "batch 146: loss 0.294210\n",
      "batch 147: loss 0.287250\n",
      "batch 148: loss 0.380799\n",
      "batch 149: loss 0.372649\n",
      "batch 150: loss 0.548609\n",
      "batch 151: loss 0.297726\n",
      "batch 152: loss 0.390104\n",
      "batch 153: loss 0.307506\n",
      "batch 154: loss 0.282154\n",
      "batch 155: loss 0.242398\n",
      "batch 156: loss 0.382334\n",
      "batch 157: loss 0.467994\n",
      "batch 158: loss 0.464323\n",
      "batch 159: loss 0.272071\n",
      "batch 160: loss 0.387800\n",
      "batch 161: loss 0.369671\n",
      "batch 162: loss 0.567985\n",
      "batch 163: loss 0.193560\n",
      "batch 164: loss 0.363352\n",
      "batch 165: loss 0.460408\n",
      "batch 166: loss 0.388439\n",
      "batch 167: loss 0.363542\n",
      "batch 168: loss 0.305742\n",
      "batch 169: loss 0.328106\n",
      "batch 170: loss 0.368947\n",
      "batch 171: loss 0.205627\n",
      "batch 172: loss 0.355763\n",
      "batch 173: loss 0.511055\n",
      "batch 174: loss 0.158312\n",
      "batch 175: loss 0.202243\n",
      "batch 176: loss 0.277568\n",
      "batch 177: loss 0.241599\n",
      "batch 178: loss 0.325438\n",
      "batch 179: loss 0.372776\n",
      "batch 180: loss 0.263927\n",
      "batch 181: loss 0.308615\n",
      "batch 182: loss 0.343246\n",
      "batch 183: loss 0.288671\n",
      "batch 184: loss 0.201782\n",
      "batch 185: loss 0.360879\n",
      "batch 186: loss 0.343626\n",
      "batch 187: loss 0.284776\n",
      "batch 188: loss 0.387647\n",
      "batch 189: loss 0.345905\n",
      "batch 190: loss 0.391786\n",
      "batch 191: loss 0.388718\n",
      "batch 192: loss 0.402056\n",
      "batch 193: loss 0.457700\n",
      "batch 194: loss 0.393124\n",
      "batch 195: loss 0.419988\n",
      "batch 196: loss 0.274083\n",
      "batch 197: loss 0.343369\n",
      "batch 198: loss 0.365086\n",
      "batch 199: loss 0.531432\n",
      "batch 200: loss 0.290444\n",
      "batch 201: loss 0.386948\n",
      "batch 202: loss 0.330022\n",
      "batch 203: loss 0.270076\n",
      "batch 204: loss 0.385611\n",
      "batch 205: loss 0.261044\n",
      "batch 206: loss 0.360414\n",
      "batch 207: loss 0.310769\n",
      "batch 208: loss 0.186372\n",
      "batch 209: loss 0.432043\n",
      "batch 210: loss 0.307772\n",
      "batch 211: loss 0.208729\n",
      "batch 212: loss 0.418149\n",
      "batch 213: loss 0.574855\n",
      "batch 214: loss 0.315520\n",
      "batch 215: loss 0.388218\n",
      "batch 216: loss 0.396893\n",
      "batch 217: loss 0.209183\n",
      "batch 218: loss 0.416156\n",
      "batch 219: loss 0.371296\n",
      "batch 220: loss 0.353555\n",
      "batch 221: loss 0.395200\n",
      "batch 222: loss 0.345652\n",
      "batch 223: loss 0.253353\n",
      "batch 224: loss 0.274762\n",
      "batch 225: loss 0.336039\n",
      "batch 226: loss 0.282741\n",
      "batch 227: loss 0.457531\n",
      "batch 228: loss 0.603944\n",
      "batch 229: loss 0.185678\n",
      "batch 230: loss 0.362082\n",
      "batch 231: loss 0.385180\n",
      "batch 232: loss 0.289130\n",
      "batch 233: loss 0.306494\n",
      "batch 234: loss 0.328836\n",
      "batch 235: loss 0.366798\n",
      "batch 236: loss 0.580430\n",
      "batch 237: loss 0.409849\n",
      "batch 238: loss 0.385311\n",
      "batch 239: loss 0.330095\n",
      "batch 240: loss 0.366453\n",
      "batch 241: loss 0.360399\n",
      "batch 242: loss 0.293690\n",
      "batch 243: loss 0.347713\n",
      "batch 244: loss 0.310296\n",
      "batch 245: loss 0.454649\n",
      "batch 246: loss 0.453177\n",
      "batch 247: loss 0.217053\n",
      "batch 248: loss 0.466286\n",
      "batch 249: loss 0.241209\n",
      "batch 250: loss 0.332573\n",
      "batch 251: loss 0.393895\n",
      "batch 252: loss 0.463172\n",
      "batch 253: loss 0.205448\n",
      "batch 254: loss 0.368365\n",
      "batch 255: loss 0.173803\n",
      "batch 256: loss 0.277361\n",
      "batch 257: loss 0.381095\n",
      "batch 258: loss 0.308039\n",
      "batch 259: loss 0.374692\n",
      "batch 260: loss 0.165486\n",
      "batch 261: loss 0.414308\n",
      "batch 262: loss 0.385251\n",
      "batch 263: loss 0.241084\n",
      "batch 264: loss 0.151025\n",
      "batch 265: loss 0.280719\n",
      "batch 266: loss 0.119251\n",
      "batch 267: loss 0.349201\n",
      "batch 268: loss 0.308631\n",
      "batch 269: loss 0.395764\n",
      "batch 270: loss 0.374149\n",
      "batch 271: loss 0.329852\n",
      "batch 272: loss 0.437099\n",
      "batch 273: loss 0.336907\n",
      "batch 274: loss 0.253442\n",
      "batch 275: loss 0.401184\n",
      "batch 276: loss 0.223760\n",
      "batch 277: loss 0.421201\n",
      "batch 278: loss 0.271059\n",
      "batch 279: loss 0.237785\n",
      "batch 280: loss 0.268778\n",
      "batch 281: loss 0.137439\n",
      "batch 282: loss 0.190203\n",
      "batch 283: loss 0.403601\n",
      "batch 284: loss 0.500709\n",
      "batch 285: loss 0.389790\n",
      "batch 286: loss 0.560661\n",
      "batch 287: loss 0.454579\n",
      "batch 288: loss 0.289884\n",
      "batch 289: loss 0.277228\n",
      "batch 290: loss 0.269174\n",
      "batch 291: loss 0.414199\n",
      "batch 292: loss 0.340972\n",
      "batch 293: loss 0.202600\n",
      "batch 294: loss 0.311274\n",
      "batch 295: loss 0.574364\n",
      "batch 296: loss 0.154172\n",
      "batch 297: loss 0.297668\n",
      "batch 298: loss 0.253359\n",
      "batch 299: loss 0.522048\n",
      "batch 300: loss 0.259819\n",
      "batch 301: loss 0.151609\n",
      "batch 302: loss 0.247924\n",
      "batch 303: loss 0.227001\n",
      "batch 304: loss 0.358875\n",
      "batch 305: loss 0.341477\n",
      "batch 306: loss 0.174164\n",
      "batch 307: loss 0.361993\n",
      "batch 308: loss 0.294231\n",
      "batch 309: loss 0.108300\n",
      "batch 310: loss 0.244218\n",
      "batch 311: loss 0.258063\n",
      "batch 312: loss 0.120398\n",
      "batch 313: loss 0.222536\n",
      "batch 314: loss 0.308354\n",
      "batch 315: loss 0.370323\n",
      "batch 316: loss 0.585374\n",
      "batch 317: loss 0.476982\n",
      "batch 318: loss 0.274853\n",
      "batch 319: loss 0.331180\n",
      "batch 320: loss 0.168084\n",
      "batch 321: loss 0.204990\n",
      "batch 322: loss 0.240261\n",
      "batch 323: loss 0.440943\n",
      "batch 324: loss 0.510001\n",
      "batch 325: loss 0.173608\n",
      "batch 326: loss 0.348249\n",
      "batch 327: loss 0.291412\n",
      "batch 328: loss 0.266576\n",
      "batch 329: loss 0.410149\n",
      "batch 330: loss 0.445570\n",
      "batch 331: loss 0.527785\n",
      "batch 332: loss 0.272271\n",
      "batch 333: loss 0.336262\n",
      "batch 334: loss 0.333678\n",
      "batch 335: loss 0.396246\n",
      "batch 336: loss 0.324130\n",
      "batch 337: loss 0.480021\n",
      "batch 338: loss 0.303348\n",
      "batch 339: loss 0.364450\n",
      "batch 340: loss 0.271574\n",
      "batch 341: loss 0.308823\n",
      "batch 342: loss 0.078418\n",
      "batch 343: loss 0.288650\n",
      "batch 344: loss 0.292378\n",
      "batch 345: loss 0.372023\n",
      "batch 346: loss 0.409073\n",
      "batch 347: loss 0.147342\n",
      "batch 348: loss 0.528069\n",
      "batch 349: loss 0.256436\n",
      "batch 350: loss 0.184579\n",
      "batch 351: loss 0.406407\n",
      "batch 352: loss 0.204030\n",
      "batch 353: loss 0.453205\n",
      "batch 354: loss 0.417131\n",
      "batch 355: loss 0.243764\n",
      "batch 356: loss 0.297764\n",
      "batch 357: loss 0.252586\n",
      "batch 358: loss 0.274745\n",
      "batch 359: loss 0.237667\n",
      "batch 360: loss 0.229021\n",
      "batch 361: loss 0.242294\n",
      "batch 362: loss 0.252069\n",
      "batch 363: loss 0.295178\n",
      "batch 364: loss 0.318558\n",
      "batch 365: loss 0.283616\n",
      "batch 366: loss 0.300867\n",
      "batch 367: loss 0.343245\n",
      "batch 368: loss 0.128374\n",
      "batch 369: loss 0.163625\n",
      "batch 370: loss 0.239244\n",
      "batch 371: loss 0.230173\n",
      "batch 372: loss 0.214348\n",
      "batch 373: loss 0.202327\n",
      "batch 374: loss 0.337117\n",
      "batch 375: loss 0.318242\n",
      "batch 376: loss 0.240712\n",
      "batch 377: loss 0.244443\n",
      "batch 378: loss 0.367242\n",
      "batch 379: loss 0.276901\n",
      "batch 380: loss 0.279319\n",
      "batch 381: loss 0.283929\n",
      "batch 382: loss 0.113808\n",
      "batch 383: loss 0.257828\n",
      "batch 384: loss 0.238380\n",
      "batch 385: loss 0.210748\n",
      "batch 386: loss 0.583575\n",
      "batch 387: loss 0.160881\n",
      "batch 388: loss 0.396505\n",
      "batch 389: loss 0.276769\n",
      "batch 390: loss 0.315640\n",
      "batch 391: loss 0.107167\n",
      "batch 392: loss 0.177091\n",
      "batch 393: loss 0.382562\n",
      "batch 394: loss 0.264673\n",
      "batch 395: loss 0.333035\n",
      "batch 396: loss 0.168643\n",
      "batch 397: loss 0.266767\n",
      "batch 398: loss 0.219535\n",
      "batch 399: loss 0.233120\n",
      "batch 400: loss 0.288629\n",
      "batch 401: loss 0.292043\n",
      "batch 402: loss 0.368905\n",
      "batch 403: loss 0.344513\n",
      "batch 404: loss 0.185107\n",
      "batch 405: loss 0.263210\n",
      "batch 406: loss 0.347261\n",
      "batch 407: loss 0.381256\n",
      "batch 408: loss 0.218729\n",
      "batch 409: loss 0.281420\n",
      "batch 410: loss 0.210748\n",
      "batch 411: loss 0.318066\n",
      "batch 412: loss 0.321976\n",
      "batch 413: loss 0.239807\n",
      "batch 414: loss 0.222687\n",
      "batch 415: loss 0.422660\n",
      "batch 416: loss 0.419205\n",
      "batch 417: loss 0.315073\n",
      "batch 418: loss 0.248880\n",
      "batch 419: loss 0.577274\n",
      "batch 420: loss 0.137148\n",
      "batch 421: loss 0.174972\n",
      "batch 422: loss 0.271409\n",
      "batch 423: loss 0.327294\n",
      "batch 424: loss 0.108192\n",
      "batch 425: loss 0.347178\n",
      "batch 426: loss 0.219783\n",
      "batch 427: loss 0.373163\n",
      "batch 428: loss 0.405422\n",
      "batch 429: loss 0.104683\n",
      "batch 430: loss 0.209676\n",
      "batch 431: loss 0.154666\n",
      "batch 432: loss 0.274991\n",
      "batch 433: loss 0.199744\n",
      "batch 434: loss 0.217209\n",
      "batch 435: loss 0.450570\n",
      "batch 436: loss 0.281030\n",
      "batch 437: loss 0.239704\n",
      "batch 438: loss 0.291149\n",
      "batch 439: loss 0.259483\n",
      "batch 440: loss 0.227651\n",
      "batch 441: loss 0.131472\n",
      "batch 442: loss 0.352254\n",
      "batch 443: loss 0.311587\n",
      "batch 444: loss 0.147935\n",
      "batch 445: loss 0.258381\n",
      "batch 446: loss 0.266772\n",
      "batch 447: loss 0.142470\n",
      "batch 448: loss 0.338867\n",
      "batch 449: loss 0.260013\n",
      "batch 450: loss 0.411715\n",
      "batch 451: loss 0.276952\n",
      "batch 452: loss 0.338137\n",
      "batch 453: loss 0.206084\n",
      "batch 454: loss 0.231182\n",
      "batch 455: loss 0.213495\n",
      "batch 456: loss 0.266488\n",
      "batch 457: loss 0.158070\n",
      "batch 458: loss 0.173022\n",
      "batch 459: loss 0.397787\n",
      "batch 460: loss 0.151303\n",
      "batch 461: loss 0.195366\n",
      "batch 462: loss 0.293665\n",
      "batch 463: loss 0.299992\n",
      "batch 464: loss 0.346927\n",
      "batch 465: loss 0.188860\n",
      "batch 466: loss 0.216001\n",
      "batch 467: loss 0.160145\n",
      "batch 468: loss 0.238005\n",
      "batch 469: loss 0.132062\n",
      "batch 470: loss 0.369759\n",
      "batch 471: loss 0.310987\n",
      "batch 472: loss 0.279396\n",
      "batch 473: loss 0.398142\n",
      "batch 474: loss 0.285133\n",
      "batch 475: loss 0.317511\n",
      "batch 476: loss 0.298576\n",
      "batch 477: loss 0.239029\n",
      "batch 478: loss 0.256906\n",
      "batch 479: loss 0.327522\n",
      "batch 480: loss 0.126484\n",
      "batch 481: loss 0.299535\n",
      "batch 482: loss 0.228624\n",
      "batch 483: loss 0.277519\n",
      "batch 484: loss 0.192512\n",
      "batch 485: loss 0.137583\n",
      "batch 486: loss 0.248316\n",
      "batch 487: loss 0.619091\n",
      "batch 488: loss 0.624722\n",
      "batch 489: loss 0.319237\n",
      "batch 490: loss 0.266462\n",
      "batch 491: loss 0.286148\n",
      "batch 492: loss 0.390892\n",
      "batch 493: loss 0.484046\n",
      "batch 494: loss 0.381080\n",
      "batch 495: loss 0.093801\n",
      "batch 496: loss 0.166000\n",
      "batch 497: loss 0.240034\n",
      "batch 498: loss 0.154158\n",
      "batch 499: loss 0.261345\n",
      "batch 500: loss 0.323132\n",
      "batch 501: loss 0.241439\n",
      "batch 502: loss 0.304234\n",
      "batch 503: loss 0.339799\n",
      "batch 504: loss 0.229356\n",
      "batch 505: loss 0.324758\n",
      "batch 506: loss 0.497106\n",
      "batch 507: loss 0.124050\n",
      "batch 508: loss 0.326340\n",
      "batch 509: loss 0.208283\n",
      "batch 510: loss 0.246009\n",
      "batch 511: loss 0.310895\n",
      "batch 512: loss 0.109611\n",
      "batch 513: loss 0.438265\n",
      "batch 514: loss 0.193905\n",
      "batch 515: loss 0.244841\n",
      "batch 516: loss 0.227792\n",
      "batch 517: loss 0.278467\n",
      "batch 518: loss 0.319768\n",
      "batch 519: loss 0.313554\n",
      "batch 520: loss 0.285463\n",
      "batch 521: loss 0.142888\n",
      "batch 522: loss 0.313062\n",
      "batch 523: loss 0.144737\n",
      "batch 524: loss 0.201686\n",
      "batch 525: loss 0.135933\n",
      "batch 526: loss 0.229985\n",
      "batch 527: loss 0.061810\n",
      "batch 528: loss 0.360286\n",
      "batch 529: loss 0.571184\n",
      "batch 530: loss 0.483511\n",
      "batch 531: loss 0.374950\n",
      "batch 532: loss 0.180095\n",
      "batch 533: loss 0.334124\n",
      "batch 534: loss 0.406091\n",
      "batch 535: loss 0.154512\n",
      "batch 536: loss 0.156953\n",
      "batch 537: loss 0.155190\n",
      "batch 538: loss 0.286363\n",
      "batch 539: loss 0.114800\n",
      "batch 540: loss 0.148489\n",
      "batch 541: loss 0.206055\n",
      "batch 542: loss 0.587826\n",
      "batch 543: loss 0.424018\n",
      "batch 544: loss 0.158245\n",
      "batch 545: loss 0.277787\n",
      "batch 546: loss 0.093970\n",
      "batch 547: loss 0.401705\n",
      "batch 548: loss 0.215481\n",
      "batch 549: loss 0.327706\n",
      "batch 550: loss 0.282031\n",
      "batch 551: loss 0.175931\n",
      "batch 552: loss 0.338165\n",
      "batch 553: loss 0.156866\n",
      "batch 554: loss 0.241266\n",
      "batch 555: loss 0.234518\n",
      "batch 556: loss 0.328193\n",
      "batch 557: loss 0.616292\n",
      "batch 558: loss 0.322407\n",
      "batch 559: loss 0.180264\n",
      "batch 560: loss 0.183320\n",
      "batch 561: loss 0.223057\n",
      "batch 562: loss 0.188405\n",
      "batch 563: loss 0.193699\n",
      "batch 564: loss 0.415607\n",
      "batch 565: loss 0.176363\n",
      "batch 566: loss 0.140156\n",
      "batch 567: loss 0.377036\n",
      "batch 568: loss 0.246038\n",
      "batch 569: loss 0.192837\n",
      "batch 570: loss 0.196685\n",
      "batch 571: loss 0.314513\n",
      "batch 572: loss 0.322673\n",
      "batch 573: loss 0.265456\n",
      "batch 574: loss 0.237463\n",
      "batch 575: loss 0.079576\n",
      "batch 576: loss 0.172133\n",
      "batch 577: loss 0.155723\n",
      "batch 578: loss 0.080118\n",
      "batch 579: loss 0.140213\n",
      "batch 580: loss 0.142886\n",
      "batch 581: loss 0.185471\n",
      "batch 582: loss 0.246374\n",
      "batch 583: loss 0.135761\n",
      "batch 584: loss 0.195548\n",
      "batch 585: loss 0.207243\n",
      "batch 586: loss 0.121969\n",
      "batch 587: loss 0.216773\n",
      "batch 588: loss 0.286158\n",
      "batch 589: loss 0.152521\n",
      "batch 590: loss 0.297086\n",
      "batch 591: loss 0.173785\n",
      "batch 592: loss 0.124943\n",
      "batch 593: loss 0.200542\n",
      "batch 594: loss 0.176631\n",
      "batch 595: loss 0.164709\n",
      "batch 596: loss 0.276831\n",
      "batch 597: loss 0.251205\n",
      "batch 598: loss 0.121997\n",
      "batch 599: loss 0.358797\n",
      "batch 600: loss 0.289517\n",
      "batch 601: loss 0.040949\n",
      "batch 602: loss 0.194377\n",
      "batch 603: loss 0.293214\n",
      "batch 604: loss 0.094922\n",
      "batch 605: loss 0.176285\n",
      "batch 606: loss 0.290108\n",
      "batch 607: loss 0.341742\n",
      "batch 608: loss 0.236300\n",
      "batch 609: loss 0.237509\n",
      "batch 610: loss 0.212831\n",
      "batch 611: loss 0.223575\n",
      "batch 612: loss 0.246396\n",
      "batch 613: loss 0.235474\n",
      "batch 614: loss 0.135110\n",
      "batch 615: loss 0.137916\n",
      "batch 616: loss 0.184842\n",
      "batch 617: loss 0.126979\n",
      "batch 618: loss 0.100369\n",
      "batch 619: loss 0.167638\n",
      "batch 620: loss 0.189544\n",
      "batch 621: loss 0.162677\n",
      "batch 622: loss 0.619765\n",
      "batch 623: loss 0.104586\n",
      "batch 624: loss 0.191069\n",
      "batch 625: loss 0.047870\n",
      "batch 626: loss 0.207122\n",
      "batch 627: loss 0.222951\n",
      "batch 628: loss 0.235420\n",
      "batch 629: loss 0.089069\n",
      "batch 630: loss 0.166138\n",
      "batch 631: loss 0.124925\n",
      "batch 632: loss 0.173844\n",
      "batch 633: loss 0.242014\n",
      "batch 634: loss 0.463234\n",
      "batch 635: loss 0.163949\n",
      "batch 636: loss 0.448822\n",
      "batch 637: loss 0.258278\n",
      "batch 638: loss 0.295098\n",
      "batch 639: loss 0.207625\n",
      "batch 640: loss 0.133120\n",
      "batch 641: loss 0.201515\n",
      "batch 642: loss 0.367662\n",
      "batch 643: loss 0.271613\n",
      "batch 644: loss 0.148308\n",
      "batch 645: loss 0.080829\n",
      "batch 646: loss 0.307501\n",
      "batch 647: loss 0.238694\n",
      "batch 648: loss 0.289383\n",
      "batch 649: loss 0.090449\n",
      "batch 650: loss 0.272432\n",
      "batch 651: loss 0.105804\n",
      "batch 652: loss 0.349521\n",
      "batch 653: loss 0.247518\n",
      "batch 654: loss 0.118157\n",
      "batch 655: loss 0.108082\n",
      "batch 656: loss 0.065925\n",
      "batch 657: loss 0.323453\n",
      "batch 658: loss 0.289074\n",
      "batch 659: loss 0.093954\n",
      "batch 660: loss 0.230781\n",
      "batch 661: loss 0.141364\n",
      "batch 662: loss 0.194280\n",
      "batch 663: loss 0.498545\n",
      "batch 664: loss 0.180939\n",
      "batch 665: loss 0.122484\n",
      "batch 666: loss 0.159543\n",
      "batch 667: loss 0.217922\n",
      "batch 668: loss 0.179480\n",
      "batch 669: loss 0.148899\n",
      "batch 670: loss 0.200193\n",
      "batch 671: loss 0.141598\n",
      "batch 672: loss 0.280849\n",
      "batch 673: loss 0.113020\n",
      "batch 674: loss 0.292980\n",
      "batch 675: loss 0.293636\n",
      "batch 676: loss 0.093420\n",
      "batch 677: loss 0.103580\n",
      "batch 678: loss 0.197186\n",
      "batch 679: loss 0.392141\n",
      "batch 680: loss 0.206534\n",
      "batch 681: loss 0.208954\n",
      "batch 682: loss 0.462047\n",
      "batch 683: loss 0.433510\n",
      "batch 684: loss 0.114745\n",
      "batch 685: loss 0.174571\n",
      "batch 686: loss 0.299549\n",
      "batch 687: loss 0.180736\n",
      "batch 688: loss 0.238682\n",
      "batch 689: loss 0.489059\n",
      "batch 690: loss 0.261129\n",
      "batch 691: loss 0.216600\n",
      "batch 692: loss 0.167404\n",
      "batch 693: loss 0.119036\n",
      "batch 694: loss 0.140686\n",
      "batch 695: loss 0.261935\n",
      "batch 696: loss 0.254405\n",
      "batch 697: loss 0.106279\n",
      "batch 698: loss 0.082499\n",
      "batch 699: loss 0.146032\n",
      "batch 700: loss 0.222556\n",
      "batch 701: loss 0.248481\n",
      "batch 702: loss 0.178319\n",
      "batch 703: loss 0.109762\n",
      "batch 704: loss 0.111942\n",
      "batch 705: loss 0.094974\n",
      "batch 706: loss 0.164271\n",
      "batch 707: loss 0.148301\n",
      "batch 708: loss 0.138643\n",
      "batch 709: loss 0.231699\n",
      "batch 710: loss 0.228575\n",
      "batch 711: loss 0.239078\n",
      "batch 712: loss 0.114147\n",
      "batch 713: loss 0.265014\n",
      "batch 714: loss 0.053077\n",
      "batch 715: loss 0.109492\n",
      "batch 716: loss 0.325672\n",
      "batch 717: loss 0.145712\n",
      "batch 718: loss 0.100328\n",
      "batch 719: loss 0.196896\n",
      "batch 720: loss 0.294744\n",
      "batch 721: loss 0.214707\n",
      "batch 722: loss 0.207001\n",
      "batch 723: loss 0.313114\n",
      "batch 724: loss 0.092544\n",
      "batch 725: loss 0.104362\n",
      "batch 726: loss 0.227279\n",
      "batch 727: loss 0.181436\n",
      "batch 728: loss 0.152101\n",
      "batch 729: loss 0.141669\n",
      "batch 730: loss 0.295167\n",
      "batch 731: loss 0.087884\n",
      "batch 732: loss 0.174068\n",
      "batch 733: loss 0.359311\n",
      "batch 734: loss 0.126773\n",
      "batch 735: loss 0.122857\n",
      "batch 736: loss 0.351526\n",
      "batch 737: loss 0.197915\n",
      "batch 738: loss 0.372802\n",
      "batch 739: loss 0.185020\n",
      "batch 740: loss 0.294879\n",
      "batch 741: loss 0.060992\n",
      "batch 742: loss 0.159418\n",
      "batch 743: loss 0.350114\n",
      "batch 744: loss 0.217631\n",
      "batch 745: loss 0.240707\n",
      "batch 746: loss 0.170271\n",
      "batch 747: loss 0.258181\n",
      "batch 748: loss 0.280709\n",
      "batch 749: loss 0.161412\n",
      "batch 750: loss 0.161548\n",
      "batch 751: loss 0.192870\n",
      "batch 752: loss 0.164367\n",
      "batch 753: loss 0.127415\n",
      "batch 754: loss 0.099162\n",
      "batch 755: loss 0.216864\n",
      "batch 756: loss 0.232435\n",
      "batch 757: loss 0.315003\n",
      "batch 758: loss 0.090937\n",
      "batch 759: loss 0.106811\n",
      "batch 760: loss 0.218655\n",
      "batch 761: loss 0.188131\n",
      "batch 762: loss 0.075913\n",
      "batch 763: loss 0.069920\n",
      "batch 764: loss 0.308469\n",
      "batch 765: loss 0.142339\n",
      "batch 766: loss 0.243584\n",
      "batch 767: loss 0.245242\n",
      "batch 768: loss 0.234327\n",
      "batch 769: loss 0.147032\n",
      "batch 770: loss 0.175729\n",
      "batch 771: loss 0.116736\n",
      "batch 772: loss 0.085752\n",
      "batch 773: loss 0.152482\n",
      "batch 774: loss 0.090277\n",
      "batch 775: loss 0.221605\n",
      "batch 776: loss 0.277377\n",
      "batch 777: loss 0.257009\n",
      "batch 778: loss 0.130079\n",
      "batch 779: loss 0.211980\n",
      "batch 780: loss 0.145896\n",
      "batch 781: loss 0.101761\n",
      "batch 782: loss 0.112485\n",
      "batch 783: loss 0.190776\n",
      "batch 784: loss 0.152706\n",
      "batch 785: loss 0.220380\n",
      "batch 786: loss 0.215575\n",
      "batch 787: loss 0.324076\n",
      "batch 788: loss 0.241278\n",
      "batch 789: loss 0.118975\n",
      "batch 790: loss 0.164416\n",
      "batch 791: loss 0.264798\n",
      "batch 792: loss 0.351399\n",
      "batch 793: loss 0.060956\n",
      "batch 794: loss 0.286104\n",
      "batch 795: loss 0.200581\n",
      "batch 796: loss 0.237497\n",
      "batch 797: loss 0.147305\n",
      "batch 798: loss 0.260024\n",
      "batch 799: loss 0.161064\n",
      "batch 800: loss 0.148512\n",
      "batch 801: loss 0.088003\n",
      "batch 802: loss 0.260539\n",
      "batch 803: loss 0.189309\n",
      "batch 804: loss 0.210800\n",
      "batch 805: loss 0.167944\n",
      "batch 806: loss 0.234606\n",
      "batch 807: loss 0.101078\n",
      "batch 808: loss 0.596151\n",
      "batch 809: loss 0.136210\n",
      "batch 810: loss 0.240128\n",
      "batch 811: loss 0.300245\n",
      "batch 812: loss 0.209613\n",
      "batch 813: loss 0.274653\n",
      "batch 814: loss 0.254843\n",
      "batch 815: loss 0.275656\n",
      "batch 816: loss 0.130073\n",
      "batch 817: loss 0.387181\n",
      "batch 818: loss 0.131812\n",
      "batch 819: loss 0.066671\n",
      "batch 820: loss 0.163847\n",
      "batch 821: loss 0.147373\n",
      "batch 822: loss 0.218350\n",
      "batch 823: loss 0.059005\n",
      "batch 824: loss 0.303712\n",
      "batch 825: loss 0.135237\n",
      "batch 826: loss 0.074865\n",
      "batch 827: loss 0.243291\n",
      "batch 828: loss 0.172602\n",
      "batch 829: loss 0.158621\n",
      "batch 830: loss 0.179203\n",
      "batch 831: loss 0.207457\n",
      "batch 832: loss 0.332390\n",
      "batch 833: loss 0.099622\n",
      "batch 834: loss 0.089756\n",
      "batch 835: loss 0.046570\n",
      "batch 836: loss 0.076710\n",
      "batch 837: loss 0.274982\n",
      "batch 838: loss 0.208282\n",
      "batch 839: loss 0.294067\n",
      "batch 840: loss 0.191549\n",
      "batch 841: loss 0.116920\n",
      "batch 842: loss 0.191658\n",
      "batch 843: loss 0.270698\n",
      "batch 844: loss 0.315301\n",
      "batch 845: loss 0.373113\n",
      "batch 846: loss 0.174733\n",
      "batch 847: loss 0.385544\n",
      "batch 848: loss 0.157899\n",
      "batch 849: loss 0.150985\n",
      "batch 850: loss 0.266468\n",
      "batch 851: loss 0.155881\n",
      "batch 852: loss 0.191557\n",
      "batch 853: loss 0.150117\n",
      "batch 854: loss 0.132858\n",
      "batch 855: loss 0.086511\n",
      "batch 856: loss 0.082170\n",
      "batch 857: loss 0.223751\n",
      "batch 858: loss 0.368588\n",
      "batch 859: loss 0.191178\n",
      "batch 860: loss 0.277188\n",
      "batch 861: loss 0.203735\n",
      "batch 862: loss 0.227830\n",
      "batch 863: loss 0.155099\n",
      "batch 864: loss 0.126987\n",
      "batch 865: loss 0.099984\n",
      "batch 866: loss 0.175839\n",
      "batch 867: loss 0.200130\n",
      "batch 868: loss 0.232930\n",
      "batch 869: loss 0.167460\n",
      "batch 870: loss 0.161783\n",
      "batch 871: loss 0.390452\n",
      "batch 872: loss 0.144688\n",
      "batch 873: loss 0.122942\n",
      "batch 874: loss 0.283755\n",
      "batch 875: loss 0.186539\n",
      "batch 876: loss 0.241036\n",
      "batch 877: loss 0.193160\n",
      "batch 878: loss 0.321906\n",
      "batch 879: loss 0.103674\n",
      "batch 880: loss 0.270126\n",
      "batch 881: loss 0.178846\n",
      "batch 882: loss 0.163060\n",
      "batch 883: loss 0.257508\n",
      "batch 884: loss 0.192522\n",
      "batch 885: loss 0.211687\n",
      "batch 886: loss 0.201714\n",
      "batch 887: loss 0.170903\n",
      "batch 888: loss 0.217216\n",
      "batch 889: loss 0.184434\n",
      "batch 890: loss 0.203221\n",
      "batch 891: loss 0.189028\n",
      "batch 892: loss 0.127986\n",
      "batch 893: loss 0.308314\n",
      "batch 894: loss 0.212284\n",
      "batch 895: loss 0.115272\n",
      "batch 896: loss 0.179254\n",
      "batch 897: loss 0.271751\n",
      "batch 898: loss 0.187840\n",
      "batch 899: loss 0.414518\n",
      "batch 900: loss 0.123038\n",
      "batch 901: loss 0.146220\n",
      "batch 902: loss 0.331395\n",
      "batch 903: loss 0.082203\n",
      "batch 904: loss 0.189187\n",
      "batch 905: loss 0.094806\n",
      "batch 906: loss 0.156450\n",
      "batch 907: loss 0.400821\n",
      "batch 908: loss 0.208173\n",
      "batch 909: loss 0.269554\n",
      "batch 910: loss 0.132166\n",
      "batch 911: loss 0.279706\n",
      "batch 912: loss 0.186981\n",
      "batch 913: loss 0.225079\n",
      "batch 914: loss 0.327449\n",
      "batch 915: loss 0.103475\n",
      "batch 916: loss 0.153989\n",
      "batch 917: loss 0.148812\n",
      "batch 918: loss 0.295721\n",
      "batch 919: loss 0.181953\n",
      "batch 920: loss 0.138689\n",
      "batch 921: loss 0.350654\n",
      "batch 922: loss 0.249820\n",
      "batch 923: loss 0.177449\n",
      "batch 924: loss 0.234781\n",
      "batch 925: loss 0.262893\n",
      "batch 926: loss 0.224575\n",
      "batch 927: loss 0.072735\n",
      "batch 928: loss 0.094627\n",
      "batch 929: loss 0.124124\n",
      "batch 930: loss 0.193856\n",
      "batch 931: loss 0.247114\n",
      "batch 932: loss 0.109904\n",
      "batch 933: loss 0.134369\n",
      "batch 934: loss 0.250513\n",
      "batch 935: loss 0.196872\n",
      "batch 936: loss 0.112095\n",
      "batch 937: loss 0.149192\n",
      "batch 938: loss 0.305356\n",
      "batch 939: loss 0.388117\n",
      "batch 940: loss 0.241651\n",
      "batch 941: loss 0.109040\n",
      "batch 942: loss 0.298567\n",
      "batch 943: loss 0.163947\n",
      "batch 944: loss 0.276169\n",
      "batch 945: loss 0.088152\n",
      "batch 946: loss 0.198957\n",
      "batch 947: loss 0.121420\n",
      "batch 948: loss 0.098170\n",
      "batch 949: loss 0.308698\n",
      "batch 950: loss 0.320187\n",
      "batch 951: loss 0.160742\n",
      "batch 952: loss 0.061101\n",
      "batch 953: loss 0.117306\n",
      "batch 954: loss 0.115127\n",
      "batch 955: loss 0.122864\n",
      "batch 956: loss 0.076049\n",
      "batch 957: loss 0.227556\n",
      "batch 958: loss 0.234414\n",
      "batch 959: loss 0.130316\n",
      "batch 960: loss 0.093942\n",
      "batch 961: loss 0.155056\n",
      "batch 962: loss 0.055219\n",
      "batch 963: loss 0.109182\n",
      "batch 964: loss 0.200399\n",
      "batch 965: loss 0.194530\n",
      "batch 966: loss 0.245368\n",
      "batch 967: loss 0.139560\n",
      "batch 968: loss 0.109802\n",
      "batch 969: loss 0.246665\n",
      "batch 970: loss 0.359263\n",
      "batch 971: loss 0.149370\n",
      "batch 972: loss 0.133806\n",
      "batch 973: loss 0.190701\n",
      "batch 974: loss 0.209746\n",
      "batch 975: loss 0.244304\n",
      "batch 976: loss 0.163765\n",
      "batch 977: loss 0.111077\n",
      "batch 978: loss 0.168089\n",
      "batch 979: loss 0.236999\n",
      "batch 980: loss 0.130987\n",
      "batch 981: loss 0.245528\n",
      "batch 982: loss 0.105910\n",
      "batch 983: loss 0.136494\n",
      "batch 984: loss 0.232971\n",
      "batch 985: loss 0.131351\n",
      "batch 986: loss 0.166647\n",
      "batch 987: loss 0.109499\n",
      "batch 988: loss 0.184384\n",
      "batch 989: loss 0.154406\n",
      "batch 990: loss 0.113739\n",
      "batch 991: loss 0.292770\n",
      "batch 992: loss 0.181763\n",
      "batch 993: loss 0.096248\n",
      "batch 994: loss 0.102705\n",
      "batch 995: loss 0.378290\n",
      "batch 996: loss 0.176618\n",
      "batch 997: loss 0.242509\n",
      "batch 998: loss 0.199366\n",
      "batch 999: loss 0.142506\n",
      "batch 1000: loss 0.125373\n",
      "batch 1001: loss 0.092427\n",
      "batch 1002: loss 0.164562\n",
      "batch 1003: loss 0.160872\n",
      "batch 1004: loss 0.255990\n",
      "batch 1005: loss 0.179592\n",
      "batch 1006: loss 0.231022\n",
      "batch 1007: loss 0.200131\n",
      "batch 1008: loss 0.222330\n",
      "batch 1009: loss 0.239484\n",
      "batch 1010: loss 0.336105\n",
      "batch 1011: loss 0.219571\n",
      "batch 1012: loss 0.062673\n",
      "batch 1013: loss 0.057567\n",
      "batch 1014: loss 0.289384\n",
      "batch 1015: loss 0.225227\n",
      "batch 1016: loss 0.142958\n",
      "batch 1017: loss 0.127964\n",
      "batch 1018: loss 0.064579\n",
      "batch 1019: loss 0.349477\n",
      "batch 1020: loss 0.103560\n",
      "batch 1021: loss 0.170121\n",
      "batch 1022: loss 0.320103\n",
      "batch 1023: loss 0.207170\n",
      "batch 1024: loss 0.228345\n",
      "batch 1025: loss 0.142387\n",
      "batch 1026: loss 0.180933\n",
      "batch 1027: loss 0.191240\n",
      "batch 1028: loss 0.163744\n",
      "batch 1029: loss 0.083857\n",
      "batch 1030: loss 0.138744\n",
      "batch 1031: loss 0.302030\n",
      "batch 1032: loss 0.233921\n",
      "batch 1033: loss 0.155814\n",
      "batch 1034: loss 0.167978\n",
      "batch 1035: loss 0.218194\n",
      "batch 1036: loss 0.060646\n",
      "batch 1037: loss 0.041469\n",
      "batch 1038: loss 0.112619\n",
      "batch 1039: loss 0.127585\n",
      "batch 1040: loss 0.196797\n",
      "batch 1041: loss 0.327392\n",
      "batch 1042: loss 0.125235\n",
      "batch 1043: loss 0.125990\n",
      "batch 1044: loss 0.204576\n",
      "batch 1045: loss 0.164074\n",
      "batch 1046: loss 0.116740\n",
      "batch 1047: loss 0.315964\n",
      "batch 1048: loss 0.398028\n",
      "batch 1049: loss 0.141378\n",
      "batch 1050: loss 0.135799\n",
      "batch 1051: loss 0.109952\n",
      "batch 1052: loss 0.064376\n",
      "batch 1053: loss 0.064703\n",
      "batch 1054: loss 0.237707\n",
      "batch 1055: loss 0.077176\n",
      "batch 1056: loss 0.276423\n",
      "batch 1057: loss 0.187322\n",
      "batch 1058: loss 0.135963\n",
      "batch 1059: loss 0.278670\n",
      "batch 1060: loss 0.217147\n",
      "batch 1061: loss 0.229399\n",
      "batch 1062: loss 0.218892\n",
      "batch 1063: loss 0.170740\n",
      "batch 1064: loss 0.131354\n",
      "batch 1065: loss 0.109844\n",
      "batch 1066: loss 0.113431\n",
      "batch 1067: loss 0.073662\n",
      "batch 1068: loss 0.149900\n",
      "batch 1069: loss 0.159897\n",
      "batch 1070: loss 0.154295\n",
      "batch 1071: loss 0.068759\n",
      "batch 1072: loss 0.348960\n",
      "batch 1073: loss 0.209755\n",
      "batch 1074: loss 0.197231\n",
      "batch 1075: loss 0.181516\n",
      "batch 1076: loss 0.042714\n",
      "batch 1077: loss 0.153453\n",
      "batch 1078: loss 0.165110\n",
      "batch 1079: loss 0.151285\n",
      "batch 1080: loss 0.112238\n",
      "batch 1081: loss 0.074486\n",
      "batch 1082: loss 0.088057\n",
      "batch 1083: loss 0.262239\n",
      "batch 1084: loss 0.458330\n",
      "batch 1085: loss 0.269197\n",
      "batch 1086: loss 0.323306\n",
      "batch 1087: loss 0.321219\n",
      "batch 1088: loss 0.159427\n",
      "batch 1089: loss 0.418464\n",
      "batch 1090: loss 0.126861\n",
      "batch 1091: loss 0.146021\n",
      "batch 1092: loss 0.120414\n",
      "batch 1093: loss 0.138000\n",
      "batch 1094: loss 0.159358\n",
      "batch 1095: loss 0.137081\n",
      "batch 1096: loss 0.061927\n",
      "batch 1097: loss 0.107677\n",
      "batch 1098: loss 0.098775\n",
      "batch 1099: loss 0.139855\n",
      "batch 1100: loss 0.120672\n",
      "batch 1101: loss 0.159588\n",
      "batch 1102: loss 0.206740\n",
      "batch 1103: loss 0.196274\n",
      "batch 1104: loss 0.153397\n",
      "batch 1105: loss 0.193662\n",
      "batch 1106: loss 0.074557\n",
      "batch 1107: loss 0.091350\n",
      "batch 1108: loss 0.160375\n",
      "batch 1109: loss 0.137826\n",
      "batch 1110: loss 0.225218\n",
      "batch 1111: loss 0.206199\n",
      "batch 1112: loss 0.066919\n",
      "batch 1113: loss 0.065407\n",
      "batch 1114: loss 0.091374\n",
      "batch 1115: loss 0.081536\n",
      "batch 1116: loss 0.130069\n",
      "batch 1117: loss 0.075244\n",
      "batch 1118: loss 0.170039\n",
      "batch 1119: loss 0.129009\n",
      "batch 1120: loss 0.283475\n",
      "batch 1121: loss 0.173091\n",
      "batch 1122: loss 0.189856\n",
      "batch 1123: loss 0.131484\n",
      "batch 1124: loss 0.132298\n",
      "batch 1125: loss 0.094490\n",
      "batch 1126: loss 0.193091\n",
      "batch 1127: loss 0.118119\n",
      "batch 1128: loss 0.169760\n",
      "batch 1129: loss 0.159931\n",
      "batch 1130: loss 0.283076\n",
      "batch 1131: loss 0.066373\n",
      "batch 1132: loss 0.100471\n",
      "batch 1133: loss 0.103307\n",
      "batch 1134: loss 0.295151\n",
      "batch 1135: loss 0.319144\n",
      "batch 1136: loss 0.298743\n",
      "batch 1137: loss 0.177161\n",
      "batch 1138: loss 0.095569\n",
      "batch 1139: loss 0.094459\n",
      "batch 1140: loss 0.141737\n",
      "batch 1141: loss 0.156394\n",
      "batch 1142: loss 0.279763\n",
      "batch 1143: loss 0.224498\n",
      "batch 1144: loss 0.212534\n",
      "batch 1145: loss 0.116353\n",
      "batch 1146: loss 0.127071\n",
      "batch 1147: loss 0.113812\n",
      "batch 1148: loss 0.087453\n",
      "batch 1149: loss 0.074153\n",
      "batch 1150: loss 0.232729\n",
      "batch 1151: loss 0.107352\n",
      "batch 1152: loss 0.117026\n",
      "batch 1153: loss 0.128269\n",
      "batch 1154: loss 0.207573\n",
      "batch 1155: loss 0.312712\n",
      "batch 1156: loss 0.116092\n",
      "batch 1157: loss 0.050082\n",
      "batch 1158: loss 0.152399\n",
      "batch 1159: loss 0.166194\n",
      "batch 1160: loss 0.284708\n",
      "batch 1161: loss 0.219974\n",
      "batch 1162: loss 0.217404\n",
      "batch 1163: loss 0.160155\n",
      "batch 1164: loss 0.333443\n",
      "batch 1165: loss 0.141007\n",
      "batch 1166: loss 0.085415\n",
      "batch 1167: loss 0.137558\n",
      "batch 1168: loss 0.064422\n",
      "batch 1169: loss 0.073179\n",
      "batch 1170: loss 0.103790\n",
      "batch 1171: loss 0.160310\n",
      "batch 1172: loss 0.236672\n",
      "batch 1173: loss 0.067941\n",
      "batch 1174: loss 0.272774\n",
      "batch 1175: loss 0.218377\n",
      "batch 1176: loss 0.119394\n",
      "batch 1177: loss 0.127356\n",
      "batch 1178: loss 0.180915\n",
      "batch 1179: loss 0.170059\n",
      "batch 1180: loss 0.275142\n",
      "batch 1181: loss 0.143189\n",
      "batch 1182: loss 0.086789\n",
      "batch 1183: loss 0.149621\n",
      "batch 1184: loss 0.123795\n",
      "batch 1185: loss 0.266662\n",
      "batch 1186: loss 0.104090\n",
      "batch 1187: loss 0.152834\n",
      "batch 1188: loss 0.244896\n",
      "batch 1189: loss 0.112630\n",
      "batch 1190: loss 0.153693\n",
      "batch 1191: loss 0.187744\n",
      "batch 1192: loss 0.036777\n",
      "batch 1193: loss 0.211543\n",
      "batch 1194: loss 0.078813\n",
      "batch 1195: loss 0.130889\n",
      "batch 1196: loss 0.085286\n",
      "batch 1197: loss 0.065766\n",
      "batch 1198: loss 0.175549\n",
      "batch 1199: loss 0.319267\n",
      "batch 1200: loss 0.177976\n",
      "batch 1201: loss 0.034822\n",
      "batch 1202: loss 0.287433\n",
      "batch 1203: loss 0.122122\n",
      "batch 1204: loss 0.166833\n",
      "batch 1205: loss 0.276542\n",
      "batch 1206: loss 0.069775\n",
      "batch 1207: loss 0.106831\n",
      "batch 1208: loss 0.055727\n",
      "batch 1209: loss 0.254605\n",
      "batch 1210: loss 0.270047\n",
      "batch 1211: loss 0.149013\n",
      "batch 1212: loss 0.101204\n",
      "batch 1213: loss 0.160523\n",
      "batch 1214: loss 0.086107\n",
      "batch 1215: loss 0.149967\n",
      "batch 1216: loss 0.104865\n",
      "batch 1217: loss 0.155097\n",
      "batch 1218: loss 0.139202\n",
      "batch 1219: loss 0.113714\n",
      "batch 1220: loss 0.099374\n",
      "batch 1221: loss 0.117271\n",
      "batch 1222: loss 0.177457\n",
      "batch 1223: loss 0.134673\n",
      "batch 1224: loss 0.089258\n",
      "batch 1225: loss 0.146412\n",
      "batch 1226: loss 0.058379\n",
      "batch 1227: loss 0.124726\n",
      "batch 1228: loss 0.036770\n",
      "batch 1229: loss 0.276600\n",
      "batch 1230: loss 0.079065\n",
      "batch 1231: loss 0.180976\n",
      "batch 1232: loss 0.277845\n",
      "batch 1233: loss 0.276586\n",
      "batch 1234: loss 0.196030\n",
      "batch 1235: loss 0.076494\n",
      "batch 1236: loss 0.243371\n",
      "batch 1237: loss 0.103862\n",
      "batch 1238: loss 0.040509\n",
      "batch 1239: loss 0.285306\n",
      "batch 1240: loss 0.148747\n",
      "batch 1241: loss 0.192506\n",
      "batch 1242: loss 0.055023\n",
      "batch 1243: loss 0.036575\n",
      "batch 1244: loss 0.158644\n",
      "batch 1245: loss 0.231156\n",
      "batch 1246: loss 0.194554\n",
      "batch 1247: loss 0.098387\n",
      "batch 1248: loss 0.118726\n",
      "batch 1249: loss 0.313193\n",
      "batch 1250: loss 0.189471\n",
      "batch 1251: loss 0.271424\n",
      "batch 1252: loss 0.261799\n",
      "batch 1253: loss 0.156385\n",
      "batch 1254: loss 0.058875\n",
      "batch 1255: loss 0.197166\n",
      "batch 1256: loss 0.096431\n",
      "batch 1257: loss 0.185177\n",
      "batch 1258: loss 0.114074\n",
      "batch 1259: loss 0.105551\n",
      "batch 1260: loss 0.344213\n",
      "batch 1261: loss 0.114417\n",
      "batch 1262: loss 0.307855\n",
      "batch 1263: loss 0.090588\n",
      "batch 1264: loss 0.341768\n",
      "batch 1265: loss 0.270745\n",
      "batch 1266: loss 0.229437\n",
      "batch 1267: loss 0.113431\n",
      "batch 1268: loss 0.124685\n",
      "batch 1269: loss 0.223821\n",
      "batch 1270: loss 0.163954\n",
      "batch 1271: loss 0.163040\n",
      "batch 1272: loss 0.057445\n",
      "batch 1273: loss 0.088840\n",
      "batch 1274: loss 0.197950\n",
      "batch 1275: loss 0.124191\n",
      "batch 1276: loss 0.147669\n",
      "batch 1277: loss 0.132369\n",
      "batch 1278: loss 0.109776\n",
      "batch 1279: loss 0.154231\n",
      "batch 1280: loss 0.107869\n",
      "batch 1281: loss 0.126814\n",
      "batch 1282: loss 0.169062\n",
      "batch 1283: loss 0.083086\n",
      "batch 1284: loss 0.071758\n",
      "batch 1285: loss 0.146472\n",
      "batch 1286: loss 0.140808\n",
      "batch 1287: loss 0.259157\n",
      "batch 1288: loss 0.183844\n",
      "batch 1289: loss 0.420471\n",
      "batch 1290: loss 0.194282\n",
      "batch 1291: loss 0.144471\n",
      "batch 1292: loss 0.213990\n",
      "batch 1293: loss 0.299372\n",
      "batch 1294: loss 0.121466\n",
      "batch 1295: loss 0.094871\n",
      "batch 1296: loss 0.160095\n",
      "batch 1297: loss 0.123917\n",
      "batch 1298: loss 0.196321\n",
      "batch 1299: loss 0.101346\n",
      "batch 1300: loss 0.162999\n",
      "batch 1301: loss 0.099187\n",
      "batch 1302: loss 0.271466\n",
      "batch 1303: loss 0.145949\n",
      "batch 1304: loss 0.210992\n",
      "batch 1305: loss 0.127233\n",
      "batch 1306: loss 0.145804\n",
      "batch 1307: loss 0.048982\n",
      "batch 1308: loss 0.443509\n",
      "batch 1309: loss 0.123174\n",
      "batch 1310: loss 0.133299\n",
      "batch 1311: loss 0.162813\n",
      "batch 1312: loss 0.170793\n",
      "batch 1313: loss 0.196843\n",
      "batch 1314: loss 0.068914\n",
      "batch 1315: loss 0.114044\n",
      "batch 1316: loss 0.088561\n",
      "batch 1317: loss 0.046992\n",
      "batch 1318: loss 0.261097\n",
      "batch 1319: loss 0.086943\n",
      "batch 1320: loss 0.063334\n",
      "batch 1321: loss 0.141493\n",
      "batch 1322: loss 0.080046\n",
      "batch 1323: loss 0.239894\n",
      "batch 1324: loss 0.147914\n",
      "batch 1325: loss 0.247371\n",
      "batch 1326: loss 0.235404\n",
      "batch 1327: loss 0.240683\n",
      "batch 1328: loss 0.094261\n",
      "batch 1329: loss 0.097144\n",
      "batch 1330: loss 0.104203\n",
      "batch 1331: loss 0.107337\n",
      "batch 1332: loss 0.157647\n",
      "batch 1333: loss 0.146792\n",
      "batch 1334: loss 0.235079\n",
      "batch 1335: loss 0.145882\n",
      "batch 1336: loss 0.259007\n",
      "batch 1337: loss 0.100133\n",
      "batch 1338: loss 0.115230\n",
      "batch 1339: loss 0.112998\n",
      "batch 1340: loss 0.097370\n",
      "batch 1341: loss 0.048991\n",
      "batch 1342: loss 0.327871\n",
      "batch 1343: loss 0.153957\n",
      "batch 1344: loss 0.165621\n",
      "batch 1345: loss 0.515907\n",
      "batch 1346: loss 0.174517\n",
      "batch 1347: loss 0.088727\n",
      "batch 1348: loss 0.227193\n",
      "batch 1349: loss 0.089810\n",
      "batch 1350: loss 0.177681\n",
      "batch 1351: loss 0.193284\n",
      "batch 1352: loss 0.139079\n",
      "batch 1353: loss 0.043155\n",
      "batch 1354: loss 0.100862\n",
      "batch 1355: loss 0.231969\n",
      "batch 1356: loss 0.050539\n",
      "batch 1357: loss 0.164005\n",
      "batch 1358: loss 0.376293\n",
      "batch 1359: loss 0.080790\n",
      "batch 1360: loss 0.111973\n",
      "batch 1361: loss 0.094919\n",
      "batch 1362: loss 0.131102\n",
      "batch 1363: loss 0.056204\n",
      "batch 1364: loss 0.235399\n",
      "batch 1365: loss 0.180667\n",
      "batch 1366: loss 0.130598\n",
      "batch 1367: loss 0.183788\n",
      "batch 1368: loss 0.111007\n",
      "batch 1369: loss 0.299577\n",
      "batch 1370: loss 0.139286\n",
      "batch 1371: loss 0.077600\n",
      "batch 1372: loss 0.159986\n",
      "batch 1373: loss 0.115057\n",
      "batch 1374: loss 0.118172\n",
      "batch 1375: loss 0.233738\n",
      "batch 1376: loss 0.194523\n",
      "batch 1377: loss 0.359595\n",
      "batch 1378: loss 0.220653\n",
      "batch 1379: loss 0.134527\n",
      "batch 1380: loss 0.062014\n",
      "batch 1381: loss 0.118546\n",
      "batch 1382: loss 0.270247\n",
      "batch 1383: loss 0.104575\n",
      "batch 1384: loss 0.042441\n",
      "batch 1385: loss 0.100099\n",
      "batch 1386: loss 0.238945\n",
      "batch 1387: loss 0.241668\n",
      "batch 1388: loss 0.212447\n",
      "batch 1389: loss 0.083798\n",
      "batch 1390: loss 0.324143\n",
      "batch 1391: loss 0.080777\n",
      "batch 1392: loss 0.130662\n",
      "batch 1393: loss 0.105828\n",
      "batch 1394: loss 0.110836\n",
      "batch 1395: loss 0.107372\n",
      "batch 1396: loss 0.214425\n",
      "batch 1397: loss 0.174532\n",
      "batch 1398: loss 0.131318\n",
      "batch 1399: loss 0.122843\n",
      "batch 1400: loss 0.059878\n",
      "batch 1401: loss 0.112455\n",
      "batch 1402: loss 0.185496\n",
      "batch 1403: loss 0.278227\n",
      "batch 1404: loss 0.246693\n",
      "batch 1405: loss 0.237652\n",
      "batch 1406: loss 0.183833\n",
      "batch 1407: loss 0.148067\n",
      "batch 1408: loss 0.106190\n",
      "batch 1409: loss 0.187585\n",
      "batch 1410: loss 0.410689\n",
      "batch 1411: loss 0.174970\n",
      "batch 1412: loss 0.148060\n",
      "batch 1413: loss 0.115733\n",
      "batch 1414: loss 0.267291\n",
      "batch 1415: loss 0.195524\n",
      "batch 1416: loss 0.180458\n",
      "batch 1417: loss 0.063070\n",
      "batch 1418: loss 0.087977\n",
      "batch 1419: loss 0.138653\n",
      "batch 1420: loss 0.128737\n",
      "batch 1421: loss 0.205836\n",
      "batch 1422: loss 0.204496\n",
      "batch 1423: loss 0.072097\n",
      "batch 1424: loss 0.082285\n",
      "batch 1425: loss 0.205893\n",
      "batch 1426: loss 0.304770\n",
      "batch 1427: loss 0.090146\n",
      "batch 1428: loss 0.094849\n",
      "batch 1429: loss 0.052431\n",
      "batch 1430: loss 0.075521\n",
      "batch 1431: loss 0.172596\n",
      "batch 1432: loss 0.170091\n",
      "batch 1433: loss 0.128254\n",
      "batch 1434: loss 0.082587\n",
      "batch 1435: loss 0.072481\n",
      "batch 1436: loss 0.296744\n",
      "batch 1437: loss 0.233280\n",
      "batch 1438: loss 0.065029\n",
      "batch 1439: loss 0.072240\n",
      "batch 1440: loss 0.198459\n",
      "batch 1441: loss 0.382296\n",
      "batch 1442: loss 0.317839\n",
      "batch 1443: loss 0.130672\n",
      "batch 1444: loss 0.098407\n",
      "batch 1445: loss 0.134557\n",
      "batch 1446: loss 0.093350\n",
      "batch 1447: loss 0.048438\n",
      "batch 1448: loss 0.142583\n",
      "batch 1449: loss 0.228186\n",
      "batch 1450: loss 0.163961\n",
      "batch 1451: loss 0.145538\n",
      "batch 1452: loss 0.115762\n",
      "batch 1453: loss 0.115595\n",
      "batch 1454: loss 0.114521\n",
      "batch 1455: loss 0.072006\n",
      "batch 1456: loss 0.102151\n",
      "batch 1457: loss 0.427176\n",
      "batch 1458: loss 0.239551\n",
      "batch 1459: loss 0.103401\n",
      "batch 1460: loss 0.248524\n",
      "batch 1461: loss 0.145567\n",
      "batch 1462: loss 0.231536\n",
      "batch 1463: loss 0.067676\n",
      "batch 1464: loss 0.110082\n",
      "batch 1465: loss 0.165410\n",
      "batch 1466: loss 0.133566\n",
      "batch 1467: loss 0.089754\n",
      "batch 1468: loss 0.088610\n",
      "batch 1469: loss 0.103613\n",
      "batch 1470: loss 0.134325\n",
      "batch 1471: loss 0.194791\n",
      "batch 1472: loss 0.184674\n",
      "batch 1473: loss 0.120510\n",
      "batch 1474: loss 0.038186\n",
      "batch 1475: loss 0.184233\n",
      "batch 1476: loss 0.089236\n",
      "batch 1477: loss 0.243494\n",
      "batch 1478: loss 0.200474\n",
      "batch 1479: loss 0.084622\n",
      "batch 1480: loss 0.075955\n",
      "batch 1481: loss 0.082923\n",
      "batch 1482: loss 0.181024\n",
      "batch 1483: loss 0.066811\n",
      "batch 1484: loss 0.072052\n",
      "batch 1485: loss 0.129671\n",
      "batch 1486: loss 0.223010\n",
      "batch 1487: loss 0.086197\n",
      "batch 1488: loss 0.304428\n",
      "batch 1489: loss 0.295091\n",
      "batch 1490: loss 0.054291\n",
      "batch 1491: loss 0.037151\n",
      "batch 1492: loss 0.134577\n",
      "batch 1493: loss 0.068082\n",
      "batch 1494: loss 0.171990\n",
      "batch 1495: loss 0.273990\n",
      "batch 1496: loss 0.205038\n",
      "batch 1497: loss 0.124048\n",
      "batch 1498: loss 0.150908\n",
      "batch 1499: loss 0.285820\n",
      "batch 1500: loss 0.188237\n",
      "batch 1501: loss 0.273913\n",
      "batch 1502: loss 0.206081\n",
      "batch 1503: loss 0.118406\n",
      "batch 1504: loss 0.303567\n",
      "batch 1505: loss 0.259671\n",
      "batch 1506: loss 0.095674\n",
      "batch 1507: loss 0.178871\n",
      "batch 1508: loss 0.126021\n",
      "batch 1509: loss 0.192686\n",
      "batch 1510: loss 0.067083\n",
      "batch 1511: loss 0.040050\n",
      "batch 1512: loss 0.106062\n",
      "batch 1513: loss 0.070949\n",
      "batch 1514: loss 0.235889\n",
      "batch 1515: loss 0.141434\n",
      "batch 1516: loss 0.336708\n",
      "batch 1517: loss 0.144594\n",
      "batch 1518: loss 0.098865\n",
      "batch 1519: loss 0.319937\n",
      "batch 1520: loss 0.125161\n",
      "batch 1521: loss 0.061163\n",
      "batch 1522: loss 0.119199\n",
      "batch 1523: loss 0.156796\n",
      "batch 1524: loss 0.104399\n",
      "batch 1525: loss 0.043881\n",
      "batch 1526: loss 0.196708\n",
      "batch 1527: loss 0.030930\n",
      "batch 1528: loss 0.115838\n",
      "batch 1529: loss 0.125334\n",
      "batch 1530: loss 0.069576\n",
      "batch 1531: loss 0.184238\n",
      "batch 1532: loss 0.086155\n",
      "batch 1533: loss 0.094206\n",
      "batch 1534: loss 0.303314\n",
      "batch 1535: loss 0.074816\n",
      "batch 1536: loss 0.149744\n",
      "batch 1537: loss 0.127706\n",
      "batch 1538: loss 0.138452\n",
      "batch 1539: loss 0.255407\n",
      "batch 1540: loss 0.084788\n",
      "batch 1541: loss 0.151581\n",
      "batch 1542: loss 0.107076\n",
      "batch 1543: loss 0.129797\n",
      "batch 1544: loss 0.406501\n",
      "batch 1545: loss 0.148977\n",
      "batch 1546: loss 0.084415\n",
      "batch 1547: loss 0.048807\n",
      "batch 1548: loss 0.167655\n",
      "batch 1549: loss 0.073177\n",
      "batch 1550: loss 0.145931\n",
      "batch 1551: loss 0.139319\n",
      "batch 1552: loss 0.027179\n",
      "batch 1553: loss 0.089448\n",
      "batch 1554: loss 0.229360\n",
      "batch 1555: loss 0.156427\n",
      "batch 1556: loss 0.151192\n",
      "batch 1557: loss 0.064663\n",
      "batch 1558: loss 0.200477\n",
      "batch 1559: loss 0.269828\n",
      "batch 1560: loss 0.107251\n",
      "batch 1561: loss 0.048881\n",
      "batch 1562: loss 0.220274\n",
      "batch 1563: loss 0.141783\n",
      "batch 1564: loss 0.382582\n",
      "batch 1565: loss 0.044984\n",
      "batch 1566: loss 0.060537\n",
      "batch 1567: loss 0.047342\n",
      "batch 1568: loss 0.105794\n",
      "batch 1569: loss 0.264349\n",
      "batch 1570: loss 0.249156\n",
      "batch 1571: loss 0.061639\n",
      "batch 1572: loss 0.105496\n",
      "batch 1573: loss 0.104724\n",
      "batch 1574: loss 0.210904\n",
      "batch 1575: loss 0.090768\n",
      "batch 1576: loss 0.067682\n",
      "batch 1577: loss 0.284074\n",
      "batch 1578: loss 0.089233\n",
      "batch 1579: loss 0.137556\n",
      "batch 1580: loss 0.196642\n",
      "batch 1581: loss 0.334993\n",
      "batch 1582: loss 0.139997\n",
      "batch 1583: loss 0.124995\n",
      "batch 1584: loss 0.101264\n",
      "batch 1585: loss 0.054500\n",
      "batch 1586: loss 0.099455\n",
      "batch 1587: loss 0.134268\n",
      "batch 1588: loss 0.087215\n",
      "batch 1589: loss 0.150068\n",
      "batch 1590: loss 0.079334\n",
      "batch 1591: loss 0.081054\n",
      "batch 1592: loss 0.053718\n",
      "batch 1593: loss 0.069672\n",
      "batch 1594: loss 0.116005\n",
      "batch 1595: loss 0.087036\n",
      "batch 1596: loss 0.123489\n",
      "batch 1597: loss 0.087207\n",
      "batch 1598: loss 0.093217\n",
      "batch 1599: loss 0.061049\n",
      "batch 1600: loss 0.086054\n",
      "batch 1601: loss 0.105457\n",
      "batch 1602: loss 0.232539\n",
      "batch 1603: loss 0.131176\n",
      "batch 1604: loss 0.235990\n",
      "batch 1605: loss 0.028912\n",
      "batch 1606: loss 0.134411\n",
      "batch 1607: loss 0.161205\n",
      "batch 1608: loss 0.248251\n",
      "batch 1609: loss 0.094193\n",
      "batch 1610: loss 0.108710\n",
      "batch 1611: loss 0.259979\n",
      "batch 1612: loss 0.140467\n",
      "batch 1613: loss 0.087128\n",
      "batch 1614: loss 0.034043\n",
      "batch 1615: loss 0.169966\n",
      "batch 1616: loss 0.129041\n",
      "batch 1617: loss 0.147115\n",
      "batch 1618: loss 0.271665\n",
      "batch 1619: loss 0.147304\n",
      "batch 1620: loss 0.096311\n",
      "batch 1621: loss 0.060157\n",
      "batch 1622: loss 0.135302\n",
      "batch 1623: loss 0.164624\n",
      "batch 1624: loss 0.043325\n",
      "batch 1625: loss 0.100005\n",
      "batch 1626: loss 0.140937\n",
      "batch 1627: loss 0.077139\n",
      "batch 1628: loss 0.229278\n",
      "batch 1629: loss 0.216974\n",
      "batch 1630: loss 0.136867\n",
      "batch 1631: loss 0.173123\n",
      "batch 1632: loss 0.089629\n",
      "batch 1633: loss 0.041021\n",
      "batch 1634: loss 0.091996\n",
      "batch 1635: loss 0.157715\n",
      "batch 1636: loss 0.209884\n",
      "batch 1637: loss 0.095268\n",
      "batch 1638: loss 0.227523\n",
      "batch 1639: loss 0.055447\n",
      "batch 1640: loss 0.239003\n",
      "batch 1641: loss 0.038515\n",
      "batch 1642: loss 0.083218\n",
      "batch 1643: loss 0.126371\n",
      "batch 1644: loss 0.157072\n",
      "batch 1645: loss 0.213984\n",
      "batch 1646: loss 0.050337\n",
      "batch 1647: loss 0.143912\n",
      "batch 1648: loss 0.082359\n",
      "batch 1649: loss 0.143638\n",
      "batch 1650: loss 0.074575\n",
      "batch 1651: loss 0.128540\n",
      "batch 1652: loss 0.175516\n",
      "batch 1653: loss 0.180073\n",
      "batch 1654: loss 0.179454\n",
      "batch 1655: loss 0.137097\n",
      "batch 1656: loss 0.093132\n",
      "batch 1657: loss 0.168965\n",
      "batch 1658: loss 0.096634\n",
      "batch 1659: loss 0.052002\n",
      "batch 1660: loss 0.148799\n",
      "batch 1661: loss 0.096058\n",
      "batch 1662: loss 0.068502\n",
      "batch 1663: loss 0.116024\n",
      "batch 1664: loss 0.079870\n",
      "batch 1665: loss 0.236433\n",
      "batch 1666: loss 0.180257\n",
      "batch 1667: loss 0.128528\n",
      "batch 1668: loss 0.143008\n",
      "batch 1669: loss 0.289832\n",
      "batch 1670: loss 0.080083\n",
      "batch 1671: loss 0.117033\n",
      "batch 1672: loss 0.119761\n",
      "batch 1673: loss 0.054399\n",
      "batch 1674: loss 0.199344\n",
      "batch 1675: loss 0.137180\n",
      "batch 1676: loss 0.113069\n",
      "batch 1677: loss 0.069146\n",
      "batch 1678: loss 0.073714\n",
      "batch 1679: loss 0.124832\n",
      "batch 1680: loss 0.128678\n",
      "batch 1681: loss 0.089178\n",
      "batch 1682: loss 0.150472\n",
      "batch 1683: loss 0.183394\n",
      "batch 1684: loss 0.218249\n",
      "batch 1685: loss 0.094091\n",
      "batch 1686: loss 0.063886\n",
      "batch 1687: loss 0.073381\n",
      "batch 1688: loss 0.085363\n",
      "batch 1689: loss 0.227926\n",
      "batch 1690: loss 0.125227\n",
      "batch 1691: loss 0.144311\n",
      "batch 1692: loss 0.215355\n",
      "batch 1693: loss 0.147556\n",
      "batch 1694: loss 0.252117\n",
      "batch 1695: loss 0.038709\n",
      "batch 1696: loss 0.020018\n",
      "batch 1697: loss 0.277109\n",
      "batch 1698: loss 0.157150\n",
      "batch 1699: loss 0.272215\n",
      "batch 1700: loss 0.043110\n",
      "batch 1701: loss 0.084770\n",
      "batch 1702: loss 0.152214\n",
      "batch 1703: loss 0.135764\n",
      "batch 1704: loss 0.070854\n",
      "batch 1705: loss 0.085196\n",
      "batch 1706: loss 0.187095\n",
      "batch 1707: loss 0.044165\n",
      "batch 1708: loss 0.071859\n",
      "batch 1709: loss 0.220413\n",
      "batch 1710: loss 0.043779\n",
      "batch 1711: loss 0.211576\n",
      "batch 1712: loss 0.071317\n",
      "batch 1713: loss 0.358357\n",
      "batch 1714: loss 0.117257\n",
      "batch 1715: loss 0.082313\n",
      "batch 1716: loss 0.125677\n",
      "batch 1717: loss 0.101292\n",
      "batch 1718: loss 0.062743\n",
      "batch 1719: loss 0.150087\n",
      "batch 1720: loss 0.085660\n",
      "batch 1721: loss 0.339742\n",
      "batch 1722: loss 0.024256\n",
      "batch 1723: loss 0.017647\n",
      "batch 1724: loss 0.135625\n",
      "batch 1725: loss 0.041073\n",
      "batch 1726: loss 0.041631\n",
      "batch 1727: loss 0.062463\n",
      "batch 1728: loss 0.053551\n",
      "batch 1729: loss 0.074385\n",
      "batch 1730: loss 0.078928\n",
      "batch 1731: loss 0.169801\n",
      "batch 1732: loss 0.155761\n",
      "batch 1733: loss 0.146214\n",
      "batch 1734: loss 0.099814\n",
      "batch 1735: loss 0.150414\n",
      "batch 1736: loss 0.129090\n",
      "batch 1737: loss 0.139767\n",
      "batch 1738: loss 0.072515\n",
      "batch 1739: loss 0.073436\n",
      "batch 1740: loss 0.085911\n",
      "batch 1741: loss 0.078960\n",
      "batch 1742: loss 0.033322\n",
      "batch 1743: loss 0.147050\n",
      "batch 1744: loss 0.052722\n",
      "batch 1745: loss 0.026187\n",
      "batch 1746: loss 0.124415\n",
      "batch 1747: loss 0.118098\n",
      "batch 1748: loss 0.052482\n",
      "batch 1749: loss 0.044115\n",
      "batch 1750: loss 0.075344\n",
      "batch 1751: loss 0.175479\n",
      "batch 1752: loss 0.067461\n",
      "batch 1753: loss 0.217859\n",
      "batch 1754: loss 0.099529\n",
      "batch 1755: loss 0.085210\n",
      "batch 1756: loss 0.256385\n",
      "batch 1757: loss 0.155310\n",
      "batch 1758: loss 0.056158\n",
      "batch 1759: loss 0.172526\n",
      "batch 1760: loss 0.065587\n",
      "batch 1761: loss 0.087357\n",
      "batch 1762: loss 0.068403\n",
      "batch 1763: loss 0.068178\n",
      "batch 1764: loss 0.465457\n",
      "batch 1765: loss 0.085281\n",
      "batch 1766: loss 0.052983\n",
      "batch 1767: loss 0.121784\n",
      "batch 1768: loss 0.383561\n",
      "batch 1769: loss 0.077679\n",
      "batch 1770: loss 0.092827\n",
      "batch 1771: loss 0.053358\n",
      "batch 1772: loss 0.095952\n",
      "batch 1773: loss 0.079265\n",
      "batch 1774: loss 0.079119\n",
      "batch 1775: loss 0.174917\n",
      "batch 1776: loss 0.060985\n",
      "batch 1777: loss 0.278692\n",
      "batch 1778: loss 0.187267\n",
      "batch 1779: loss 0.095622\n",
      "batch 1780: loss 0.040162\n",
      "batch 1781: loss 0.134211\n",
      "batch 1782: loss 0.163831\n",
      "batch 1783: loss 0.101186\n",
      "batch 1784: loss 0.114660\n",
      "batch 1785: loss 0.086408\n",
      "batch 1786: loss 0.057648\n",
      "batch 1787: loss 0.108081\n",
      "batch 1788: loss 0.115583\n",
      "batch 1789: loss 0.110660\n",
      "batch 1790: loss 0.037604\n",
      "batch 1791: loss 0.143440\n",
      "batch 1792: loss 0.055574\n",
      "batch 1793: loss 0.068695\n",
      "batch 1794: loss 0.303542\n",
      "batch 1795: loss 0.081973\n",
      "batch 1796: loss 0.112920\n",
      "batch 1797: loss 0.064742\n",
      "batch 1798: loss 0.262440\n",
      "batch 1799: loss 0.024220\n",
      "batch 1800: loss 0.106712\n",
      "batch 1801: loss 0.025320\n",
      "batch 1802: loss 0.194121\n",
      "batch 1803: loss 0.062282\n",
      "batch 1804: loss 0.116042\n",
      "batch 1805: loss 0.082500\n",
      "batch 1806: loss 0.070850\n",
      "batch 1807: loss 0.039875\n",
      "batch 1808: loss 0.132593\n",
      "batch 1809: loss 0.192209\n",
      "batch 1810: loss 0.169712\n",
      "batch 1811: loss 0.028593\n",
      "batch 1812: loss 0.085266\n",
      "batch 1813: loss 0.128673\n",
      "batch 1814: loss 0.234753\n",
      "batch 1815: loss 0.299984\n",
      "batch 1816: loss 0.171369\n",
      "batch 1817: loss 0.143047\n",
      "batch 1818: loss 0.179841\n",
      "batch 1819: loss 0.098746\n",
      "batch 1820: loss 0.036398\n",
      "batch 1821: loss 0.078512\n",
      "batch 1822: loss 0.126345\n",
      "batch 1823: loss 0.271415\n",
      "batch 1824: loss 0.167656\n",
      "batch 1825: loss 0.218399\n",
      "batch 1826: loss 0.059789\n",
      "batch 1827: loss 0.049812\n",
      "batch 1828: loss 0.465134\n",
      "batch 1829: loss 0.175432\n",
      "batch 1830: loss 0.139079\n",
      "batch 1831: loss 0.131170\n",
      "batch 1832: loss 0.182286\n",
      "batch 1833: loss 0.128125\n",
      "batch 1834: loss 0.042599\n",
      "batch 1835: loss 0.078457\n",
      "batch 1836: loss 0.100952\n",
      "batch 1837: loss 0.124442\n",
      "batch 1838: loss 0.237510\n",
      "batch 1839: loss 0.153321\n",
      "batch 1840: loss 0.137417\n",
      "batch 1841: loss 0.266977\n",
      "batch 1842: loss 0.200457\n",
      "batch 1843: loss 0.241868\n",
      "batch 1844: loss 0.076653\n",
      "batch 1845: loss 0.098974\n",
      "batch 1846: loss 0.125448\n",
      "batch 1847: loss 0.218492\n",
      "batch 1848: loss 0.091965\n",
      "batch 1849: loss 0.129997\n",
      "batch 1850: loss 0.211198\n",
      "batch 1851: loss 0.094152\n",
      "batch 1852: loss 0.439333\n",
      "batch 1853: loss 0.019132\n",
      "batch 1854: loss 0.202976\n",
      "batch 1855: loss 0.071216\n",
      "batch 1856: loss 0.123950\n",
      "batch 1857: loss 0.035847\n",
      "batch 1858: loss 0.073839\n",
      "batch 1859: loss 0.154980\n",
      "batch 1860: loss 0.161102\n",
      "batch 1861: loss 0.232486\n",
      "batch 1862: loss 0.079745\n",
      "batch 1863: loss 0.040503\n",
      "batch 1864: loss 0.039852\n",
      "batch 1865: loss 0.204871\n",
      "batch 1866: loss 0.318419\n",
      "batch 1867: loss 0.171577\n",
      "batch 1868: loss 0.105875\n",
      "batch 1869: loss 0.067136\n",
      "batch 1870: loss 0.229143\n",
      "batch 1871: loss 0.087899\n",
      "batch 1872: loss 0.183569\n",
      "batch 1873: loss 0.251773\n",
      "batch 1874: loss 0.039740\n",
      "batch 1875: loss 0.173371\n",
      "batch 1876: loss 0.078589\n",
      "batch 1877: loss 0.059324\n",
      "batch 1878: loss 0.114373\n",
      "batch 1879: loss 0.037070\n",
      "batch 1880: loss 0.355354\n",
      "batch 1881: loss 0.232840\n",
      "batch 1882: loss 0.116765\n",
      "batch 1883: loss 0.065809\n",
      "batch 1884: loss 0.190973\n",
      "batch 1885: loss 0.069069\n",
      "batch 1886: loss 0.109431\n",
      "batch 1887: loss 0.330846\n",
      "batch 1888: loss 0.115449\n",
      "batch 1889: loss 0.215955\n",
      "batch 1890: loss 0.151412\n",
      "batch 1891: loss 0.027201\n",
      "batch 1892: loss 0.074741\n",
      "batch 1893: loss 0.100240\n",
      "batch 1894: loss 0.023627\n",
      "batch 1895: loss 0.176732\n",
      "batch 1896: loss 0.210444\n",
      "batch 1897: loss 0.046058\n",
      "batch 1898: loss 0.106791\n",
      "batch 1899: loss 0.092752\n",
      "batch 1900: loss 0.147311\n",
      "batch 1901: loss 0.078999\n",
      "batch 1902: loss 0.031740\n",
      "batch 1903: loss 0.064928\n",
      "batch 1904: loss 0.143051\n",
      "batch 1905: loss 0.099044\n",
      "batch 1906: loss 0.142477\n",
      "batch 1907: loss 0.158349\n",
      "batch 1908: loss 0.101024\n",
      "batch 1909: loss 0.185137\n",
      "batch 1910: loss 0.102753\n",
      "batch 1911: loss 0.043236\n",
      "batch 1912: loss 0.131847\n",
      "batch 1913: loss 0.123458\n",
      "batch 1914: loss 0.084669\n",
      "batch 1915: loss 0.113142\n",
      "batch 1916: loss 0.111242\n",
      "batch 1917: loss 0.028207\n",
      "batch 1918: loss 0.112255\n",
      "batch 1919: loss 0.318269\n",
      "batch 1920: loss 0.097272\n",
      "batch 1921: loss 0.072599\n",
      "batch 1922: loss 0.103996\n",
      "batch 1923: loss 0.109976\n",
      "batch 1924: loss 0.048171\n",
      "batch 1925: loss 0.245951\n",
      "batch 1926: loss 0.063951\n",
      "batch 1927: loss 0.118928\n",
      "batch 1928: loss 0.083921\n",
      "batch 1929: loss 0.386150\n",
      "batch 1930: loss 0.109674\n",
      "batch 1931: loss 0.105463\n",
      "batch 1932: loss 0.073519\n",
      "batch 1933: loss 0.083049\n",
      "batch 1934: loss 0.177872\n",
      "batch 1935: loss 0.065748\n",
      "batch 1936: loss 0.281728\n",
      "batch 1937: loss 0.053664\n",
      "batch 1938: loss 0.070329\n",
      "batch 1939: loss 0.126952\n",
      "batch 1940: loss 0.271258\n",
      "batch 1941: loss 0.335292\n",
      "batch 1942: loss 0.055489\n",
      "batch 1943: loss 0.073462\n",
      "batch 1944: loss 0.134226\n",
      "batch 1945: loss 0.115925\n",
      "batch 1946: loss 0.028763\n",
      "batch 1947: loss 0.116351\n",
      "batch 1948: loss 0.061286\n",
      "batch 1949: loss 0.265451\n",
      "batch 1950: loss 0.037096\n",
      "batch 1951: loss 0.060239\n",
      "batch 1952: loss 0.096287\n",
      "batch 1953: loss 0.060351\n",
      "batch 1954: loss 0.061311\n",
      "batch 1955: loss 0.063315\n",
      "batch 1956: loss 0.045767\n",
      "batch 1957: loss 0.145037\n",
      "batch 1958: loss 0.070716\n",
      "batch 1959: loss 0.043350\n",
      "batch 1960: loss 0.150925\n",
      "batch 1961: loss 0.150627\n",
      "batch 1962: loss 0.045897\n",
      "batch 1963: loss 0.434424\n",
      "batch 1964: loss 0.267895\n",
      "batch 1965: loss 0.167570\n",
      "batch 1966: loss 0.057849\n",
      "batch 1967: loss 0.062612\n",
      "batch 1968: loss 0.049195\n",
      "batch 1969: loss 0.095497\n",
      "batch 1970: loss 0.154142\n",
      "batch 1971: loss 0.066963\n",
      "batch 1972: loss 0.163044\n",
      "batch 1973: loss 0.152959\n",
      "batch 1974: loss 0.058393\n",
      "batch 1975: loss 0.059805\n",
      "batch 1976: loss 0.078287\n",
      "batch 1977: loss 0.229424\n",
      "batch 1978: loss 0.200922\n",
      "batch 1979: loss 0.082889\n",
      "batch 1980: loss 0.161691\n",
      "batch 1981: loss 0.082867\n",
      "batch 1982: loss 0.078912\n",
      "batch 1983: loss 0.200977\n",
      "batch 1984: loss 0.052062\n",
      "batch 1985: loss 0.044344\n",
      "batch 1986: loss 0.053606\n",
      "batch 1987: loss 0.118963\n",
      "batch 1988: loss 0.061326\n",
      "batch 1989: loss 0.111878\n",
      "batch 1990: loss 0.058658\n",
      "batch 1991: loss 0.105421\n",
      "batch 1992: loss 0.146692\n",
      "batch 1993: loss 0.063410\n",
      "batch 1994: loss 0.175629\n",
      "batch 1995: loss 0.193998\n",
      "batch 1996: loss 0.033860\n",
      "batch 1997: loss 0.075278\n",
      "batch 1998: loss 0.109603\n",
      "batch 1999: loss 0.044419\n",
      "batch 2000: loss 0.073645\n",
      "batch 2001: loss 0.058940\n",
      "batch 2002: loss 0.106078\n",
      "batch 2003: loss 0.085118\n",
      "batch 2004: loss 0.186105\n",
      "batch 2005: loss 0.063007\n",
      "batch 2006: loss 0.060707\n",
      "batch 2007: loss 0.114253\n",
      "batch 2008: loss 0.380479\n",
      "batch 2009: loss 0.052827\n",
      "batch 2010: loss 0.054095\n",
      "batch 2011: loss 0.176898\n",
      "batch 2012: loss 0.208249\n",
      "batch 2013: loss 0.177418\n",
      "batch 2014: loss 0.189604\n",
      "batch 2015: loss 0.183428\n",
      "batch 2016: loss 0.096854\n",
      "batch 2017: loss 0.170670\n",
      "batch 2018: loss 0.235573\n",
      "batch 2019: loss 0.076632\n",
      "batch 2020: loss 0.142635\n",
      "batch 2021: loss 0.105341\n",
      "batch 2022: loss 0.073487\n",
      "batch 2023: loss 0.277335\n",
      "batch 2024: loss 0.197165\n",
      "batch 2025: loss 0.220954\n",
      "batch 2026: loss 0.136011\n",
      "batch 2027: loss 0.283077\n",
      "batch 2028: loss 0.091400\n",
      "batch 2029: loss 0.169487\n",
      "batch 2030: loss 0.089308\n",
      "batch 2031: loss 0.117732\n",
      "batch 2032: loss 0.047748\n",
      "batch 2033: loss 0.107140\n",
      "batch 2034: loss 0.106049\n",
      "batch 2035: loss 0.077763\n",
      "batch 2036: loss 0.081979\n",
      "batch 2037: loss 0.123725\n",
      "batch 2038: loss 0.115648\n",
      "batch 2039: loss 0.069094\n",
      "batch 2040: loss 0.186858\n",
      "batch 2041: loss 0.029351\n",
      "batch 2042: loss 0.120368\n",
      "batch 2043: loss 0.057230\n",
      "batch 2044: loss 0.145820\n",
      "batch 2045: loss 0.155338\n",
      "batch 2046: loss 0.039200\n",
      "batch 2047: loss 0.056229\n",
      "batch 2048: loss 0.109935\n",
      "batch 2049: loss 0.091847\n",
      "batch 2050: loss 0.229371\n",
      "batch 2051: loss 0.198730\n",
      "batch 2052: loss 0.159382\n",
      "batch 2053: loss 0.039371\n",
      "batch 2054: loss 0.107078\n",
      "batch 2055: loss 0.101390\n",
      "batch 2056: loss 0.116822\n",
      "batch 2057: loss 0.132441\n",
      "batch 2058: loss 0.046274\n",
      "batch 2059: loss 0.046470\n",
      "batch 2060: loss 0.158330\n",
      "batch 2061: loss 0.032468\n",
      "batch 2062: loss 0.053011\n",
      "batch 2063: loss 0.284353\n",
      "batch 2064: loss 0.089066\n",
      "batch 2065: loss 0.130240\n",
      "batch 2066: loss 0.132332\n",
      "batch 2067: loss 0.144680\n",
      "batch 2068: loss 0.244862\n",
      "batch 2069: loss 0.077758\n",
      "batch 2070: loss 0.306110\n",
      "batch 2071: loss 0.160059\n",
      "batch 2072: loss 0.087382\n",
      "batch 2073: loss 0.074894\n",
      "batch 2074: loss 0.117483\n",
      "batch 2075: loss 0.045611\n",
      "batch 2076: loss 0.091341\n",
      "batch 2077: loss 0.113583\n",
      "batch 2078: loss 0.027583\n",
      "batch 2079: loss 0.178464\n",
      "batch 2080: loss 0.186362\n",
      "batch 2081: loss 0.115842\n",
      "batch 2082: loss 0.187912\n",
      "batch 2083: loss 0.129787\n",
      "batch 2084: loss 0.235667\n",
      "batch 2085: loss 0.138975\n",
      "batch 2086: loss 0.053694\n",
      "batch 2087: loss 0.064967\n",
      "batch 2088: loss 0.178525\n",
      "batch 2089: loss 0.155845\n",
      "batch 2090: loss 0.097923\n",
      "batch 2091: loss 0.019056\n",
      "batch 2092: loss 0.158660\n",
      "batch 2093: loss 0.215686\n",
      "batch 2094: loss 0.084138\n",
      "batch 2095: loss 0.205550\n",
      "batch 2096: loss 0.029630\n",
      "batch 2097: loss 0.096785\n",
      "batch 2098: loss 0.055205\n",
      "batch 2099: loss 0.093987\n",
      "batch 2100: loss 0.053147\n",
      "batch 2101: loss 0.193533\n",
      "batch 2102: loss 0.114008\n",
      "batch 2103: loss 0.122101\n",
      "batch 2104: loss 0.143655\n",
      "batch 2105: loss 0.120514\n",
      "batch 2106: loss 0.049170\n",
      "batch 2107: loss 0.085933\n",
      "batch 2108: loss 0.049589\n",
      "batch 2109: loss 0.213260\n",
      "batch 2110: loss 0.192339\n",
      "batch 2111: loss 0.088604\n",
      "batch 2112: loss 0.038566\n",
      "batch 2113: loss 0.158809\n",
      "batch 2114: loss 0.102784\n",
      "batch 2115: loss 0.228345\n",
      "batch 2116: loss 0.054154\n",
      "batch 2117: loss 0.081237\n",
      "batch 2118: loss 0.137049\n",
      "batch 2119: loss 0.132272\n",
      "batch 2120: loss 0.118317\n",
      "batch 2121: loss 0.108422\n",
      "batch 2122: loss 0.175823\n",
      "batch 2123: loss 0.081755\n",
      "batch 2124: loss 0.103247\n",
      "batch 2125: loss 0.079676\n",
      "batch 2126: loss 0.103473\n",
      "batch 2127: loss 0.057916\n",
      "batch 2128: loss 0.181958\n",
      "batch 2129: loss 0.060039\n",
      "batch 2130: loss 0.158527\n",
      "batch 2131: loss 0.092191\n",
      "batch 2132: loss 0.157162\n",
      "batch 2133: loss 0.048290\n",
      "batch 2134: loss 0.134593\n",
      "batch 2135: loss 0.083124\n",
      "batch 2136: loss 0.069882\n",
      "batch 2137: loss 0.066103\n",
      "batch 2138: loss 0.218279\n",
      "batch 2139: loss 0.120266\n",
      "batch 2140: loss 0.194268\n",
      "batch 2141: loss 0.066635\n",
      "batch 2142: loss 0.127403\n",
      "batch 2143: loss 0.030767\n",
      "batch 2144: loss 0.080449\n",
      "batch 2145: loss 0.033563\n",
      "batch 2146: loss 0.216479\n",
      "batch 2147: loss 0.104591\n",
      "batch 2148: loss 0.044235\n",
      "batch 2149: loss 0.159609\n",
      "batch 2150: loss 0.052755\n",
      "batch 2151: loss 0.094397\n",
      "batch 2152: loss 0.101063\n",
      "batch 2153: loss 0.133509\n",
      "batch 2154: loss 0.162417\n",
      "batch 2155: loss 0.141732\n",
      "batch 2156: loss 0.137928\n",
      "batch 2157: loss 0.149342\n",
      "batch 2158: loss 0.102156\n",
      "batch 2159: loss 0.149008\n",
      "batch 2160: loss 0.131024\n",
      "batch 2161: loss 0.221680\n",
      "batch 2162: loss 0.135543\n",
      "batch 2163: loss 0.201972\n",
      "batch 2164: loss 0.239249\n",
      "batch 2165: loss 0.298625\n",
      "batch 2166: loss 0.165689\n",
      "batch 2167: loss 0.143052\n",
      "batch 2168: loss 0.226650\n",
      "batch 2169: loss 0.192861\n",
      "batch 2170: loss 0.096385\n",
      "batch 2171: loss 0.204743\n",
      "batch 2172: loss 0.095635\n",
      "batch 2173: loss 0.223311\n",
      "batch 2174: loss 0.192125\n",
      "batch 2175: loss 0.062629\n",
      "batch 2176: loss 0.035489\n",
      "batch 2177: loss 0.043142\n",
      "batch 2178: loss 0.051471\n",
      "batch 2179: loss 0.068656\n",
      "batch 2180: loss 0.074109\n",
      "batch 2181: loss 0.054334\n",
      "batch 2182: loss 0.024507\n",
      "batch 2183: loss 0.179483\n",
      "batch 2184: loss 0.106593\n",
      "batch 2185: loss 0.096914\n",
      "batch 2186: loss 0.058529\n",
      "batch 2187: loss 0.087364\n",
      "batch 2188: loss 0.041417\n",
      "batch 2189: loss 0.153619\n",
      "batch 2190: loss 0.045503\n",
      "batch 2191: loss 0.051231\n",
      "batch 2192: loss 0.060992\n",
      "batch 2193: loss 0.094874\n",
      "batch 2194: loss 0.096245\n",
      "batch 2195: loss 0.059969\n",
      "batch 2196: loss 0.045626\n",
      "batch 2197: loss 0.062791\n",
      "batch 2198: loss 0.112339\n",
      "batch 2199: loss 0.143232\n",
      "batch 2200: loss 0.060633\n",
      "batch 2201: loss 0.160515\n",
      "batch 2202: loss 0.033101\n",
      "batch 2203: loss 0.167421\n",
      "batch 2204: loss 0.021366\n",
      "batch 2205: loss 0.227186\n",
      "batch 2206: loss 0.070565\n",
      "batch 2207: loss 0.055044\n",
      "batch 2208: loss 0.046563\n",
      "batch 2209: loss 0.027791\n",
      "batch 2210: loss 0.071320\n",
      "batch 2211: loss 0.043522\n",
      "batch 2212: loss 0.191718\n",
      "batch 2213: loss 0.065796\n",
      "batch 2214: loss 0.051802\n",
      "batch 2215: loss 0.105388\n",
      "batch 2216: loss 0.194727\n",
      "batch 2217: loss 0.070864\n",
      "batch 2218: loss 0.059262\n",
      "batch 2219: loss 0.059085\n",
      "batch 2220: loss 0.214426\n",
      "batch 2221: loss 0.170567\n",
      "batch 2222: loss 0.052356\n",
      "batch 2223: loss 0.042481\n",
      "batch 2224: loss 0.035289\n",
      "batch 2225: loss 0.320253\n",
      "batch 2226: loss 0.048469\n",
      "batch 2227: loss 0.060552\n",
      "batch 2228: loss 0.124684\n",
      "batch 2229: loss 0.070595\n",
      "batch 2230: loss 0.195898\n",
      "batch 2231: loss 0.058690\n",
      "batch 2232: loss 0.122807\n",
      "batch 2233: loss 0.045700\n",
      "batch 2234: loss 0.110945\n",
      "batch 2235: loss 0.164723\n",
      "batch 2236: loss 0.048644\n",
      "batch 2237: loss 0.118694\n",
      "batch 2238: loss 0.078183\n",
      "batch 2239: loss 0.056832\n",
      "batch 2240: loss 0.095252\n",
      "batch 2241: loss 0.107424\n",
      "batch 2242: loss 0.159878\n",
      "batch 2243: loss 0.079511\n",
      "batch 2244: loss 0.102482\n",
      "batch 2245: loss 0.019247\n",
      "batch 2246: loss 0.016949\n",
      "batch 2247: loss 0.051656\n",
      "batch 2248: loss 0.255110\n",
      "batch 2249: loss 0.139411\n",
      "batch 2250: loss 0.054206\n",
      "batch 2251: loss 0.049765\n",
      "batch 2252: loss 0.142247\n",
      "batch 2253: loss 0.240347\n",
      "batch 2254: loss 0.039001\n",
      "batch 2255: loss 0.089140\n",
      "batch 2256: loss 0.085650\n",
      "batch 2257: loss 0.192356\n",
      "batch 2258: loss 0.015520\n",
      "batch 2259: loss 0.123178\n",
      "batch 2260: loss 0.075609\n",
      "batch 2261: loss 0.369262\n",
      "batch 2262: loss 0.090573\n",
      "batch 2263: loss 0.048692\n",
      "batch 2264: loss 0.024929\n",
      "batch 2265: loss 0.024439\n",
      "batch 2266: loss 0.181127\n",
      "batch 2267: loss 0.057405\n",
      "batch 2268: loss 0.065943\n",
      "batch 2269: loss 0.035184\n",
      "batch 2270: loss 0.041181\n",
      "batch 2271: loss 0.081801\n",
      "batch 2272: loss 0.079073\n",
      "batch 2273: loss 0.117872\n",
      "batch 2274: loss 0.041387\n",
      "batch 2275: loss 0.022012\n",
      "batch 2276: loss 0.103273\n",
      "batch 2277: loss 0.091251\n",
      "batch 2278: loss 0.042224\n",
      "batch 2279: loss 0.071867\n",
      "batch 2280: loss 0.084688\n",
      "batch 2281: loss 0.057359\n",
      "batch 2282: loss 0.043662\n",
      "batch 2283: loss 0.299628\n",
      "batch 2284: loss 0.110512\n",
      "batch 2285: loss 0.034201\n",
      "batch 2286: loss 0.033334\n",
      "batch 2287: loss 0.055796\n",
      "batch 2288: loss 0.058407\n",
      "batch 2289: loss 0.140393\n",
      "batch 2290: loss 0.109328\n",
      "batch 2291: loss 0.147542\n",
      "batch 2292: loss 0.060577\n",
      "batch 2293: loss 0.082466\n",
      "batch 2294: loss 0.088395\n",
      "batch 2295: loss 0.154978\n",
      "batch 2296: loss 0.079303\n",
      "batch 2297: loss 0.032561\n",
      "batch 2298: loss 0.119781\n",
      "batch 2299: loss 0.124251\n",
      "batch 2300: loss 0.287254\n",
      "batch 2301: loss 0.178642\n",
      "batch 2302: loss 0.106975\n",
      "batch 2303: loss 0.054969\n",
      "batch 2304: loss 0.193460\n",
      "batch 2305: loss 0.161707\n",
      "batch 2306: loss 0.110420\n",
      "batch 2307: loss 0.081394\n",
      "batch 2308: loss 0.266113\n",
      "batch 2309: loss 0.089674\n",
      "batch 2310: loss 0.042079\n",
      "batch 2311: loss 0.077425\n",
      "batch 2312: loss 0.026097\n",
      "batch 2313: loss 0.028001\n",
      "batch 2314: loss 0.226251\n",
      "batch 2315: loss 0.166468\n",
      "batch 2316: loss 0.190929\n",
      "batch 2317: loss 0.180976\n",
      "batch 2318: loss 0.050624\n",
      "batch 2319: loss 0.047997\n",
      "batch 2320: loss 0.190672\n",
      "batch 2321: loss 0.209513\n",
      "batch 2322: loss 0.085700\n",
      "batch 2323: loss 0.055508\n",
      "batch 2324: loss 0.072772\n",
      "batch 2325: loss 0.098538\n",
      "batch 2326: loss 0.105673\n",
      "batch 2327: loss 0.170886\n",
      "batch 2328: loss 0.159578\n",
      "batch 2329: loss 0.136203\n",
      "batch 2330: loss 0.114457\n",
      "batch 2331: loss 0.101783\n",
      "batch 2332: loss 0.089999\n",
      "batch 2333: loss 0.046276\n",
      "batch 2334: loss 0.091392\n",
      "batch 2335: loss 0.149071\n",
      "batch 2336: loss 0.061109\n",
      "batch 2337: loss 0.084817\n",
      "batch 2338: loss 0.126410\n",
      "batch 2339: loss 0.069648\n",
      "batch 2340: loss 0.049997\n",
      "batch 2341: loss 0.038747\n",
      "batch 2342: loss 0.078268\n",
      "batch 2343: loss 0.079800\n",
      "batch 2344: loss 0.089688\n",
      "batch 2345: loss 0.108790\n",
      "batch 2346: loss 0.114798\n",
      "batch 2347: loss 0.119867\n",
      "batch 2348: loss 0.126899\n",
      "batch 2349: loss 0.169325\n",
      "batch 2350: loss 0.043180\n",
      "batch 2351: loss 0.032722\n",
      "batch 2352: loss 0.073583\n",
      "batch 2353: loss 0.066128\n",
      "batch 2354: loss 0.114755\n",
      "batch 2355: loss 0.107700\n",
      "batch 2356: loss 0.064075\n",
      "batch 2357: loss 0.270205\n",
      "batch 2358: loss 0.164235\n",
      "batch 2359: loss 0.154171\n",
      "batch 2360: loss 0.086406\n",
      "batch 2361: loss 0.065053\n",
      "batch 2362: loss 0.084923\n",
      "batch 2363: loss 0.067038\n",
      "batch 2364: loss 0.047981\n",
      "batch 2365: loss 0.083591\n",
      "batch 2366: loss 0.195121\n",
      "batch 2367: loss 0.119032\n",
      "batch 2368: loss 0.128557\n",
      "batch 2369: loss 0.107464\n",
      "batch 2370: loss 0.190735\n",
      "batch 2371: loss 0.144042\n",
      "batch 2372: loss 0.126439\n",
      "batch 2373: loss 0.157076\n",
      "batch 2374: loss 0.103331\n",
      "batch 2375: loss 0.161089\n",
      "batch 2376: loss 0.077607\n",
      "batch 2377: loss 0.045711\n",
      "batch 2378: loss 0.088906\n",
      "batch 2379: loss 0.093769\n",
      "batch 2380: loss 0.066073\n",
      "batch 2381: loss 0.033019\n",
      "batch 2382: loss 0.171861\n",
      "batch 2383: loss 0.032772\n",
      "batch 2384: loss 0.181268\n",
      "batch 2385: loss 0.085996\n",
      "batch 2386: loss 0.209662\n",
      "batch 2387: loss 0.195604\n",
      "batch 2388: loss 0.032761\n",
      "batch 2389: loss 0.045327\n",
      "batch 2390: loss 0.104767\n",
      "batch 2391: loss 0.262596\n",
      "batch 2392: loss 0.030421\n",
      "batch 2393: loss 0.211592\n",
      "batch 2394: loss 0.098214\n",
      "batch 2395: loss 0.084756\n",
      "batch 2396: loss 0.176845\n",
      "batch 2397: loss 0.101716\n",
      "batch 2398: loss 0.121506\n",
      "batch 2399: loss 0.057985\n",
      "batch 2400: loss 0.076821\n",
      "batch 2401: loss 0.069721\n",
      "batch 2402: loss 0.059647\n",
      "batch 2403: loss 0.054408\n",
      "batch 2404: loss 0.065229\n",
      "batch 2405: loss 0.058696\n",
      "batch 2406: loss 0.066253\n",
      "batch 2407: loss 0.127043\n",
      "batch 2408: loss 0.128690\n",
      "batch 2409: loss 0.139550\n",
      "batch 2410: loss 0.051745\n",
      "batch 2411: loss 0.097993\n",
      "batch 2412: loss 0.094186\n",
      "batch 2413: loss 0.045960\n",
      "batch 2414: loss 0.146170\n",
      "batch 2415: loss 0.185857\n",
      "batch 2416: loss 0.095532\n",
      "batch 2417: loss 0.094825\n",
      "batch 2418: loss 0.082937\n",
      "batch 2419: loss 0.107273\n",
      "batch 2420: loss 0.072554\n",
      "batch 2421: loss 0.131404\n",
      "batch 2422: loss 0.260332\n",
      "batch 2423: loss 0.074175\n",
      "batch 2424: loss 0.145560\n",
      "batch 2425: loss 0.125311\n",
      "batch 2426: loss 0.027871\n",
      "batch 2427: loss 0.035419\n",
      "batch 2428: loss 0.165968\n",
      "batch 2429: loss 0.194645\n",
      "batch 2430: loss 0.125127\n",
      "batch 2431: loss 0.111442\n",
      "batch 2432: loss 0.095039\n",
      "batch 2433: loss 0.107692\n",
      "batch 2434: loss 0.022880\n",
      "batch 2435: loss 0.106748\n",
      "batch 2436: loss 0.028967\n",
      "batch 2437: loss 0.119809\n",
      "batch 2438: loss 0.078317\n",
      "batch 2439: loss 0.288552\n",
      "batch 2440: loss 0.046085\n",
      "batch 2441: loss 0.034672\n",
      "batch 2442: loss 0.094043\n",
      "batch 2443: loss 0.129420\n",
      "batch 2444: loss 0.224788\n",
      "batch 2445: loss 0.038446\n",
      "batch 2446: loss 0.084155\n",
      "batch 2447: loss 0.049109\n",
      "batch 2448: loss 0.053148\n",
      "batch 2449: loss 0.060230\n",
      "batch 2450: loss 0.129668\n",
      "batch 2451: loss 0.046569\n",
      "batch 2452: loss 0.032266\n",
      "batch 2453: loss 0.197275\n",
      "batch 2454: loss 0.031834\n",
      "batch 2455: loss 0.067543\n",
      "batch 2456: loss 0.037124\n",
      "batch 2457: loss 0.068298\n",
      "batch 2458: loss 0.066090\n",
      "batch 2459: loss 0.120906\n",
      "batch 2460: loss 0.046255\n",
      "batch 2461: loss 0.070626\n",
      "batch 2462: loss 0.072723\n",
      "batch 2463: loss 0.070348\n",
      "batch 2464: loss 0.325361\n",
      "batch 2465: loss 0.060483\n",
      "batch 2466: loss 0.114101\n",
      "batch 2467: loss 0.109982\n",
      "batch 2468: loss 0.114021\n",
      "batch 2469: loss 0.024013\n",
      "batch 2470: loss 0.079991\n",
      "batch 2471: loss 0.054486\n",
      "batch 2472: loss 0.047754\n",
      "batch 2473: loss 0.100793\n",
      "batch 2474: loss 0.024345\n",
      "batch 2475: loss 0.141640\n",
      "batch 2476: loss 0.160555\n",
      "batch 2477: loss 0.019088\n",
      "batch 2478: loss 0.051513\n",
      "batch 2479: loss 0.052637\n",
      "batch 2480: loss 0.132163\n",
      "batch 2481: loss 0.083183\n",
      "batch 2482: loss 0.058148\n",
      "batch 2483: loss 0.059990\n",
      "batch 2484: loss 0.057023\n",
      "batch 2485: loss 0.140007\n",
      "batch 2486: loss 0.143018\n",
      "batch 2487: loss 0.179716\n",
      "batch 2488: loss 0.056372\n",
      "batch 2489: loss 0.034030\n",
      "batch 2490: loss 0.083857\n",
      "batch 2491: loss 0.147930\n",
      "batch 2492: loss 0.048903\n",
      "batch 2493: loss 0.257297\n",
      "batch 2494: loss 0.076991\n",
      "batch 2495: loss 0.107651\n",
      "batch 2496: loss 0.171525\n",
      "batch 2497: loss 0.081304\n",
      "batch 2498: loss 0.152966\n",
      "batch 2499: loss 0.044588\n",
      "batch 2500: loss 0.078129\n",
      "batch 2501: loss 0.171814\n",
      "batch 2502: loss 0.124709\n",
      "batch 2503: loss 0.094886\n",
      "batch 2504: loss 0.061122\n",
      "batch 2505: loss 0.166193\n",
      "batch 2506: loss 0.067083\n",
      "batch 2507: loss 0.050592\n",
      "batch 2508: loss 0.052218\n",
      "batch 2509: loss 0.110582\n",
      "batch 2510: loss 0.058666\n",
      "batch 2511: loss 0.205351\n",
      "batch 2512: loss 0.074352\n",
      "batch 2513: loss 0.064356\n",
      "batch 2514: loss 0.178568\n",
      "batch 2515: loss 0.163406\n",
      "batch 2516: loss 0.062393\n",
      "batch 2517: loss 0.057707\n",
      "batch 2518: loss 0.293748\n",
      "batch 2519: loss 0.059400\n",
      "batch 2520: loss 0.097233\n",
      "batch 2521: loss 0.304307\n",
      "batch 2522: loss 0.045193\n",
      "batch 2523: loss 0.078531\n",
      "batch 2524: loss 0.176350\n",
      "batch 2525: loss 0.078454\n",
      "batch 2526: loss 0.032603\n",
      "batch 2527: loss 0.022613\n",
      "batch 2528: loss 0.112593\n",
      "batch 2529: loss 0.108211\n",
      "batch 2530: loss 0.022062\n",
      "batch 2531: loss 0.275109\n",
      "batch 2532: loss 0.073674\n",
      "batch 2533: loss 0.073714\n",
      "batch 2534: loss 0.192311\n",
      "batch 2535: loss 0.043444\n",
      "batch 2536: loss 0.137031\n",
      "batch 2537: loss 0.391322\n",
      "batch 2538: loss 0.200157\n",
      "batch 2539: loss 0.141352\n",
      "batch 2540: loss 0.113977\n",
      "batch 2541: loss 0.091081\n",
      "batch 2542: loss 0.054055\n",
      "batch 2543: loss 0.071299\n",
      "batch 2544: loss 0.238817\n",
      "batch 2545: loss 0.048404\n",
      "batch 2546: loss 0.244549\n",
      "batch 2547: loss 0.020872\n",
      "batch 2548: loss 0.194796\n",
      "batch 2549: loss 0.055086\n",
      "batch 2550: loss 0.066495\n",
      "batch 2551: loss 0.067660\n",
      "batch 2552: loss 0.070955\n",
      "batch 2553: loss 0.068133\n",
      "batch 2554: loss 0.084646\n",
      "batch 2555: loss 0.190661\n",
      "batch 2556: loss 0.142952\n",
      "batch 2557: loss 0.035120\n",
      "batch 2558: loss 0.093588\n",
      "batch 2559: loss 0.065355\n",
      "batch 2560: loss 0.197820\n",
      "batch 2561: loss 0.088316\n",
      "batch 2562: loss 0.058508\n",
      "batch 2563: loss 0.045531\n",
      "batch 2564: loss 0.019180\n",
      "batch 2565: loss 0.108733\n",
      "batch 2566: loss 0.061479\n",
      "batch 2567: loss 0.113137\n",
      "batch 2568: loss 0.030591\n",
      "batch 2569: loss 0.136190\n",
      "batch 2570: loss 0.190556\n",
      "batch 2571: loss 0.172437\n",
      "batch 2572: loss 0.057071\n",
      "batch 2573: loss 0.077933\n",
      "batch 2574: loss 0.106233\n",
      "batch 2575: loss 0.078300\n",
      "batch 2576: loss 0.075548\n",
      "batch 2577: loss 0.036728\n",
      "batch 2578: loss 0.090942\n",
      "batch 2579: loss 0.020521\n",
      "batch 2580: loss 0.035295\n",
      "batch 2581: loss 0.078273\n",
      "batch 2582: loss 0.044210\n",
      "batch 2583: loss 0.028187\n",
      "batch 2584: loss 0.188404\n",
      "batch 2585: loss 0.061900\n",
      "batch 2586: loss 0.016602\n",
      "batch 2587: loss 0.093468\n",
      "batch 2588: loss 0.075798\n",
      "batch 2589: loss 0.042017\n",
      "batch 2590: loss 0.233402\n",
      "batch 2591: loss 0.165244\n",
      "batch 2592: loss 0.031736\n",
      "batch 2593: loss 0.124004\n",
      "batch 2594: loss 0.216235\n",
      "batch 2595: loss 0.211762\n",
      "batch 2596: loss 0.103048\n",
      "batch 2597: loss 0.233238\n",
      "batch 2598: loss 0.051536\n",
      "batch 2599: loss 0.085353\n",
      "batch 2600: loss 0.274331\n",
      "batch 2601: loss 0.054791\n",
      "batch 2602: loss 0.061698\n",
      "batch 2603: loss 0.128595\n",
      "batch 2604: loss 0.220503\n",
      "batch 2605: loss 0.113479\n",
      "batch 2606: loss 0.094723\n",
      "batch 2607: loss 0.126892\n",
      "batch 2608: loss 0.188842\n",
      "batch 2609: loss 0.134501\n",
      "batch 2610: loss 0.066973\n",
      "batch 2611: loss 0.203156\n",
      "batch 2612: loss 0.065471\n",
      "batch 2613: loss 0.228234\n",
      "batch 2614: loss 0.108182\n",
      "batch 2615: loss 0.038435\n",
      "batch 2616: loss 0.209553\n",
      "batch 2617: loss 0.049785\n",
      "batch 2618: loss 0.096175\n",
      "batch 2619: loss 0.132514\n",
      "batch 2620: loss 0.050377\n",
      "batch 2621: loss 0.051291\n",
      "batch 2622: loss 0.030230\n",
      "batch 2623: loss 0.308562\n",
      "batch 2624: loss 0.024902\n",
      "batch 2625: loss 0.090956\n",
      "batch 2626: loss 0.049262\n",
      "batch 2627: loss 0.057393\n",
      "batch 2628: loss 0.170655\n",
      "batch 2629: loss 0.019393\n",
      "batch 2630: loss 0.199304\n",
      "batch 2631: loss 0.083548\n",
      "batch 2632: loss 0.036189\n",
      "batch 2633: loss 0.039730\n",
      "batch 2634: loss 0.083806\n",
      "batch 2635: loss 0.084062\n",
      "batch 2636: loss 0.033226\n",
      "batch 2637: loss 0.131983\n",
      "batch 2638: loss 0.189423\n",
      "batch 2639: loss 0.045171\n",
      "batch 2640: loss 0.121796\n",
      "batch 2641: loss 0.057335\n",
      "batch 2642: loss 0.048844\n",
      "batch 2643: loss 0.099941\n",
      "batch 2644: loss 0.319209\n",
      "batch 2645: loss 0.211218\n",
      "batch 2646: loss 0.156475\n",
      "batch 2647: loss 0.047514\n",
      "batch 2648: loss 0.132289\n",
      "batch 2649: loss 0.203999\n",
      "batch 2650: loss 0.069698\n",
      "batch 2651: loss 0.051862\n",
      "batch 2652: loss 0.036821\n",
      "batch 2653: loss 0.145327\n",
      "batch 2654: loss 0.049276\n",
      "batch 2655: loss 0.149148\n",
      "batch 2656: loss 0.078926\n",
      "batch 2657: loss 0.048539\n",
      "batch 2658: loss 0.026895\n",
      "batch 2659: loss 0.280978\n",
      "batch 2660: loss 0.088213\n",
      "batch 2661: loss 0.083530\n",
      "batch 2662: loss 0.113479\n",
      "batch 2663: loss 0.145291\n",
      "batch 2664: loss 0.118118\n",
      "batch 2665: loss 0.053919\n",
      "batch 2666: loss 0.040478\n",
      "batch 2667: loss 0.067246\n",
      "batch 2668: loss 0.146039\n",
      "batch 2669: loss 0.029491\n",
      "batch 2670: loss 0.098171\n",
      "batch 2671: loss 0.120469\n",
      "batch 2672: loss 0.365523\n",
      "batch 2673: loss 0.121064\n",
      "batch 2674: loss 0.051738\n",
      "batch 2675: loss 0.014120\n",
      "batch 2676: loss 0.136603\n",
      "batch 2677: loss 0.115384\n",
      "batch 2678: loss 0.162765\n",
      "batch 2679: loss 0.059334\n",
      "batch 2680: loss 0.069313\n",
      "batch 2681: loss 0.068965\n",
      "batch 2682: loss 0.099354\n",
      "batch 2683: loss 0.029440\n",
      "batch 2684: loss 0.218804\n",
      "batch 2685: loss 0.060676\n",
      "batch 2686: loss 0.111615\n",
      "batch 2687: loss 0.204630\n",
      "batch 2688: loss 0.037345\n",
      "batch 2689: loss 0.183961\n",
      "batch 2690: loss 0.068574\n",
      "batch 2691: loss 0.131848\n",
      "batch 2692: loss 0.109120\n",
      "batch 2693: loss 0.041473\n",
      "batch 2694: loss 0.105889\n",
      "batch 2695: loss 0.077440\n",
      "batch 2696: loss 0.059841\n",
      "batch 2697: loss 0.314511\n",
      "batch 2698: loss 0.043637\n",
      "batch 2699: loss 0.072224\n",
      "batch 2700: loss 0.292031\n",
      "batch 2701: loss 0.081467\n",
      "batch 2702: loss 0.233582\n",
      "batch 2703: loss 0.122598\n",
      "batch 2704: loss 0.152284\n",
      "batch 2705: loss 0.098814\n",
      "batch 2706: loss 0.015114\n",
      "batch 2707: loss 0.066035\n",
      "batch 2708: loss 0.168692\n",
      "batch 2709: loss 0.086176\n",
      "batch 2710: loss 0.034966\n",
      "batch 2711: loss 0.124941\n",
      "batch 2712: loss 0.034368\n",
      "batch 2713: loss 0.040721\n",
      "batch 2714: loss 0.140759\n",
      "batch 2715: loss 0.218977\n",
      "batch 2716: loss 0.089525\n",
      "batch 2717: loss 0.073223\n",
      "batch 2718: loss 0.078015\n",
      "batch 2719: loss 0.027910\n",
      "batch 2720: loss 0.115302\n",
      "batch 2721: loss 0.076759\n",
      "batch 2722: loss 0.062589\n",
      "batch 2723: loss 0.083801\n",
      "batch 2724: loss 0.039647\n",
      "batch 2725: loss 0.052809\n",
      "batch 2726: loss 0.084734\n",
      "batch 2727: loss 0.051604\n",
      "batch 2728: loss 0.149881\n",
      "batch 2729: loss 0.038917\n",
      "batch 2730: loss 0.068542\n",
      "batch 2731: loss 0.158809\n",
      "batch 2732: loss 0.052556\n",
      "batch 2733: loss 0.067679\n",
      "batch 2734: loss 0.105430\n",
      "batch 2735: loss 0.018112\n",
      "batch 2736: loss 0.087064\n",
      "batch 2737: loss 0.085168\n",
      "batch 2738: loss 0.137982\n",
      "batch 2739: loss 0.082087\n",
      "batch 2740: loss 0.158537\n",
      "batch 2741: loss 0.054041\n",
      "batch 2742: loss 0.074589\n",
      "batch 2743: loss 0.105678\n",
      "batch 2744: loss 0.092924\n",
      "batch 2745: loss 0.129372\n",
      "batch 2746: loss 0.047109\n",
      "batch 2747: loss 0.065693\n",
      "batch 2748: loss 0.113884\n",
      "batch 2749: loss 0.167040\n",
      "batch 2750: loss 0.125921\n",
      "batch 2751: loss 0.077087\n",
      "batch 2752: loss 0.044541\n",
      "batch 2753: loss 0.027541\n",
      "batch 2754: loss 0.112784\n",
      "batch 2755: loss 0.193005\n",
      "batch 2756: loss 0.048571\n",
      "batch 2757: loss 0.108006\n",
      "batch 2758: loss 0.085208\n",
      "batch 2759: loss 0.013308\n",
      "batch 2760: loss 0.045896\n",
      "batch 2761: loss 0.072862\n",
      "batch 2762: loss 0.047967\n",
      "batch 2763: loss 0.083145\n",
      "batch 2764: loss 0.030242\n",
      "batch 2765: loss 0.147629\n",
      "batch 2766: loss 0.119639\n",
      "batch 2767: loss 0.236556\n",
      "batch 2768: loss 0.166639\n",
      "batch 2769: loss 0.022446\n",
      "batch 2770: loss 0.089336\n",
      "batch 2771: loss 0.070674\n",
      "batch 2772: loss 0.087634\n",
      "batch 2773: loss 0.026920\n",
      "batch 2774: loss 0.022214\n",
      "batch 2775: loss 0.018985\n",
      "batch 2776: loss 0.195922\n",
      "batch 2777: loss 0.070814\n",
      "batch 2778: loss 0.022983\n",
      "batch 2779: loss 0.057985\n",
      "batch 2780: loss 0.051305\n",
      "batch 2781: loss 0.037744\n",
      "batch 2782: loss 0.083709\n",
      "batch 2783: loss 0.161433\n",
      "batch 2784: loss 0.015512\n",
      "batch 2785: loss 0.088177\n",
      "batch 2786: loss 0.213425\n",
      "batch 2787: loss 0.085989\n",
      "batch 2788: loss 0.024612\n",
      "batch 2789: loss 0.089298\n",
      "batch 2790: loss 0.091346\n",
      "batch 2791: loss 0.042003\n",
      "batch 2792: loss 0.022359\n",
      "batch 2793: loss 0.054488\n",
      "batch 2794: loss 0.105799\n",
      "batch 2795: loss 0.088491\n",
      "batch 2796: loss 0.093067\n",
      "batch 2797: loss 0.042875\n",
      "batch 2798: loss 0.108116\n",
      "batch 2799: loss 0.155294\n",
      "batch 2800: loss 0.054147\n",
      "batch 2801: loss 0.099837\n",
      "batch 2802: loss 0.130131\n",
      "batch 2803: loss 0.031884\n",
      "batch 2804: loss 0.057228\n",
      "batch 2805: loss 0.106653\n",
      "batch 2806: loss 0.017680\n",
      "batch 2807: loss 0.245562\n",
      "batch 2808: loss 0.184552\n",
      "batch 2809: loss 0.070946\n",
      "batch 2810: loss 0.038251\n",
      "batch 2811: loss 0.027559\n",
      "batch 2812: loss 0.179662\n",
      "batch 2813: loss 0.108796\n",
      "batch 2814: loss 0.054405\n",
      "batch 2815: loss 0.116398\n",
      "batch 2816: loss 0.068270\n",
      "batch 2817: loss 0.097184\n",
      "batch 2818: loss 0.030439\n",
      "batch 2819: loss 0.142619\n",
      "batch 2820: loss 0.066938\n",
      "batch 2821: loss 0.184422\n",
      "batch 2822: loss 0.105576\n",
      "batch 2823: loss 0.118136\n",
      "batch 2824: loss 0.073328\n",
      "batch 2825: loss 0.129224\n",
      "batch 2826: loss 0.050711\n",
      "batch 2827: loss 0.129650\n",
      "batch 2828: loss 0.091795\n",
      "batch 2829: loss 0.108888\n",
      "batch 2830: loss 0.063010\n",
      "batch 2831: loss 0.212552\n",
      "batch 2832: loss 0.265703\n",
      "batch 2833: loss 0.034758\n",
      "batch 2834: loss 0.112060\n",
      "batch 2835: loss 0.073597\n",
      "batch 2836: loss 0.213308\n",
      "batch 2837: loss 0.064190\n",
      "batch 2838: loss 0.333401\n",
      "batch 2839: loss 0.114725\n",
      "batch 2840: loss 0.032843\n",
      "batch 2841: loss 0.038118\n",
      "batch 2842: loss 0.024796\n",
      "batch 2843: loss 0.116699\n",
      "batch 2844: loss 0.098816\n",
      "batch 2845: loss 0.055176\n",
      "batch 2846: loss 0.057039\n",
      "batch 2847: loss 0.118105\n",
      "batch 2848: loss 0.135216\n",
      "batch 2849: loss 0.032874\n",
      "batch 2850: loss 0.075326\n",
      "batch 2851: loss 0.047665\n",
      "batch 2852: loss 0.058148\n",
      "batch 2853: loss 0.022721\n",
      "batch 2854: loss 0.034800\n",
      "batch 2855: loss 0.095069\n",
      "batch 2856: loss 0.069165\n",
      "batch 2857: loss 0.168248\n",
      "batch 2858: loss 0.037564\n",
      "batch 2859: loss 0.079147\n",
      "batch 2860: loss 0.115091\n",
      "batch 2861: loss 0.102226\n",
      "batch 2862: loss 0.077604\n",
      "batch 2863: loss 0.088778\n",
      "batch 2864: loss 0.052950\n",
      "batch 2865: loss 0.061611\n",
      "batch 2866: loss 0.033974\n",
      "batch 2867: loss 0.117148\n",
      "batch 2868: loss 0.139806\n",
      "batch 2869: loss 0.117454\n",
      "batch 2870: loss 0.024286\n",
      "batch 2871: loss 0.023224\n",
      "batch 2872: loss 0.115232\n",
      "batch 2873: loss 0.176634\n",
      "batch 2874: loss 0.056399\n",
      "batch 2875: loss 0.113164\n",
      "batch 2876: loss 0.101228\n",
      "batch 2877: loss 0.013144\n",
      "batch 2878: loss 0.054441\n",
      "batch 2879: loss 0.239674\n",
      "batch 2880: loss 0.287529\n",
      "batch 2881: loss 0.076219\n",
      "batch 2882: loss 0.202395\n",
      "batch 2883: loss 0.012362\n",
      "batch 2884: loss 0.023414\n",
      "batch 2885: loss 0.047336\n",
      "batch 2886: loss 0.132306\n",
      "batch 2887: loss 0.089690\n",
      "batch 2888: loss 0.112344\n",
      "batch 2889: loss 0.064412\n",
      "batch 2890: loss 0.141097\n",
      "batch 2891: loss 0.065399\n",
      "batch 2892: loss 0.053898\n",
      "batch 2893: loss 0.226057\n",
      "batch 2894: loss 0.047383\n",
      "batch 2895: loss 0.085823\n",
      "batch 2896: loss 0.153584\n",
      "batch 2897: loss 0.036848\n",
      "batch 2898: loss 0.017894\n",
      "batch 2899: loss 0.203309\n",
      "batch 2900: loss 0.054915\n",
      "batch 2901: loss 0.128457\n",
      "batch 2902: loss 0.074471\n",
      "batch 2903: loss 0.068836\n",
      "batch 2904: loss 0.026373\n",
      "batch 2905: loss 0.124490\n",
      "batch 2906: loss 0.163018\n",
      "batch 2907: loss 0.362809\n",
      "batch 2908: loss 0.128049\n",
      "batch 2909: loss 0.040586\n",
      "batch 2910: loss 0.064080\n",
      "batch 2911: loss 0.043812\n",
      "batch 2912: loss 0.026736\n",
      "batch 2913: loss 0.072842\n",
      "batch 2914: loss 0.063783\n",
      "batch 2915: loss 0.049579\n",
      "batch 2916: loss 0.041171\n",
      "batch 2917: loss 0.045403\n",
      "batch 2918: loss 0.026901\n",
      "batch 2919: loss 0.174496\n",
      "batch 2920: loss 0.013538\n",
      "batch 2921: loss 0.033627\n",
      "batch 2922: loss 0.028026\n",
      "batch 2923: loss 0.070781\n",
      "batch 2924: loss 0.265358\n",
      "batch 2925: loss 0.226085\n",
      "batch 2926: loss 0.190419\n",
      "batch 2927: loss 0.095862\n",
      "batch 2928: loss 0.166719\n",
      "batch 2929: loss 0.130317\n",
      "batch 2930: loss 0.324351\n",
      "batch 2931: loss 0.246324\n",
      "batch 2932: loss 0.114883\n",
      "batch 2933: loss 0.210318\n",
      "batch 2934: loss 0.158148\n",
      "batch 2935: loss 0.095840\n",
      "batch 2936: loss 0.088970\n",
      "batch 2937: loss 0.023797\n",
      "batch 2938: loss 0.128781\n",
      "batch 2939: loss 0.153072\n",
      "batch 2940: loss 0.042404\n",
      "batch 2941: loss 0.122616\n",
      "batch 2942: loss 0.035089\n",
      "batch 2943: loss 0.126521\n",
      "batch 2944: loss 0.201848\n",
      "batch 2945: loss 0.047195\n",
      "batch 2946: loss 0.042307\n",
      "batch 2947: loss 0.211654\n",
      "batch 2948: loss 0.138036\n",
      "batch 2949: loss 0.145938\n",
      "batch 2950: loss 0.131713\n",
      "batch 2951: loss 0.133613\n",
      "batch 2952: loss 0.242262\n",
      "batch 2953: loss 0.035741\n",
      "batch 2954: loss 0.066255\n",
      "batch 2955: loss 0.065389\n",
      "batch 2956: loss 0.040743\n",
      "batch 2957: loss 0.162334\n",
      "batch 2958: loss 0.114820\n",
      "batch 2959: loss 0.122317\n",
      "batch 2960: loss 0.020451\n",
      "batch 2961: loss 0.185389\n",
      "batch 2962: loss 0.134071\n",
      "batch 2963: loss 0.070858\n",
      "batch 2964: loss 0.147099\n",
      "batch 2965: loss 0.303276\n",
      "batch 2966: loss 0.113470\n",
      "batch 2967: loss 0.065315\n",
      "batch 2968: loss 0.109618\n",
      "batch 2969: loss 0.063369\n",
      "batch 2970: loss 0.047495\n",
      "batch 2971: loss 0.083397\n",
      "batch 2972: loss 0.067076\n",
      "batch 2973: loss 0.046317\n",
      "batch 2974: loss 0.046093\n",
      "batch 2975: loss 0.071138\n",
      "batch 2976: loss 0.231092\n",
      "batch 2977: loss 0.026668\n",
      "batch 2978: loss 0.057122\n",
      "batch 2979: loss 0.197087\n",
      "batch 2980: loss 0.028653\n",
      "batch 2981: loss 0.085785\n",
      "batch 2982: loss 0.025668\n",
      "batch 2983: loss 0.041814\n",
      "batch 2984: loss 0.133636\n",
      "batch 2985: loss 0.057736\n",
      "batch 2986: loss 0.222569\n",
      "batch 2987: loss 0.249337\n",
      "batch 2988: loss 0.089733\n",
      "batch 2989: loss 0.077120\n",
      "batch 2990: loss 0.122365\n",
      "batch 2991: loss 0.047165\n",
      "batch 2992: loss 0.115864\n",
      "batch 2993: loss 0.155876\n",
      "batch 2994: loss 0.118326\n",
      "batch 2995: loss 0.150152\n",
      "batch 2996: loss 0.145578\n",
      "batch 2997: loss 0.094271\n",
      "batch 2998: loss 0.112290\n",
      "batch 2999: loss 0.127734\n",
      "batch 3000: loss 0.055361\n",
      "batch 3001: loss 0.031114\n",
      "batch 3002: loss 0.230716\n",
      "batch 3003: loss 0.108450\n",
      "batch 3004: loss 0.046671\n",
      "batch 3005: loss 0.113934\n",
      "batch 3006: loss 0.091537\n",
      "batch 3007: loss 0.170171\n",
      "batch 3008: loss 0.124853\n",
      "batch 3009: loss 0.060125\n",
      "batch 3010: loss 0.115294\n",
      "batch 3011: loss 0.108556\n",
      "batch 3012: loss 0.047649\n",
      "batch 3013: loss 0.084596\n",
      "batch 3014: loss 0.122769\n",
      "batch 3015: loss 0.143190\n",
      "batch 3016: loss 0.043486\n",
      "batch 3017: loss 0.071673\n",
      "batch 3018: loss 0.216156\n",
      "batch 3019: loss 0.160767\n",
      "batch 3020: loss 0.357250\n",
      "batch 3021: loss 0.060566\n",
      "batch 3022: loss 0.125138\n",
      "batch 3023: loss 0.068096\n",
      "batch 3024: loss 0.148547\n",
      "batch 3025: loss 0.106881\n",
      "batch 3026: loss 0.048512\n",
      "batch 3027: loss 0.045933\n",
      "batch 3028: loss 0.086699\n",
      "batch 3029: loss 0.056985\n",
      "batch 3030: loss 0.064690\n",
      "batch 3031: loss 0.080083\n",
      "batch 3032: loss 0.159993\n",
      "batch 3033: loss 0.169688\n",
      "batch 3034: loss 0.222060\n",
      "batch 3035: loss 0.115894\n",
      "batch 3036: loss 0.088831\n",
      "batch 3037: loss 0.033481\n",
      "batch 3038: loss 0.024796\n",
      "batch 3039: loss 0.091411\n",
      "batch 3040: loss 0.119012\n",
      "batch 3041: loss 0.053614\n",
      "batch 3042: loss 0.033228\n",
      "batch 3043: loss 0.185507\n",
      "batch 3044: loss 0.088684\n",
      "batch 3045: loss 0.032196\n",
      "batch 3046: loss 0.062150\n",
      "batch 3047: loss 0.095226\n",
      "batch 3048: loss 0.048503\n",
      "batch 3049: loss 0.092680\n",
      "batch 3050: loss 0.165840\n",
      "batch 3051: loss 0.085450\n",
      "batch 3052: loss 0.219677\n",
      "batch 3053: loss 0.060537\n",
      "batch 3054: loss 0.067992\n",
      "batch 3055: loss 0.033175\n",
      "batch 3056: loss 0.028511\n",
      "batch 3057: loss 0.144675\n",
      "batch 3058: loss 0.272366\n",
      "batch 3059: loss 0.044164\n",
      "batch 3060: loss 0.197956\n",
      "batch 3061: loss 0.022438\n",
      "batch 3062: loss 0.038523\n",
      "batch 3063: loss 0.061664\n",
      "batch 3064: loss 0.075374\n",
      "batch 3065: loss 0.024750\n",
      "batch 3066: loss 0.283950\n",
      "batch 3067: loss 0.016288\n",
      "batch 3068: loss 0.020773\n",
      "batch 3069: loss 0.136356\n",
      "batch 3070: loss 0.163194\n",
      "batch 3071: loss 0.081036\n",
      "batch 3072: loss 0.035561\n",
      "batch 3073: loss 0.090954\n",
      "batch 3074: loss 0.102428\n",
      "batch 3075: loss 0.139786\n",
      "batch 3076: loss 0.101754\n",
      "batch 3077: loss 0.034071\n",
      "batch 3078: loss 0.055329\n",
      "batch 3079: loss 0.020410\n",
      "batch 3080: loss 0.099820\n",
      "batch 3081: loss 0.105103\n",
      "batch 3082: loss 0.047875\n",
      "batch 3083: loss 0.143241\n",
      "batch 3084: loss 0.030048\n",
      "batch 3085: loss 0.080788\n",
      "batch 3086: loss 0.314807\n",
      "batch 3087: loss 0.028738\n",
      "batch 3088: loss 0.142343\n",
      "batch 3089: loss 0.066805\n",
      "batch 3090: loss 0.059531\n",
      "batch 3091: loss 0.102909\n",
      "batch 3092: loss 0.149288\n",
      "batch 3093: loss 0.118749\n",
      "batch 3094: loss 0.175886\n",
      "batch 3095: loss 0.030348\n",
      "batch 3096: loss 0.014931\n",
      "batch 3097: loss 0.030457\n",
      "batch 3098: loss 0.097629\n",
      "batch 3099: loss 0.026756\n",
      "batch 3100: loss 0.027440\n",
      "batch 3101: loss 0.046121\n",
      "batch 3102: loss 0.083993\n",
      "batch 3103: loss 0.132240\n",
      "batch 3104: loss 0.042513\n",
      "batch 3105: loss 0.024138\n",
      "batch 3106: loss 0.246153\n",
      "batch 3107: loss 0.014882\n",
      "batch 3108: loss 0.057370\n",
      "batch 3109: loss 0.287957\n",
      "batch 3110: loss 0.084767\n",
      "batch 3111: loss 0.050325\n",
      "batch 3112: loss 0.075711\n",
      "batch 3113: loss 0.024609\n",
      "batch 3114: loss 0.078149\n",
      "batch 3115: loss 0.119682\n",
      "batch 3116: loss 0.030189\n",
      "batch 3117: loss 0.127805\n",
      "batch 3118: loss 0.109673\n",
      "batch 3119: loss 0.058012\n",
      "batch 3120: loss 0.064529\n",
      "batch 3121: loss 0.034902\n",
      "batch 3122: loss 0.022078\n",
      "batch 3123: loss 0.080883\n",
      "batch 3124: loss 0.085887\n",
      "batch 3125: loss 0.071350\n",
      "batch 3126: loss 0.046927\n",
      "batch 3127: loss 0.110394\n",
      "batch 3128: loss 0.062791\n",
      "batch 3129: loss 0.224133\n",
      "batch 3130: loss 0.182404\n",
      "batch 3131: loss 0.097971\n",
      "batch 3132: loss 0.025758\n",
      "batch 3133: loss 0.053673\n",
      "batch 3134: loss 0.038995\n",
      "batch 3135: loss 0.067267\n",
      "batch 3136: loss 0.068722\n",
      "batch 3137: loss 0.277580\n",
      "batch 3138: loss 0.039387\n",
      "batch 3139: loss 0.061037\n",
      "batch 3140: loss 0.105812\n",
      "batch 3141: loss 0.038081\n",
      "batch 3142: loss 0.109827\n",
      "batch 3143: loss 0.020335\n",
      "batch 3144: loss 0.173749\n",
      "batch 3145: loss 0.184695\n",
      "batch 3146: loss 0.255057\n",
      "batch 3147: loss 0.013570\n",
      "batch 3148: loss 0.012979\n",
      "batch 3149: loss 0.054673\n",
      "batch 3150: loss 0.089301\n",
      "batch 3151: loss 0.230459\n",
      "batch 3152: loss 0.054680\n",
      "batch 3153: loss 0.098623\n",
      "batch 3154: loss 0.206384\n",
      "batch 3155: loss 0.076486\n",
      "batch 3156: loss 0.072011\n",
      "batch 3157: loss 0.042721\n",
      "batch 3158: loss 0.097066\n",
      "batch 3159: loss 0.080598\n",
      "batch 3160: loss 0.036380\n",
      "batch 3161: loss 0.066910\n",
      "batch 3162: loss 0.070477\n",
      "batch 3163: loss 0.171365\n",
      "batch 3164: loss 0.144813\n",
      "batch 3165: loss 0.026554\n",
      "batch 3166: loss 0.087425\n",
      "batch 3167: loss 0.040386\n",
      "batch 3168: loss 0.101874\n",
      "batch 3169: loss 0.224467\n",
      "batch 3170: loss 0.079734\n",
      "batch 3171: loss 0.163911\n",
      "batch 3172: loss 0.102258\n",
      "batch 3173: loss 0.044247\n",
      "batch 3174: loss 0.080696\n",
      "batch 3175: loss 0.052206\n",
      "batch 3176: loss 0.199613\n",
      "batch 3177: loss 0.074316\n",
      "batch 3178: loss 0.018048\n",
      "batch 3179: loss 0.093900\n",
      "batch 3180: loss 0.040326\n",
      "batch 3181: loss 0.044854\n",
      "batch 3182: loss 0.093545\n",
      "batch 3183: loss 0.039090\n",
      "batch 3184: loss 0.017351\n",
      "batch 3185: loss 0.092447\n",
      "batch 3186: loss 0.068397\n",
      "batch 3187: loss 0.163420\n",
      "batch 3188: loss 0.286665\n",
      "batch 3189: loss 0.080394\n",
      "batch 3190: loss 0.304861\n",
      "batch 3191: loss 0.063806\n",
      "batch 3192: loss 0.024083\n",
      "batch 3193: loss 0.116510\n",
      "batch 3194: loss 0.067477\n",
      "batch 3195: loss 0.043226\n",
      "batch 3196: loss 0.067308\n",
      "batch 3197: loss 0.034605\n",
      "batch 3198: loss 0.048987\n",
      "batch 3199: loss 0.151401\n",
      "batch 3200: loss 0.036314\n",
      "batch 3201: loss 0.197797\n",
      "batch 3202: loss 0.040592\n",
      "batch 3203: loss 0.020022\n",
      "batch 3204: loss 0.096836\n",
      "batch 3205: loss 0.115528\n",
      "batch 3206: loss 0.039648\n",
      "batch 3207: loss 0.224076\n",
      "batch 3208: loss 0.034984\n",
      "batch 3209: loss 0.121874\n",
      "batch 3210: loss 0.046395\n",
      "batch 3211: loss 0.069768\n",
      "batch 3212: loss 0.085485\n",
      "batch 3213: loss 0.067630\n",
      "batch 3214: loss 0.035136\n",
      "batch 3215: loss 0.219125\n",
      "batch 3216: loss 0.038580\n",
      "batch 3217: loss 0.047069\n",
      "batch 3218: loss 0.136750\n",
      "batch 3219: loss 0.057740\n",
      "batch 3220: loss 0.084521\n",
      "batch 3221: loss 0.053186\n",
      "batch 3222: loss 0.095851\n",
      "batch 3223: loss 0.114212\n",
      "batch 3224: loss 0.046894\n",
      "batch 3225: loss 0.055189\n",
      "batch 3226: loss 0.104927\n",
      "batch 3227: loss 0.132622\n",
      "batch 3228: loss 0.057534\n",
      "batch 3229: loss 0.033513\n",
      "batch 3230: loss 0.071393\n",
      "batch 3231: loss 0.038267\n",
      "batch 3232: loss 0.110658\n",
      "batch 3233: loss 0.009446\n",
      "batch 3234: loss 0.076452\n",
      "batch 3235: loss 0.212627\n",
      "batch 3236: loss 0.074816\n",
      "batch 3237: loss 0.196506\n",
      "batch 3238: loss 0.085121\n",
      "batch 3239: loss 0.145017\n",
      "batch 3240: loss 0.078531\n",
      "batch 3241: loss 0.077716\n",
      "batch 3242: loss 0.091030\n",
      "batch 3243: loss 0.185978\n",
      "batch 3244: loss 0.038995\n",
      "batch 3245: loss 0.076057\n",
      "batch 3246: loss 0.069806\n",
      "batch 3247: loss 0.164464\n",
      "batch 3248: loss 0.101582\n",
      "batch 3249: loss 0.092901\n",
      "batch 3250: loss 0.065152\n",
      "batch 3251: loss 0.056955\n",
      "batch 3252: loss 0.071581\n",
      "batch 3253: loss 0.091440\n",
      "batch 3254: loss 0.038292\n",
      "batch 3255: loss 0.127061\n",
      "batch 3256: loss 0.041868\n",
      "batch 3257: loss 0.104064\n",
      "batch 3258: loss 0.082381\n",
      "batch 3259: loss 0.048436\n",
      "batch 3260: loss 0.129652\n",
      "batch 3261: loss 0.033219\n",
      "batch 3262: loss 0.186089\n",
      "batch 3263: loss 0.046121\n",
      "batch 3264: loss 0.061129\n",
      "batch 3265: loss 0.146489\n",
      "batch 3266: loss 0.159281\n",
      "batch 3267: loss 0.112516\n",
      "batch 3268: loss 0.177944\n",
      "batch 3269: loss 0.044487\n",
      "batch 3270: loss 0.194237\n",
      "batch 3271: loss 0.081537\n",
      "batch 3272: loss 0.205020\n",
      "batch 3273: loss 0.011379\n",
      "batch 3274: loss 0.047289\n",
      "batch 3275: loss 0.029809\n",
      "batch 3276: loss 0.027049\n",
      "batch 3277: loss 0.025661\n",
      "batch 3278: loss 0.065183\n",
      "batch 3279: loss 0.034565\n",
      "batch 3280: loss 0.138642\n",
      "batch 3281: loss 0.111381\n",
      "batch 3282: loss 0.025208\n",
      "batch 3283: loss 0.036321\n",
      "batch 3284: loss 0.041624\n",
      "batch 3285: loss 0.062934\n",
      "batch 3286: loss 0.041608\n",
      "batch 3287: loss 0.112154\n",
      "batch 3288: loss 0.183277\n",
      "batch 3289: loss 0.029134\n",
      "batch 3290: loss 0.039458\n",
      "batch 3291: loss 0.040919\n",
      "batch 3292: loss 0.139766\n",
      "batch 3293: loss 0.077410\n",
      "batch 3294: loss 0.127229\n",
      "batch 3295: loss 0.136097\n",
      "batch 3296: loss 0.120540\n",
      "batch 3297: loss 0.082608\n",
      "batch 3298: loss 0.045670\n",
      "batch 3299: loss 0.083677\n",
      "batch 3300: loss 0.131308\n",
      "batch 3301: loss 0.039790\n",
      "batch 3302: loss 0.112867\n",
      "batch 3303: loss 0.084887\n",
      "batch 3304: loss 0.025079\n",
      "batch 3305: loss 0.294385\n",
      "batch 3306: loss 0.028731\n",
      "batch 3307: loss 0.057631\n",
      "batch 3308: loss 0.059443\n",
      "batch 3309: loss 0.045290\n",
      "batch 3310: loss 0.120254\n",
      "batch 3311: loss 0.106026\n",
      "batch 3312: loss 0.102050\n",
      "batch 3313: loss 0.074248\n",
      "batch 3314: loss 0.102132\n",
      "batch 3315: loss 0.100829\n",
      "batch 3316: loss 0.062030\n",
      "batch 3317: loss 0.134698\n",
      "batch 3318: loss 0.077129\n",
      "batch 3319: loss 0.109480\n",
      "batch 3320: loss 0.147354\n",
      "batch 3321: loss 0.042314\n",
      "batch 3322: loss 0.092168\n",
      "batch 3323: loss 0.052724\n",
      "batch 3324: loss 0.075859\n",
      "batch 3325: loss 0.092930\n",
      "batch 3326: loss 0.048622\n",
      "batch 3327: loss 0.111240\n",
      "batch 3328: loss 0.041968\n",
      "batch 3329: loss 0.054988\n",
      "batch 3330: loss 0.031243\n",
      "batch 3331: loss 0.040967\n",
      "batch 3332: loss 0.058725\n",
      "batch 3333: loss 0.194215\n",
      "batch 3334: loss 0.076690\n",
      "batch 3335: loss 0.189463\n",
      "batch 3336: loss 0.050189\n",
      "batch 3337: loss 0.104975\n",
      "batch 3338: loss 0.048065\n",
      "batch 3339: loss 0.150174\n",
      "batch 3340: loss 0.104309\n",
      "batch 3341: loss 0.082757\n",
      "batch 3342: loss 0.064086\n",
      "batch 3343: loss 0.056664\n",
      "batch 3344: loss 0.055624\n",
      "batch 3345: loss 0.060886\n",
      "batch 3346: loss 0.220706\n",
      "batch 3347: loss 0.195625\n",
      "batch 3348: loss 0.041484\n",
      "batch 3349: loss 0.109164\n",
      "batch 3350: loss 0.035339\n",
      "batch 3351: loss 0.067891\n",
      "batch 3352: loss 0.046296\n",
      "batch 3353: loss 0.249327\n",
      "batch 3354: loss 0.030549\n",
      "batch 3355: loss 0.021306\n",
      "batch 3356: loss 0.124661\n",
      "batch 3357: loss 0.127832\n",
      "batch 3358: loss 0.106196\n",
      "batch 3359: loss 0.015634\n",
      "batch 3360: loss 0.064125\n",
      "batch 3361: loss 0.147501\n",
      "batch 3362: loss 0.033545\n",
      "batch 3363: loss 0.163002\n",
      "batch 3364: loss 0.034190\n",
      "batch 3365: loss 0.115134\n",
      "batch 3366: loss 0.062851\n",
      "batch 3367: loss 0.153827\n",
      "batch 3368: loss 0.316748\n",
      "batch 3369: loss 0.088754\n",
      "batch 3370: loss 0.103160\n",
      "batch 3371: loss 0.271381\n",
      "batch 3372: loss 0.064162\n",
      "batch 3373: loss 0.082149\n",
      "batch 3374: loss 0.096602\n",
      "batch 3375: loss 0.083048\n",
      "batch 3376: loss 0.097556\n",
      "batch 3377: loss 0.069051\n",
      "batch 3378: loss 0.028549\n",
      "batch 3379: loss 0.089321\n",
      "batch 3380: loss 0.082808\n",
      "batch 3381: loss 0.114162\n",
      "batch 3382: loss 0.100261\n",
      "batch 3383: loss 0.039985\n",
      "batch 3384: loss 0.042514\n",
      "batch 3385: loss 0.054153\n",
      "batch 3386: loss 0.060815\n",
      "batch 3387: loss 0.080870\n",
      "batch 3388: loss 0.053950\n",
      "batch 3389: loss 0.163938\n",
      "batch 3390: loss 0.046101\n",
      "batch 3391: loss 0.039824\n",
      "batch 3392: loss 0.073283\n",
      "batch 3393: loss 0.092574\n",
      "batch 3394: loss 0.057272\n",
      "batch 3395: loss 0.027722\n",
      "batch 3396: loss 0.155726\n",
      "batch 3397: loss 0.073557\n",
      "batch 3398: loss 0.042361\n",
      "batch 3399: loss 0.087735\n",
      "batch 3400: loss 0.166008\n",
      "batch 3401: loss 0.096347\n",
      "batch 3402: loss 0.031229\n",
      "batch 3403: loss 0.045515\n",
      "batch 3404: loss 0.077650\n",
      "batch 3405: loss 0.041494\n",
      "batch 3406: loss 0.068889\n",
      "batch 3407: loss 0.156965\n",
      "batch 3408: loss 0.124423\n",
      "batch 3409: loss 0.074554\n",
      "batch 3410: loss 0.038154\n",
      "batch 3411: loss 0.084799\n",
      "batch 3412: loss 0.046874\n",
      "batch 3413: loss 0.020505\n",
      "batch 3414: loss 0.027447\n",
      "batch 3415: loss 0.014566\n",
      "batch 3416: loss 0.041508\n",
      "batch 3417: loss 0.023530\n",
      "batch 3418: loss 0.132537\n",
      "batch 3419: loss 0.173553\n",
      "batch 3420: loss 0.093559\n",
      "batch 3421: loss 0.021872\n",
      "batch 3422: loss 0.088283\n",
      "batch 3423: loss 0.102289\n",
      "batch 3424: loss 0.116436\n",
      "batch 3425: loss 0.166738\n",
      "batch 3426: loss 0.012644\n",
      "batch 3427: loss 0.049498\n",
      "batch 3428: loss 0.056000\n",
      "batch 3429: loss 0.131701\n",
      "batch 3430: loss 0.192833\n",
      "batch 3431: loss 0.017055\n",
      "batch 3432: loss 0.270748\n",
      "batch 3433: loss 0.120686\n",
      "batch 3434: loss 0.263566\n",
      "batch 3435: loss 0.056514\n",
      "batch 3436: loss 0.044937\n",
      "batch 3437: loss 0.058865\n",
      "batch 3438: loss 0.074166\n",
      "batch 3439: loss 0.094140\n",
      "batch 3440: loss 0.017467\n",
      "batch 3441: loss 0.039381\n",
      "batch 3442: loss 0.141385\n",
      "batch 3443: loss 0.120551\n",
      "batch 3444: loss 0.020508\n",
      "batch 3445: loss 0.046877\n",
      "batch 3446: loss 0.117961\n",
      "batch 3447: loss 0.032821\n",
      "batch 3448: loss 0.130176\n",
      "batch 3449: loss 0.089981\n",
      "batch 3450: loss 0.030010\n",
      "batch 3451: loss 0.463394\n",
      "batch 3452: loss 0.174094\n",
      "batch 3453: loss 0.048986\n",
      "batch 3454: loss 0.016760\n",
      "batch 3455: loss 0.029400\n",
      "batch 3456: loss 0.226528\n",
      "batch 3457: loss 0.052262\n",
      "batch 3458: loss 0.101775\n",
      "batch 3459: loss 0.080006\n",
      "batch 3460: loss 0.026516\n",
      "batch 3461: loss 0.026338\n",
      "batch 3462: loss 0.042184\n",
      "batch 3463: loss 0.062414\n",
      "batch 3464: loss 0.032678\n",
      "batch 3465: loss 0.085056\n",
      "batch 3466: loss 0.025553\n",
      "batch 3467: loss 0.062237\n",
      "batch 3468: loss 0.017829\n",
      "batch 3469: loss 0.064700\n",
      "batch 3470: loss 0.030589\n",
      "batch 3471: loss 0.109834\n",
      "batch 3472: loss 0.132781\n",
      "batch 3473: loss 0.090456\n",
      "batch 3474: loss 0.035689\n",
      "batch 3475: loss 0.104732\n",
      "batch 3476: loss 0.060815\n",
      "batch 3477: loss 0.115224\n",
      "batch 3478: loss 0.103106\n",
      "batch 3479: loss 0.057987\n",
      "batch 3480: loss 0.114255\n",
      "batch 3481: loss 0.173850\n",
      "batch 3482: loss 0.040251\n",
      "batch 3483: loss 0.071006\n",
      "batch 3484: loss 0.025493\n",
      "batch 3485: loss 0.077910\n",
      "batch 3486: loss 0.021426\n",
      "batch 3487: loss 0.070985\n",
      "batch 3488: loss 0.064061\n",
      "batch 3489: loss 0.126238\n",
      "batch 3490: loss 0.023850\n",
      "batch 3491: loss 0.031906\n",
      "batch 3492: loss 0.047940\n",
      "batch 3493: loss 0.057675\n",
      "batch 3494: loss 0.123789\n",
      "batch 3495: loss 0.080442\n",
      "batch 3496: loss 0.090727\n",
      "batch 3497: loss 0.146552\n",
      "batch 3498: loss 0.033204\n",
      "batch 3499: loss 0.039251\n",
      "batch 3500: loss 0.150578\n",
      "batch 3501: loss 0.178143\n",
      "batch 3502: loss 0.102675\n",
      "batch 3503: loss 0.035988\n",
      "batch 3504: loss 0.138733\n",
      "batch 3505: loss 0.099756\n",
      "batch 3506: loss 0.394724\n",
      "batch 3507: loss 0.022028\n",
      "batch 3508: loss 0.172968\n",
      "batch 3509: loss 0.161769\n",
      "batch 3510: loss 0.049193\n",
      "batch 3511: loss 0.029612\n",
      "batch 3512: loss 0.260993\n",
      "batch 3513: loss 0.050803\n",
      "batch 3514: loss 0.046405\n",
      "batch 3515: loss 0.245905\n",
      "batch 3516: loss 0.128812\n",
      "batch 3517: loss 0.042018\n",
      "batch 3518: loss 0.029324\n",
      "batch 3519: loss 0.044543\n",
      "batch 3520: loss 0.026506\n",
      "batch 3521: loss 0.075373\n",
      "batch 3522: loss 0.106214\n",
      "batch 3523: loss 0.053199\n",
      "batch 3524: loss 0.119918\n",
      "batch 3525: loss 0.042023\n",
      "batch 3526: loss 0.068539\n",
      "batch 3527: loss 0.141526\n",
      "batch 3528: loss 0.032353\n",
      "batch 3529: loss 0.083056\n",
      "batch 3530: loss 0.147859\n",
      "batch 3531: loss 0.040633\n",
      "batch 3532: loss 0.043689\n",
      "batch 3533: loss 0.055391\n",
      "batch 3534: loss 0.057037\n",
      "batch 3535: loss 0.094664\n",
      "batch 3536: loss 0.171393\n",
      "batch 3537: loss 0.029501\n",
      "batch 3538: loss 0.147109\n",
      "batch 3539: loss 0.209485\n",
      "batch 3540: loss 0.097725\n",
      "batch 3541: loss 0.013212\n",
      "batch 3542: loss 0.140957\n",
      "batch 3543: loss 0.042773\n",
      "batch 3544: loss 0.038524\n",
      "batch 3545: loss 0.024080\n",
      "batch 3546: loss 0.051838\n",
      "batch 3547: loss 0.126266\n",
      "batch 3548: loss 0.018854\n",
      "batch 3549: loss 0.094769\n",
      "batch 3550: loss 0.215810\n",
      "batch 3551: loss 0.226676\n",
      "batch 3552: loss 0.258274\n",
      "batch 3553: loss 0.088116\n",
      "batch 3554: loss 0.013862\n",
      "batch 3555: loss 0.123352\n",
      "batch 3556: loss 0.053720\n",
      "batch 3557: loss 0.017426\n",
      "batch 3558: loss 0.217507\n",
      "batch 3559: loss 0.030623\n",
      "batch 3560: loss 0.056008\n",
      "batch 3561: loss 0.056212\n",
      "batch 3562: loss 0.116813\n",
      "batch 3563: loss 0.096334\n",
      "batch 3564: loss 0.018176\n",
      "batch 3565: loss 0.086023\n",
      "batch 3566: loss 0.266866\n",
      "batch 3567: loss 0.058884\n",
      "batch 3568: loss 0.132226\n",
      "batch 3569: loss 0.099401\n",
      "batch 3570: loss 0.112545\n",
      "batch 3571: loss 0.068508\n",
      "batch 3572: loss 0.015998\n",
      "batch 3573: loss 0.178779\n",
      "batch 3574: loss 0.071374\n",
      "batch 3575: loss 0.035539\n",
      "batch 3576: loss 0.172910\n",
      "batch 3577: loss 0.039962\n",
      "batch 3578: loss 0.134168\n",
      "batch 3579: loss 0.206537\n",
      "batch 3580: loss 0.080459\n",
      "batch 3581: loss 0.091308\n",
      "batch 3582: loss 0.112972\n",
      "batch 3583: loss 0.182648\n",
      "batch 3584: loss 0.047721\n",
      "batch 3585: loss 0.063318\n",
      "batch 3586: loss 0.189812\n",
      "batch 3587: loss 0.032146\n",
      "batch 3588: loss 0.012734\n",
      "batch 3589: loss 0.087853\n",
      "batch 3590: loss 0.087918\n",
      "batch 3591: loss 0.062080\n",
      "batch 3592: loss 0.137921\n",
      "batch 3593: loss 0.107400\n",
      "batch 3594: loss 0.083990\n",
      "batch 3595: loss 0.194259\n",
      "batch 3596: loss 0.045551\n",
      "batch 3597: loss 0.033158\n",
      "batch 3598: loss 0.115846\n",
      "batch 3599: loss 0.100244\n",
      "batch 3600: loss 0.032358\n",
      "batch 3601: loss 0.046578\n",
      "batch 3602: loss 0.161341\n",
      "batch 3603: loss 0.048670\n",
      "batch 3604: loss 0.057309\n",
      "batch 3605: loss 0.025914\n",
      "batch 3606: loss 0.070891\n",
      "batch 3607: loss 0.041180\n",
      "batch 3608: loss 0.022000\n",
      "batch 3609: loss 0.051692\n",
      "batch 3610: loss 0.134655\n",
      "batch 3611: loss 0.053910\n",
      "batch 3612: loss 0.178971\n",
      "batch 3613: loss 0.086443\n",
      "batch 3614: loss 0.074792\n",
      "batch 3615: loss 0.121759\n",
      "batch 3616: loss 0.086296\n",
      "batch 3617: loss 0.112496\n",
      "batch 3618: loss 0.208275\n",
      "batch 3619: loss 0.100806\n",
      "batch 3620: loss 0.026814\n",
      "batch 3621: loss 0.044021\n",
      "batch 3622: loss 0.101739\n",
      "batch 3623: loss 0.071449\n",
      "batch 3624: loss 0.046374\n",
      "batch 3625: loss 0.105228\n",
      "batch 3626: loss 0.055872\n",
      "batch 3627: loss 0.115192\n",
      "batch 3628: loss 0.037467\n",
      "batch 3629: loss 0.077204\n",
      "batch 3630: loss 0.063104\n",
      "batch 3631: loss 0.041636\n",
      "batch 3632: loss 0.063152\n",
      "batch 3633: loss 0.082760\n",
      "batch 3634: loss 0.061008\n",
      "batch 3635: loss 0.099061\n",
      "batch 3636: loss 0.045256\n",
      "batch 3637: loss 0.084997\n",
      "batch 3638: loss 0.035782\n",
      "batch 3639: loss 0.041136\n",
      "batch 3640: loss 0.048641\n",
      "batch 3641: loss 0.120767\n",
      "batch 3642: loss 0.047353\n",
      "batch 3643: loss 0.074738\n",
      "batch 3644: loss 0.031187\n",
      "batch 3645: loss 0.092014\n",
      "batch 3646: loss 0.015715\n",
      "batch 3647: loss 0.123705\n",
      "batch 3648: loss 0.061776\n",
      "batch 3649: loss 0.177697\n",
      "batch 3650: loss 0.020702\n",
      "batch 3651: loss 0.049123\n",
      "batch 3652: loss 0.164423\n",
      "batch 3653: loss 0.161190\n",
      "batch 3654: loss 0.048973\n",
      "batch 3655: loss 0.079075\n",
      "batch 3656: loss 0.106010\n",
      "batch 3657: loss 0.078581\n",
      "batch 3658: loss 0.020324\n",
      "batch 3659: loss 0.119456\n",
      "batch 3660: loss 0.053651\n",
      "batch 3661: loss 0.021635\n",
      "batch 3662: loss 0.156362\n",
      "batch 3663: loss 0.131197\n",
      "batch 3664: loss 0.080352\n",
      "batch 3665: loss 0.064425\n",
      "batch 3666: loss 0.183372\n",
      "batch 3667: loss 0.045128\n",
      "batch 3668: loss 0.082785\n",
      "batch 3669: loss 0.048599\n",
      "batch 3670: loss 0.204177\n",
      "batch 3671: loss 0.048630\n",
      "batch 3672: loss 0.074417\n",
      "batch 3673: loss 0.048344\n",
      "batch 3674: loss 0.028975\n",
      "batch 3675: loss 0.093597\n",
      "batch 3676: loss 0.074122\n",
      "batch 3677: loss 0.020730\n",
      "batch 3678: loss 0.017285\n",
      "batch 3679: loss 0.012707\n",
      "batch 3680: loss 0.031980\n",
      "batch 3681: loss 0.029716\n",
      "batch 3682: loss 0.038289\n",
      "batch 3683: loss 0.095847\n",
      "batch 3684: loss 0.084656\n",
      "batch 3685: loss 0.045359\n",
      "batch 3686: loss 0.036199\n",
      "batch 3687: loss 0.031323\n",
      "batch 3688: loss 0.056880\n",
      "batch 3689: loss 0.145232\n",
      "batch 3690: loss 0.059743\n",
      "batch 3691: loss 0.098499\n",
      "batch 3692: loss 0.030732\n",
      "batch 3693: loss 0.034103\n",
      "batch 3694: loss 0.043030\n",
      "batch 3695: loss 0.045800\n",
      "batch 3696: loss 0.068200\n",
      "batch 3697: loss 0.319007\n",
      "batch 3698: loss 0.012817\n",
      "batch 3699: loss 0.064385\n",
      "batch 3700: loss 0.042847\n",
      "batch 3701: loss 0.019556\n",
      "batch 3702: loss 0.062371\n",
      "batch 3703: loss 0.023346\n",
      "batch 3704: loss 0.061760\n",
      "batch 3705: loss 0.046043\n",
      "batch 3706: loss 0.059792\n",
      "batch 3707: loss 0.091819\n",
      "batch 3708: loss 0.040176\n",
      "batch 3709: loss 0.190503\n",
      "batch 3710: loss 0.008351\n",
      "batch 3711: loss 0.026425\n",
      "batch 3712: loss 0.019753\n",
      "batch 3713: loss 0.024790\n",
      "batch 3714: loss 0.017344\n",
      "batch 3715: loss 0.030964\n",
      "batch 3716: loss 0.164310\n",
      "batch 3717: loss 0.085160\n",
      "batch 3718: loss 0.059118\n",
      "batch 3719: loss 0.066292\n",
      "batch 3720: loss 0.073177\n",
      "batch 3721: loss 0.048707\n",
      "batch 3722: loss 0.139404\n",
      "batch 3723: loss 0.027998\n",
      "batch 3724: loss 0.016044\n",
      "batch 3725: loss 0.018615\n",
      "batch 3726: loss 0.076658\n",
      "batch 3727: loss 0.063637\n",
      "batch 3728: loss 0.017858\n",
      "batch 3729: loss 0.023985\n",
      "batch 3730: loss 0.021725\n",
      "batch 3731: loss 0.065699\n",
      "batch 3732: loss 0.029857\n",
      "batch 3733: loss 0.169664\n",
      "batch 3734: loss 0.022436\n",
      "batch 3735: loss 0.183655\n",
      "batch 3736: loss 0.081959\n",
      "batch 3737: loss 0.048268\n",
      "batch 3738: loss 0.142648\n",
      "batch 3739: loss 0.125842\n",
      "batch 3740: loss 0.040271\n",
      "batch 3741: loss 0.018776\n",
      "batch 3742: loss 0.039753\n",
      "batch 3743: loss 0.046823\n",
      "batch 3744: loss 0.024423\n",
      "batch 3745: loss 0.026378\n",
      "batch 3746: loss 0.022818\n",
      "batch 3747: loss 0.023216\n",
      "batch 3748: loss 0.036476\n",
      "batch 3749: loss 0.028790\n",
      "batch 3750: loss 0.113177\n",
      "batch 3751: loss 0.044406\n",
      "batch 3752: loss 0.026433\n",
      "batch 3753: loss 0.183167\n",
      "batch 3754: loss 0.024374\n",
      "batch 3755: loss 0.026728\n",
      "batch 3756: loss 0.029736\n",
      "batch 3757: loss 0.040921\n",
      "batch 3758: loss 0.028599\n",
      "batch 3759: loss 0.072043\n",
      "batch 3760: loss 0.092305\n",
      "batch 3761: loss 0.063507\n",
      "batch 3762: loss 0.046450\n",
      "batch 3763: loss 0.016036\n",
      "batch 3764: loss 0.036472\n",
      "batch 3765: loss 0.054758\n",
      "batch 3766: loss 0.044121\n",
      "batch 3767: loss 0.019628\n",
      "batch 3768: loss 0.211091\n",
      "batch 3769: loss 0.029864\n",
      "batch 3770: loss 0.038595\n",
      "batch 3771: loss 0.096145\n",
      "batch 3772: loss 0.145804\n",
      "batch 3773: loss 0.181156\n",
      "batch 3774: loss 0.136843\n",
      "batch 3775: loss 0.053469\n",
      "batch 3776: loss 0.023971\n",
      "batch 3777: loss 0.097286\n",
      "batch 3778: loss 0.005031\n",
      "batch 3779: loss 0.141566\n",
      "batch 3780: loss 0.008900\n",
      "batch 3781: loss 0.034516\n",
      "batch 3782: loss 0.088568\n",
      "batch 3783: loss 0.039226\n",
      "batch 3784: loss 0.169993\n",
      "batch 3785: loss 0.073400\n",
      "batch 3786: loss 0.038630\n",
      "batch 3787: loss 0.138405\n",
      "batch 3788: loss 0.083454\n",
      "batch 3789: loss 0.032787\n",
      "batch 3790: loss 0.129919\n",
      "batch 3791: loss 0.026028\n",
      "batch 3792: loss 0.020309\n",
      "batch 3793: loss 0.061739\n",
      "batch 3794: loss 0.044226\n",
      "batch 3795: loss 0.081246\n",
      "batch 3796: loss 0.018513\n",
      "batch 3797: loss 0.025364\n",
      "batch 3798: loss 0.083128\n",
      "batch 3799: loss 0.029311\n",
      "batch 3800: loss 0.055224\n",
      "batch 3801: loss 0.139300\n",
      "batch 3802: loss 0.110372\n",
      "batch 3803: loss 0.024079\n",
      "batch 3804: loss 0.063203\n",
      "batch 3805: loss 0.050557\n",
      "batch 3806: loss 0.093232\n",
      "batch 3807: loss 0.032747\n",
      "batch 3808: loss 0.022514\n",
      "batch 3809: loss 0.074939\n",
      "batch 3810: loss 0.022956\n",
      "batch 3811: loss 0.025986\n",
      "batch 3812: loss 0.055899\n",
      "batch 3813: loss 0.275274\n",
      "batch 3814: loss 0.022899\n",
      "batch 3815: loss 0.063389\n",
      "batch 3816: loss 0.159571\n",
      "batch 3817: loss 0.042772\n",
      "batch 3818: loss 0.062581\n",
      "batch 3819: loss 0.058425\n",
      "batch 3820: loss 0.024951\n",
      "batch 3821: loss 0.036856\n",
      "batch 3822: loss 0.029147\n",
      "batch 3823: loss 0.135405\n",
      "batch 3824: loss 0.032445\n",
      "batch 3825: loss 0.116462\n",
      "batch 3826: loss 0.092387\n",
      "batch 3827: loss 0.115550\n",
      "batch 3828: loss 0.046759\n",
      "batch 3829: loss 0.040817\n",
      "batch 3830: loss 0.081992\n",
      "batch 3831: loss 0.033227\n",
      "batch 3832: loss 0.021480\n",
      "batch 3833: loss 0.019442\n",
      "batch 3834: loss 0.006580\n",
      "batch 3835: loss 0.053557\n",
      "batch 3836: loss 0.286135\n",
      "batch 3837: loss 0.099551\n",
      "batch 3838: loss 0.018564\n",
      "batch 3839: loss 0.142581\n",
      "batch 3840: loss 0.048757\n",
      "batch 3841: loss 0.008465\n",
      "batch 3842: loss 0.064399\n",
      "batch 3843: loss 0.009917\n",
      "batch 3844: loss 0.277912\n",
      "batch 3845: loss 0.032680\n",
      "batch 3846: loss 0.036111\n",
      "batch 3847: loss 0.093811\n",
      "batch 3848: loss 0.072978\n",
      "batch 3849: loss 0.112084\n",
      "batch 3850: loss 0.136796\n",
      "batch 3851: loss 0.056289\n",
      "batch 3852: loss 0.100471\n",
      "batch 3853: loss 0.026729\n",
      "batch 3854: loss 0.061184\n",
      "batch 3855: loss 0.116695\n",
      "batch 3856: loss 0.019567\n",
      "batch 3857: loss 0.130784\n",
      "batch 3858: loss 0.042268\n",
      "batch 3859: loss 0.018830\n",
      "batch 3860: loss 0.027214\n",
      "batch 3861: loss 0.126011\n",
      "batch 3862: loss 0.036717\n",
      "batch 3863: loss 0.040231\n",
      "batch 3864: loss 0.093423\n",
      "batch 3865: loss 0.191853\n",
      "batch 3866: loss 0.042856\n",
      "batch 3867: loss 0.053261\n",
      "batch 3868: loss 0.050338\n",
      "batch 3869: loss 0.068939\n",
      "batch 3870: loss 0.018162\n",
      "batch 3871: loss 0.046119\n",
      "batch 3872: loss 0.202243\n",
      "batch 3873: loss 0.100866\n",
      "batch 3874: loss 0.133176\n",
      "batch 3875: loss 0.051632\n",
      "batch 3876: loss 0.105096\n",
      "batch 3877: loss 0.052629\n",
      "batch 3878: loss 0.065890\n",
      "batch 3879: loss 0.183477\n",
      "batch 3880: loss 0.182979\n",
      "batch 3881: loss 0.019752\n",
      "batch 3882: loss 0.090427\n",
      "batch 3883: loss 0.128046\n",
      "batch 3884: loss 0.047571\n",
      "batch 3885: loss 0.195187\n",
      "batch 3886: loss 0.030628\n",
      "batch 3887: loss 0.043639\n",
      "batch 3888: loss 0.051417\n",
      "batch 3889: loss 0.077042\n",
      "batch 3890: loss 0.067742\n",
      "batch 3891: loss 0.112361\n",
      "batch 3892: loss 0.213785\n",
      "batch 3893: loss 0.061500\n",
      "batch 3894: loss 0.101569\n",
      "batch 3895: loss 0.034691\n",
      "batch 3896: loss 0.056014\n",
      "batch 3897: loss 0.136387\n",
      "batch 3898: loss 0.030418\n",
      "batch 3899: loss 0.093030\n",
      "batch 3900: loss 0.125321\n",
      "batch 3901: loss 0.089794\n",
      "batch 3902: loss 0.094518\n",
      "batch 3903: loss 0.036848\n",
      "batch 3904: loss 0.088828\n",
      "batch 3905: loss 0.101485\n",
      "batch 3906: loss 0.081937\n",
      "batch 3907: loss 0.113856\n",
      "batch 3908: loss 0.180526\n",
      "batch 3909: loss 0.044484\n",
      "batch 3910: loss 0.021549\n",
      "batch 3911: loss 0.069607\n",
      "batch 3912: loss 0.056403\n",
      "batch 3913: loss 0.131493\n",
      "batch 3914: loss 0.031310\n",
      "batch 3915: loss 0.061265\n",
      "batch 3916: loss 0.045093\n",
      "batch 3917: loss 0.053453\n",
      "batch 3918: loss 0.020256\n",
      "batch 3919: loss 0.058388\n",
      "batch 3920: loss 0.036900\n",
      "batch 3921: loss 0.069262\n",
      "batch 3922: loss 0.165802\n",
      "batch 3923: loss 0.047902\n",
      "batch 3924: loss 0.021271\n",
      "batch 3925: loss 0.096364\n",
      "batch 3926: loss 0.017321\n",
      "batch 3927: loss 0.034396\n",
      "batch 3928: loss 0.013525\n",
      "batch 3929: loss 0.084988\n",
      "batch 3930: loss 0.013787\n",
      "batch 3931: loss 0.063326\n",
      "batch 3932: loss 0.096000\n",
      "batch 3933: loss 0.075323\n",
      "batch 3934: loss 0.054736\n",
      "batch 3935: loss 0.033219\n",
      "batch 3936: loss 0.085058\n",
      "batch 3937: loss 0.056427\n",
      "batch 3938: loss 0.062784\n",
      "batch 3939: loss 0.098618\n",
      "batch 3940: loss 0.010263\n",
      "batch 3941: loss 0.097990\n",
      "batch 3942: loss 0.076654\n",
      "batch 3943: loss 0.063420\n",
      "batch 3944: loss 0.091705\n",
      "batch 3945: loss 0.230775\n",
      "batch 3946: loss 0.079648\n",
      "batch 3947: loss 0.069705\n",
      "batch 3948: loss 0.087529\n",
      "batch 3949: loss 0.081179\n",
      "batch 3950: loss 0.189621\n",
      "batch 3951: loss 0.083036\n",
      "batch 3952: loss 0.128623\n",
      "batch 3953: loss 0.011250\n",
      "batch 3954: loss 0.053945\n",
      "batch 3955: loss 0.033094\n",
      "batch 3956: loss 0.059264\n",
      "batch 3957: loss 0.027801\n",
      "batch 3958: loss 0.102373\n",
      "batch 3959: loss 0.019142\n",
      "batch 3960: loss 0.017491\n",
      "batch 3961: loss 0.007192\n",
      "batch 3962: loss 0.026522\n",
      "batch 3963: loss 0.098621\n",
      "batch 3964: loss 0.047826\n",
      "batch 3965: loss 0.022905\n",
      "batch 3966: loss 0.081669\n",
      "batch 3967: loss 0.063332\n",
      "batch 3968: loss 0.139507\n",
      "batch 3969: loss 0.011357\n",
      "batch 3970: loss 0.036820\n",
      "batch 3971: loss 0.187007\n",
      "batch 3972: loss 0.066680\n",
      "batch 3973: loss 0.087654\n",
      "batch 3974: loss 0.115439\n",
      "batch 3975: loss 0.086443\n",
      "batch 3976: loss 0.053118\n",
      "batch 3977: loss 0.095485\n",
      "batch 3978: loss 0.140760\n",
      "batch 3979: loss 0.045050\n",
      "batch 3980: loss 0.046128\n",
      "batch 3981: loss 0.089551\n",
      "batch 3982: loss 0.027390\n",
      "batch 3983: loss 0.114060\n",
      "batch 3984: loss 0.129386\n",
      "batch 3985: loss 0.052636\n",
      "batch 3986: loss 0.034205\n",
      "batch 3987: loss 0.035350\n",
      "batch 3988: loss 0.017650\n",
      "batch 3989: loss 0.049100\n",
      "batch 3990: loss 0.048662\n",
      "batch 3991: loss 0.102134\n",
      "batch 3992: loss 0.058889\n",
      "batch 3993: loss 0.039918\n",
      "batch 3994: loss 0.042753\n",
      "batch 3995: loss 0.023602\n",
      "batch 3996: loss 0.145380\n",
      "batch 3997: loss 0.047671\n",
      "batch 3998: loss 0.035117\n",
      "batch 3999: loss 0.023958\n",
      "batch 4000: loss 0.014909\n",
      "batch 4001: loss 0.009394\n",
      "batch 4002: loss 0.050932\n",
      "batch 4003: loss 0.014483\n",
      "batch 4004: loss 0.012420\n",
      "batch 4005: loss 0.108033\n",
      "batch 4006: loss 0.039756\n",
      "batch 4007: loss 0.016098\n",
      "batch 4008: loss 0.049085\n",
      "batch 4009: loss 0.058802\n",
      "batch 4010: loss 0.013758\n",
      "batch 4011: loss 0.023900\n",
      "batch 4012: loss 0.182137\n",
      "batch 4013: loss 0.048900\n",
      "batch 4014: loss 0.080190\n",
      "batch 4015: loss 0.045525\n",
      "batch 4016: loss 0.106513\n",
      "batch 4017: loss 0.013619\n",
      "batch 4018: loss 0.055347\n",
      "batch 4019: loss 0.031183\n",
      "batch 4020: loss 0.067329\n",
      "batch 4021: loss 0.016990\n",
      "batch 4022: loss 0.037434\n",
      "batch 4023: loss 0.027230\n",
      "batch 4024: loss 0.087893\n",
      "batch 4025: loss 0.079315\n",
      "batch 4026: loss 0.189801\n",
      "batch 4027: loss 0.016518\n",
      "batch 4028: loss 0.033328\n",
      "batch 4029: loss 0.069733\n",
      "batch 4030: loss 0.056193\n",
      "batch 4031: loss 0.089023\n",
      "batch 4032: loss 0.116336\n",
      "batch 4033: loss 0.012553\n",
      "batch 4034: loss 0.048272\n",
      "batch 4035: loss 0.042608\n",
      "batch 4036: loss 0.023761\n",
      "batch 4037: loss 0.040111\n",
      "batch 4038: loss 0.094393\n",
      "batch 4039: loss 0.077557\n",
      "batch 4040: loss 0.066208\n",
      "batch 4041: loss 0.065983\n",
      "batch 4042: loss 0.107579\n",
      "batch 4043: loss 0.020384\n",
      "batch 4044: loss 0.036076\n",
      "batch 4045: loss 0.073657\n",
      "batch 4046: loss 0.076737\n",
      "batch 4047: loss 0.071795\n",
      "batch 4048: loss 0.103824\n",
      "batch 4049: loss 0.056826\n",
      "batch 4050: loss 0.054067\n",
      "batch 4051: loss 0.126070\n",
      "batch 4052: loss 0.166522\n",
      "batch 4053: loss 0.046493\n",
      "batch 4054: loss 0.054288\n",
      "batch 4055: loss 0.039526\n",
      "batch 4056: loss 0.018648\n",
      "batch 4057: loss 0.041187\n",
      "batch 4058: loss 0.031702\n",
      "batch 4059: loss 0.060303\n",
      "batch 4060: loss 0.029598\n",
      "batch 4061: loss 0.118602\n",
      "batch 4062: loss 0.019250\n",
      "batch 4063: loss 0.023584\n",
      "batch 4064: loss 0.142740\n",
      "batch 4065: loss 0.016250\n",
      "batch 4066: loss 0.014528\n",
      "batch 4067: loss 0.030776\n",
      "batch 4068: loss 0.023177\n",
      "batch 4069: loss 0.014333\n",
      "batch 4070: loss 0.015324\n",
      "batch 4071: loss 0.043186\n",
      "batch 4072: loss 0.029045\n",
      "batch 4073: loss 0.158197\n",
      "batch 4074: loss 0.052979\n",
      "batch 4075: loss 0.072135\n",
      "batch 4076: loss 0.064285\n",
      "batch 4077: loss 0.293011\n",
      "batch 4078: loss 0.047017\n",
      "batch 4079: loss 0.023722\n",
      "batch 4080: loss 0.013786\n",
      "batch 4081: loss 0.074179\n",
      "batch 4082: loss 0.065811\n",
      "batch 4083: loss 0.099587\n",
      "batch 4084: loss 0.027096\n",
      "batch 4085: loss 0.116619\n",
      "batch 4086: loss 0.047712\n",
      "batch 4087: loss 0.139462\n",
      "batch 4088: loss 0.047976\n",
      "batch 4089: loss 0.015681\n",
      "batch 4090: loss 0.045161\n",
      "batch 4091: loss 0.147175\n",
      "batch 4092: loss 0.071289\n",
      "batch 4093: loss 0.042265\n",
      "batch 4094: loss 0.079863\n",
      "batch 4095: loss 0.058449\n",
      "batch 4096: loss 0.024607\n",
      "batch 4097: loss 0.018318\n",
      "batch 4098: loss 0.075403\n",
      "batch 4099: loss 0.078342\n",
      "batch 4100: loss 0.086261\n",
      "batch 4101: loss 0.022583\n",
      "batch 4102: loss 0.064951\n",
      "batch 4103: loss 0.033738\n",
      "batch 4104: loss 0.036083\n",
      "batch 4105: loss 0.040928\n",
      "batch 4106: loss 0.166227\n",
      "batch 4107: loss 0.039970\n",
      "batch 4108: loss 0.093406\n",
      "batch 4109: loss 0.033886\n",
      "batch 4110: loss 0.067330\n",
      "batch 4111: loss 0.053906\n",
      "batch 4112: loss 0.093201\n",
      "batch 4113: loss 0.130900\n",
      "batch 4114: loss 0.026679\n",
      "batch 4115: loss 0.050086\n",
      "batch 4116: loss 0.253037\n",
      "batch 4117: loss 0.035928\n",
      "batch 4118: loss 0.253501\n",
      "batch 4119: loss 0.052766\n",
      "batch 4120: loss 0.038935\n",
      "batch 4121: loss 0.139005\n",
      "batch 4122: loss 0.123116\n",
      "batch 4123: loss 0.012039\n",
      "batch 4124: loss 0.060992\n",
      "batch 4125: loss 0.047512\n",
      "batch 4126: loss 0.037678\n",
      "batch 4127: loss 0.078965\n",
      "batch 4128: loss 0.018200\n",
      "batch 4129: loss 0.129648\n",
      "batch 4130: loss 0.090992\n",
      "batch 4131: loss 0.119652\n",
      "batch 4132: loss 0.081599\n",
      "batch 4133: loss 0.156227\n",
      "batch 4134: loss 0.010399\n",
      "batch 4135: loss 0.134328\n",
      "batch 4136: loss 0.026159\n",
      "batch 4137: loss 0.138553\n",
      "batch 4138: loss 0.041914\n",
      "batch 4139: loss 0.073552\n",
      "batch 4140: loss 0.017636\n",
      "batch 4141: loss 0.012035\n",
      "batch 4142: loss 0.089851\n",
      "batch 4143: loss 0.069666\n",
      "batch 4144: loss 0.026559\n",
      "batch 4145: loss 0.015099\n",
      "batch 4146: loss 0.012360\n",
      "batch 4147: loss 0.046405\n",
      "batch 4148: loss 0.064607\n",
      "batch 4149: loss 0.029683\n",
      "batch 4150: loss 0.038361\n",
      "batch 4151: loss 0.118548\n",
      "batch 4152: loss 0.109584\n",
      "batch 4153: loss 0.153251\n",
      "batch 4154: loss 0.024033\n",
      "batch 4155: loss 0.016386\n",
      "batch 4156: loss 0.115157\n",
      "batch 4157: loss 0.071263\n",
      "batch 4158: loss 0.014906\n",
      "batch 4159: loss 0.009804\n",
      "batch 4160: loss 0.180822\n",
      "batch 4161: loss 0.093180\n",
      "batch 4162: loss 0.099238\n",
      "batch 4163: loss 0.028044\n",
      "batch 4164: loss 0.095951\n",
      "batch 4165: loss 0.105074\n",
      "batch 4166: loss 0.131337\n",
      "batch 4167: loss 0.049457\n",
      "batch 4168: loss 0.023757\n",
      "batch 4169: loss 0.042754\n",
      "batch 4170: loss 0.022798\n",
      "batch 4171: loss 0.009654\n",
      "batch 4172: loss 0.082916\n",
      "batch 4173: loss 0.114961\n",
      "batch 4174: loss 0.158368\n",
      "batch 4175: loss 0.112930\n",
      "batch 4176: loss 0.066558\n",
      "batch 4177: loss 0.064586\n",
      "batch 4178: loss 0.059371\n",
      "batch 4179: loss 0.124836\n",
      "batch 4180: loss 0.030750\n",
      "batch 4181: loss 0.026637\n",
      "batch 4182: loss 0.030213\n",
      "batch 4183: loss 0.037376\n",
      "batch 4184: loss 0.035050\n",
      "batch 4185: loss 0.065902\n",
      "batch 4186: loss 0.026509\n",
      "batch 4187: loss 0.034577\n",
      "batch 4188: loss 0.125979\n",
      "batch 4189: loss 0.178997\n",
      "batch 4190: loss 0.032483\n",
      "batch 4191: loss 0.073053\n",
      "batch 4192: loss 0.035987\n",
      "batch 4193: loss 0.028878\n",
      "batch 4194: loss 0.010406\n",
      "batch 4195: loss 0.057003\n",
      "batch 4196: loss 0.106926\n",
      "batch 4197: loss 0.060321\n",
      "batch 4198: loss 0.090792\n",
      "batch 4199: loss 0.035147\n",
      "batch 4200: loss 0.064643\n",
      "batch 4201: loss 0.029300\n",
      "batch 4202: loss 0.102726\n",
      "batch 4203: loss 0.074003\n",
      "batch 4204: loss 0.037612\n",
      "batch 4205: loss 0.060655\n",
      "batch 4206: loss 0.087018\n",
      "batch 4207: loss 0.070826\n",
      "batch 4208: loss 0.085075\n",
      "batch 4209: loss 0.051996\n",
      "batch 4210: loss 0.027704\n",
      "batch 4211: loss 0.033194\n",
      "batch 4212: loss 0.047882\n",
      "batch 4213: loss 0.020064\n",
      "batch 4214: loss 0.139657\n",
      "batch 4215: loss 0.051904\n",
      "batch 4216: loss 0.010913\n",
      "batch 4217: loss 0.052109\n",
      "batch 4218: loss 0.099459\n",
      "batch 4219: loss 0.114621\n",
      "batch 4220: loss 0.023635\n",
      "batch 4221: loss 0.023625\n",
      "batch 4222: loss 0.030220\n",
      "batch 4223: loss 0.029676\n",
      "batch 4224: loss 0.030763\n",
      "batch 4225: loss 0.023748\n",
      "batch 4226: loss 0.075732\n",
      "batch 4227: loss 0.050684\n",
      "batch 4228: loss 0.064267\n",
      "batch 4229: loss 0.183729\n",
      "batch 4230: loss 0.046150\n",
      "batch 4231: loss 0.015324\n",
      "batch 4232: loss 0.018906\n",
      "batch 4233: loss 0.024644\n",
      "batch 4234: loss 0.028849\n",
      "batch 4235: loss 0.039056\n",
      "batch 4236: loss 0.140484\n",
      "batch 4237: loss 0.139553\n",
      "batch 4238: loss 0.017286\n",
      "batch 4239: loss 0.102609\n",
      "batch 4240: loss 0.031668\n",
      "batch 4241: loss 0.018176\n",
      "batch 4242: loss 0.029501\n",
      "batch 4243: loss 0.074687\n",
      "batch 4244: loss 0.023483\n",
      "batch 4245: loss 0.075574\n",
      "batch 4246: loss 0.071053\n",
      "batch 4247: loss 0.030088\n",
      "batch 4248: loss 0.156258\n",
      "batch 4249: loss 0.064127\n",
      "batch 4250: loss 0.090029\n",
      "batch 4251: loss 0.030556\n",
      "batch 4252: loss 0.079153\n",
      "batch 4253: loss 0.109875\n",
      "batch 4254: loss 0.013354\n",
      "batch 4255: loss 0.108234\n",
      "batch 4256: loss 0.094916\n",
      "batch 4257: loss 0.280934\n",
      "batch 4258: loss 0.123277\n",
      "batch 4259: loss 0.038053\n",
      "batch 4260: loss 0.045570\n",
      "batch 4261: loss 0.129963\n",
      "batch 4262: loss 0.090305\n",
      "batch 4263: loss 0.023298\n",
      "batch 4264: loss 0.199621\n",
      "batch 4265: loss 0.050200\n",
      "batch 4266: loss 0.027985\n",
      "batch 4267: loss 0.053221\n",
      "batch 4268: loss 0.081388\n",
      "batch 4269: loss 0.037145\n",
      "batch 4270: loss 0.083253\n",
      "batch 4271: loss 0.164471\n",
      "batch 4272: loss 0.130858\n",
      "batch 4273: loss 0.051466\n",
      "batch 4274: loss 0.104648\n",
      "batch 4275: loss 0.034224\n",
      "batch 4276: loss 0.115050\n",
      "batch 4277: loss 0.031593\n",
      "batch 4278: loss 0.019004\n",
      "batch 4279: loss 0.017977\n",
      "batch 4280: loss 0.035202\n",
      "batch 4281: loss 0.135298\n",
      "batch 4282: loss 0.032088\n",
      "batch 4283: loss 0.053190\n",
      "batch 4284: loss 0.028274\n",
      "batch 4285: loss 0.098742\n",
      "batch 4286: loss 0.048215\n",
      "batch 4287: loss 0.101870\n",
      "batch 4288: loss 0.019573\n",
      "batch 4289: loss 0.143992\n",
      "batch 4290: loss 0.099302\n",
      "batch 4291: loss 0.045319\n",
      "batch 4292: loss 0.026372\n",
      "batch 4293: loss 0.101820\n",
      "batch 4294: loss 0.094312\n",
      "batch 4295: loss 0.033087\n",
      "batch 4296: loss 0.018135\n",
      "batch 4297: loss 0.032139\n",
      "batch 4298: loss 0.010037\n",
      "batch 4299: loss 0.052849\n",
      "batch 4300: loss 0.080001\n",
      "batch 4301: loss 0.127317\n",
      "batch 4302: loss 0.069176\n",
      "batch 4303: loss 0.075350\n",
      "batch 4304: loss 0.022807\n",
      "batch 4305: loss 0.044098\n",
      "batch 4306: loss 0.232717\n",
      "batch 4307: loss 0.021457\n",
      "batch 4308: loss 0.117462\n",
      "batch 4309: loss 0.014144\n",
      "batch 4310: loss 0.276900\n",
      "batch 4311: loss 0.053706\n",
      "batch 4312: loss 0.027843\n",
      "batch 4313: loss 0.009817\n",
      "batch 4314: loss 0.027509\n",
      "batch 4315: loss 0.059774\n",
      "batch 4316: loss 0.095036\n",
      "batch 4317: loss 0.106294\n",
      "batch 4318: loss 0.021152\n",
      "batch 4319: loss 0.020844\n",
      "batch 4320: loss 0.063257\n",
      "batch 4321: loss 0.085543\n",
      "batch 4322: loss 0.104484\n",
      "batch 4323: loss 0.125557\n",
      "batch 4324: loss 0.079900\n",
      "batch 4325: loss 0.017006\n",
      "batch 4326: loss 0.013183\n",
      "batch 4327: loss 0.013294\n",
      "batch 4328: loss 0.082024\n",
      "batch 4329: loss 0.125205\n",
      "batch 4330: loss 0.074326\n",
      "batch 4331: loss 0.025438\n",
      "batch 4332: loss 0.128559\n",
      "batch 4333: loss 0.112874\n",
      "batch 4334: loss 0.076086\n",
      "batch 4335: loss 0.022391\n",
      "batch 4336: loss 0.013611\n",
      "batch 4337: loss 0.071970\n",
      "batch 4338: loss 0.048169\n",
      "batch 4339: loss 0.078815\n",
      "batch 4340: loss 0.052777\n",
      "batch 4341: loss 0.084012\n",
      "batch 4342: loss 0.036604\n",
      "batch 4343: loss 0.078151\n",
      "batch 4344: loss 0.429667\n",
      "batch 4345: loss 0.015211\n",
      "batch 4346: loss 0.025624\n",
      "batch 4347: loss 0.079342\n",
      "batch 4348: loss 0.060111\n",
      "batch 4349: loss 0.013849\n",
      "batch 4350: loss 0.065886\n",
      "batch 4351: loss 0.009383\n",
      "batch 4352: loss 0.102343\n",
      "batch 4353: loss 0.084205\n",
      "batch 4354: loss 0.056338\n",
      "batch 4355: loss 0.101593\n",
      "batch 4356: loss 0.062277\n",
      "batch 4357: loss 0.031366\n",
      "batch 4358: loss 0.166037\n",
      "batch 4359: loss 0.129966\n",
      "batch 4360: loss 0.061778\n",
      "batch 4361: loss 0.032400\n",
      "batch 4362: loss 0.077102\n",
      "batch 4363: loss 0.047529\n",
      "batch 4364: loss 0.025867\n",
      "batch 4365: loss 0.060479\n",
      "batch 4366: loss 0.070609\n",
      "batch 4367: loss 0.073699\n",
      "batch 4368: loss 0.016352\n",
      "batch 4369: loss 0.114122\n",
      "batch 4370: loss 0.052081\n",
      "batch 4371: loss 0.056682\n",
      "batch 4372: loss 0.095566\n",
      "batch 4373: loss 0.037569\n",
      "batch 4374: loss 0.109630\n",
      "batch 4375: loss 0.045968\n",
      "batch 4376: loss 0.230627\n",
      "batch 4377: loss 0.029746\n",
      "batch 4378: loss 0.076865\n",
      "batch 4379: loss 0.227693\n",
      "batch 4380: loss 0.004280\n",
      "batch 4381: loss 0.270062\n",
      "batch 4382: loss 0.041648\n",
      "batch 4383: loss 0.035842\n",
      "batch 4384: loss 0.109560\n",
      "batch 4385: loss 0.024780\n",
      "batch 4386: loss 0.027271\n",
      "batch 4387: loss 0.078213\n",
      "batch 4388: loss 0.037119\n",
      "batch 4389: loss 0.071844\n",
      "batch 4390: loss 0.033168\n",
      "batch 4391: loss 0.033183\n",
      "batch 4392: loss 0.053534\n",
      "batch 4393: loss 0.004762\n",
      "batch 4394: loss 0.028245\n",
      "batch 4395: loss 0.065281\n",
      "batch 4396: loss 0.232262\n",
      "batch 4397: loss 0.121046\n",
      "batch 4398: loss 0.043048\n",
      "batch 4399: loss 0.007941\n",
      "batch 4400: loss 0.107315\n",
      "batch 4401: loss 0.064198\n",
      "batch 4402: loss 0.019038\n",
      "batch 4403: loss 0.036657\n",
      "batch 4404: loss 0.017371\n",
      "batch 4405: loss 0.026803\n",
      "batch 4406: loss 0.136844\n",
      "batch 4407: loss 0.084898\n",
      "batch 4408: loss 0.040820\n",
      "batch 4409: loss 0.070570\n",
      "batch 4410: loss 0.050255\n",
      "batch 4411: loss 0.131051\n",
      "batch 4412: loss 0.042160\n",
      "batch 4413: loss 0.026331\n",
      "batch 4414: loss 0.088872\n",
      "batch 4415: loss 0.064680\n",
      "batch 4416: loss 0.095362\n",
      "batch 4417: loss 0.064472\n",
      "batch 4418: loss 0.033674\n",
      "batch 4419: loss 0.047649\n",
      "batch 4420: loss 0.056874\n",
      "batch 4421: loss 0.044565\n",
      "batch 4422: loss 0.152708\n",
      "batch 4423: loss 0.111723\n",
      "batch 4424: loss 0.040077\n",
      "batch 4425: loss 0.024008\n",
      "batch 4426: loss 0.108161\n",
      "batch 4427: loss 0.090394\n",
      "batch 4428: loss 0.029797\n",
      "batch 4429: loss 0.040031\n",
      "batch 4430: loss 0.069191\n",
      "batch 4431: loss 0.020827\n",
      "batch 4432: loss 0.127310\n",
      "batch 4433: loss 0.158423\n",
      "batch 4434: loss 0.099944\n",
      "batch 4435: loss 0.094549\n",
      "batch 4436: loss 0.113240\n",
      "batch 4437: loss 0.054801\n",
      "batch 4438: loss 0.056744\n",
      "batch 4439: loss 0.021240\n",
      "batch 4440: loss 0.098478\n",
      "batch 4441: loss 0.099228\n",
      "batch 4442: loss 0.121044\n",
      "batch 4443: loss 0.096966\n",
      "batch 4444: loss 0.075481\n",
      "batch 4445: loss 0.015968\n",
      "batch 4446: loss 0.019957\n",
      "batch 4447: loss 0.176497\n",
      "batch 4448: loss 0.032993\n",
      "batch 4449: loss 0.034619\n",
      "batch 4450: loss 0.052936\n",
      "batch 4451: loss 0.024362\n",
      "batch 4452: loss 0.041881\n",
      "batch 4453: loss 0.157058\n",
      "batch 4454: loss 0.097961\n",
      "batch 4455: loss 0.064798\n",
      "batch 4456: loss 0.073334\n",
      "batch 4457: loss 0.042855\n",
      "batch 4458: loss 0.058481\n",
      "batch 4459: loss 0.090958\n",
      "batch 4460: loss 0.150565\n",
      "batch 4461: loss 0.130332\n",
      "batch 4462: loss 0.084439\n",
      "batch 4463: loss 0.028472\n",
      "batch 4464: loss 0.051988\n",
      "batch 4465: loss 0.072626\n",
      "batch 4466: loss 0.003299\n",
      "batch 4467: loss 0.009471\n",
      "batch 4468: loss 0.079946\n",
      "batch 4469: loss 0.040722\n",
      "batch 4470: loss 0.289796\n",
      "batch 4471: loss 0.113112\n",
      "batch 4472: loss 0.215970\n",
      "batch 4473: loss 0.064387\n",
      "batch 4474: loss 0.032654\n",
      "batch 4475: loss 0.050652\n",
      "batch 4476: loss 0.257829\n",
      "batch 4477: loss 0.035656\n",
      "batch 4478: loss 0.037274\n",
      "batch 4479: loss 0.059030\n",
      "batch 4480: loss 0.015818\n",
      "batch 4481: loss 0.109572\n",
      "batch 4482: loss 0.250786\n",
      "batch 4483: loss 0.126361\n",
      "batch 4484: loss 0.080771\n",
      "batch 4485: loss 0.062937\n",
      "batch 4486: loss 0.074317\n",
      "batch 4487: loss 0.065046\n",
      "batch 4488: loss 0.089330\n",
      "batch 4489: loss 0.024831\n",
      "batch 4490: loss 0.023312\n",
      "batch 4491: loss 0.033089\n",
      "batch 4492: loss 0.059102\n",
      "batch 4493: loss 0.096870\n",
      "batch 4494: loss 0.081183\n",
      "batch 4495: loss 0.021055\n",
      "batch 4496: loss 0.048459\n",
      "batch 4497: loss 0.029773\n",
      "batch 4498: loss 0.064197\n",
      "batch 4499: loss 0.013734\n",
      "batch 4500: loss 0.026996\n",
      "batch 4501: loss 0.144911\n",
      "batch 4502: loss 0.173845\n",
      "batch 4503: loss 0.132771\n",
      "batch 4504: loss 0.091112\n",
      "batch 4505: loss 0.027775\n",
      "batch 4506: loss 0.024348\n",
      "batch 4507: loss 0.029253\n",
      "batch 4508: loss 0.032701\n",
      "batch 4509: loss 0.068814\n",
      "batch 4510: loss 0.011106\n",
      "batch 4511: loss 0.082046\n",
      "batch 4512: loss 0.036366\n",
      "batch 4513: loss 0.073265\n",
      "batch 4514: loss 0.111072\n",
      "batch 4515: loss 0.013828\n",
      "batch 4516: loss 0.030143\n",
      "batch 4517: loss 0.036487\n",
      "batch 4518: loss 0.019108\n",
      "batch 4519: loss 0.017627\n",
      "batch 4520: loss 0.050571\n",
      "batch 4521: loss 0.133022\n",
      "batch 4522: loss 0.104645\n",
      "batch 4523: loss 0.071341\n",
      "batch 4524: loss 0.064860\n",
      "batch 4525: loss 0.034695\n",
      "batch 4526: loss 0.035135\n",
      "batch 4527: loss 0.102410\n",
      "batch 4528: loss 0.087897\n",
      "batch 4529: loss 0.034628\n",
      "batch 4530: loss 0.120797\n",
      "batch 4531: loss 0.176453\n",
      "batch 4532: loss 0.037312\n",
      "batch 4533: loss 0.081025\n",
      "batch 4534: loss 0.029539\n",
      "batch 4535: loss 0.068756\n",
      "batch 4536: loss 0.014891\n",
      "batch 4537: loss 0.083596\n",
      "batch 4538: loss 0.238996\n",
      "batch 4539: loss 0.332929\n",
      "batch 4540: loss 0.098064\n",
      "batch 4541: loss 0.023823\n",
      "batch 4542: loss 0.035567\n",
      "batch 4543: loss 0.023628\n",
      "batch 4544: loss 0.056376\n",
      "batch 4545: loss 0.078053\n",
      "batch 4546: loss 0.130311\n",
      "batch 4547: loss 0.025164\n",
      "batch 4548: loss 0.046989\n",
      "batch 4549: loss 0.036151\n",
      "batch 4550: loss 0.153020\n",
      "batch 4551: loss 0.022254\n",
      "batch 4552: loss 0.052288\n",
      "batch 4553: loss 0.119652\n",
      "batch 4554: loss 0.173253\n",
      "batch 4555: loss 0.045226\n",
      "batch 4556: loss 0.031685\n",
      "batch 4557: loss 0.089063\n",
      "batch 4558: loss 0.026370\n",
      "batch 4559: loss 0.031727\n",
      "batch 4560: loss 0.025906\n",
      "batch 4561: loss 0.011918\n",
      "batch 4562: loss 0.019646\n",
      "batch 4563: loss 0.158020\n",
      "batch 4564: loss 0.074155\n",
      "batch 4565: loss 0.044870\n",
      "batch 4566: loss 0.249065\n",
      "batch 4567: loss 0.018476\n",
      "batch 4568: loss 0.074705\n",
      "batch 4569: loss 0.056116\n",
      "batch 4570: loss 0.029347\n",
      "batch 4571: loss 0.068747\n",
      "batch 4572: loss 0.040793\n",
      "batch 4573: loss 0.131609\n",
      "batch 4574: loss 0.045741\n",
      "batch 4575: loss 0.064668\n",
      "batch 4576: loss 0.094450\n",
      "batch 4577: loss 0.038221\n",
      "batch 4578: loss 0.032597\n",
      "batch 4579: loss 0.031148\n",
      "batch 4580: loss 0.027777\n",
      "batch 4581: loss 0.160866\n",
      "batch 4582: loss 0.059556\n",
      "batch 4583: loss 0.021998\n",
      "batch 4584: loss 0.151715\n",
      "batch 4585: loss 0.037478\n",
      "batch 4586: loss 0.052711\n",
      "batch 4587: loss 0.046812\n",
      "batch 4588: loss 0.072544\n",
      "batch 4589: loss 0.052118\n",
      "batch 4590: loss 0.018391\n",
      "batch 4591: loss 0.064326\n",
      "batch 4592: loss 0.090092\n",
      "batch 4593: loss 0.063122\n",
      "batch 4594: loss 0.092390\n",
      "batch 4595: loss 0.191788\n",
      "batch 4596: loss 0.052421\n",
      "batch 4597: loss 0.085729\n",
      "batch 4598: loss 0.012203\n",
      "batch 4599: loss 0.063634\n",
      "batch 4600: loss 0.065305\n",
      "batch 4601: loss 0.066843\n",
      "batch 4602: loss 0.061226\n",
      "batch 4603: loss 0.014404\n",
      "batch 4604: loss 0.061198\n",
      "batch 4605: loss 0.089356\n",
      "batch 4606: loss 0.087229\n",
      "batch 4607: loss 0.040915\n",
      "batch 4608: loss 0.051018\n",
      "batch 4609: loss 0.023092\n",
      "batch 4610: loss 0.177628\n",
      "batch 4611: loss 0.064315\n",
      "batch 4612: loss 0.093986\n",
      "batch 4613: loss 0.036710\n",
      "batch 4614: loss 0.013344\n",
      "batch 4615: loss 0.029059\n",
      "batch 4616: loss 0.050050\n",
      "batch 4617: loss 0.060387\n",
      "batch 4618: loss 0.041898\n",
      "batch 4619: loss 0.019482\n",
      "batch 4620: loss 0.009049\n",
      "batch 4621: loss 0.018593\n",
      "batch 4622: loss 0.126159\n",
      "batch 4623: loss 0.036738\n",
      "batch 4624: loss 0.044368\n",
      "batch 4625: loss 0.033684\n",
      "batch 4626: loss 0.080037\n",
      "batch 4627: loss 0.049452\n",
      "batch 4628: loss 0.045149\n",
      "batch 4629: loss 0.015314\n",
      "batch 4630: loss 0.021970\n",
      "batch 4631: loss 0.027936\n",
      "batch 4632: loss 0.022877\n",
      "batch 4633: loss 0.044630\n",
      "batch 4634: loss 0.043998\n",
      "batch 4635: loss 0.041838\n",
      "batch 4636: loss 0.076513\n",
      "batch 4637: loss 0.157143\n",
      "batch 4638: loss 0.024941\n",
      "batch 4639: loss 0.031798\n",
      "batch 4640: loss 0.093354\n",
      "batch 4641: loss 0.053033\n",
      "batch 4642: loss 0.050668\n",
      "batch 4643: loss 0.077622\n",
      "batch 4644: loss 0.074723\n",
      "batch 4645: loss 0.037540\n",
      "batch 4646: loss 0.023141\n",
      "batch 4647: loss 0.234849\n",
      "batch 4648: loss 0.059515\n",
      "batch 4649: loss 0.067542\n",
      "batch 4650: loss 0.068616\n",
      "batch 4651: loss 0.017854\n",
      "batch 4652: loss 0.069004\n",
      "batch 4653: loss 0.044673\n",
      "batch 4654: loss 0.028200\n",
      "batch 4655: loss 0.090202\n",
      "batch 4656: loss 0.040303\n",
      "batch 4657: loss 0.125818\n",
      "batch 4658: loss 0.010714\n",
      "batch 4659: loss 0.071899\n",
      "batch 4660: loss 0.033625\n",
      "batch 4661: loss 0.017747\n",
      "batch 4662: loss 0.062420\n",
      "batch 4663: loss 0.054184\n",
      "batch 4664: loss 0.047536\n",
      "batch 4665: loss 0.169251\n",
      "batch 4666: loss 0.036763\n",
      "batch 4667: loss 0.115619\n",
      "batch 4668: loss 0.172155\n",
      "batch 4669: loss 0.033765\n",
      "batch 4670: loss 0.031538\n",
      "batch 4671: loss 0.021907\n",
      "batch 4672: loss 0.035957\n",
      "batch 4673: loss 0.083665\n",
      "batch 4674: loss 0.196345\n",
      "batch 4675: loss 0.024848\n",
      "batch 4676: loss 0.110039\n",
      "batch 4677: loss 0.023233\n",
      "batch 4678: loss 0.048643\n",
      "batch 4679: loss 0.055774\n",
      "batch 4680: loss 0.133514\n",
      "batch 4681: loss 0.026652\n",
      "batch 4682: loss 0.029186\n",
      "batch 4683: loss 0.060683\n",
      "batch 4684: loss 0.066992\n",
      "batch 4685: loss 0.017089\n",
      "batch 4686: loss 0.104588\n",
      "batch 4687: loss 0.030467\n",
      "batch 4688: loss 0.036147\n",
      "batch 4689: loss 0.117893\n",
      "batch 4690: loss 0.049025\n",
      "batch 4691: loss 0.067507\n",
      "batch 4692: loss 0.026767\n",
      "batch 4693: loss 0.082131\n",
      "batch 4694: loss 0.019022\n",
      "batch 4695: loss 0.040748\n",
      "batch 4696: loss 0.035383\n",
      "batch 4697: loss 0.082974\n",
      "batch 4698: loss 0.020587\n",
      "batch 4699: loss 0.014126\n",
      "batch 4700: loss 0.097836\n",
      "batch 4701: loss 0.144731\n",
      "batch 4702: loss 0.099620\n",
      "batch 4703: loss 0.012036\n",
      "batch 4704: loss 0.058688\n",
      "batch 4705: loss 0.113237\n",
      "batch 4706: loss 0.029339\n",
      "batch 4707: loss 0.107600\n",
      "batch 4708: loss 0.014642\n",
      "batch 4709: loss 0.066831\n",
      "batch 4710: loss 0.091366\n",
      "batch 4711: loss 0.020830\n",
      "batch 4712: loss 0.023086\n",
      "batch 4713: loss 0.050512\n",
      "batch 4714: loss 0.011587\n",
      "batch 4715: loss 0.047022\n",
      "batch 4716: loss 0.141662\n",
      "batch 4717: loss 0.018700\n",
      "batch 4718: loss 0.039341\n",
      "batch 4719: loss 0.018475\n",
      "batch 4720: loss 0.072829\n",
      "batch 4721: loss 0.018788\n",
      "batch 4722: loss 0.024961\n",
      "batch 4723: loss 0.048227\n",
      "batch 4724: loss 0.096486\n",
      "batch 4725: loss 0.062077\n",
      "batch 4726: loss 0.163863\n",
      "batch 4727: loss 0.009015\n",
      "batch 4728: loss 0.131297\n",
      "batch 4729: loss 0.012784\n",
      "batch 4730: loss 0.011072\n",
      "batch 4731: loss 0.174932\n",
      "batch 4732: loss 0.056141\n",
      "batch 4733: loss 0.022914\n",
      "batch 4734: loss 0.028679\n",
      "batch 4735: loss 0.042768\n",
      "batch 4736: loss 0.049893\n",
      "batch 4737: loss 0.040489\n",
      "batch 4738: loss 0.037821\n",
      "batch 4739: loss 0.070836\n",
      "batch 4740: loss 0.081532\n",
      "batch 4741: loss 0.078988\n",
      "batch 4742: loss 0.119556\n",
      "batch 4743: loss 0.035223\n",
      "batch 4744: loss 0.022092\n",
      "batch 4745: loss 0.170371\n",
      "batch 4746: loss 0.036830\n",
      "batch 4747: loss 0.054940\n",
      "batch 4748: loss 0.043598\n",
      "batch 4749: loss 0.041605\n",
      "batch 4750: loss 0.050188\n",
      "batch 4751: loss 0.057743\n",
      "batch 4752: loss 0.101425\n",
      "batch 4753: loss 0.084064\n",
      "batch 4754: loss 0.082348\n",
      "batch 4755: loss 0.025481\n",
      "batch 4756: loss 0.049231\n",
      "batch 4757: loss 0.041189\n",
      "batch 4758: loss 0.072344\n",
      "batch 4759: loss 0.049340\n",
      "batch 4760: loss 0.121019\n",
      "batch 4761: loss 0.026262\n",
      "batch 4762: loss 0.089389\n",
      "batch 4763: loss 0.010706\n",
      "batch 4764: loss 0.168050\n",
      "batch 4765: loss 0.192425\n",
      "batch 4766: loss 0.020818\n",
      "batch 4767: loss 0.146121\n",
      "batch 4768: loss 0.017680\n",
      "batch 4769: loss 0.013598\n",
      "batch 4770: loss 0.034061\n",
      "batch 4771: loss 0.019161\n",
      "batch 4772: loss 0.112331\n",
      "batch 4773: loss 0.044048\n",
      "batch 4774: loss 0.101949\n",
      "batch 4775: loss 0.035243\n",
      "batch 4776: loss 0.035453\n",
      "batch 4777: loss 0.104317\n",
      "batch 4778: loss 0.027225\n",
      "batch 4779: loss 0.120721\n",
      "batch 4780: loss 0.075914\n",
      "batch 4781: loss 0.016852\n",
      "batch 4782: loss 0.062502\n",
      "batch 4783: loss 0.135431\n",
      "batch 4784: loss 0.013375\n",
      "batch 4785: loss 0.050923\n",
      "batch 4786: loss 0.053976\n",
      "batch 4787: loss 0.016144\n",
      "batch 4788: loss 0.053470\n",
      "batch 4789: loss 0.130901\n",
      "batch 4790: loss 0.086242\n",
      "batch 4791: loss 0.069801\n",
      "batch 4792: loss 0.072465\n",
      "batch 4793: loss 0.032757\n",
      "batch 4794: loss 0.029759\n",
      "batch 4795: loss 0.050227\n",
      "batch 4796: loss 0.055948\n",
      "batch 4797: loss 0.040460\n",
      "batch 4798: loss 0.033106\n",
      "batch 4799: loss 0.063143\n",
      "batch 4800: loss 0.018133\n",
      "batch 4801: loss 0.047090\n",
      "batch 4802: loss 0.069908\n",
      "batch 4803: loss 0.033628\n",
      "batch 4804: loss 0.063903\n",
      "batch 4805: loss 0.055702\n",
      "batch 4806: loss 0.032958\n",
      "batch 4807: loss 0.106877\n",
      "batch 4808: loss 0.009629\n",
      "batch 4809: loss 0.026796\n",
      "batch 4810: loss 0.167737\n",
      "batch 4811: loss 0.207912\n",
      "batch 4812: loss 0.013023\n",
      "batch 4813: loss 0.134792\n",
      "batch 4814: loss 0.077308\n",
      "batch 4815: loss 0.036858\n",
      "batch 4816: loss 0.076195\n",
      "batch 4817: loss 0.059781\n",
      "batch 4818: loss 0.041087\n",
      "batch 4819: loss 0.045437\n",
      "batch 4820: loss 0.029638\n",
      "batch 4821: loss 0.077639\n",
      "batch 4822: loss 0.007263\n",
      "batch 4823: loss 0.061454\n",
      "batch 4824: loss 0.130707\n",
      "batch 4825: loss 0.043457\n",
      "batch 4826: loss 0.029779\n",
      "batch 4827: loss 0.309004\n",
      "batch 4828: loss 0.012259\n",
      "batch 4829: loss 0.038197\n",
      "batch 4830: loss 0.021796\n",
      "batch 4831: loss 0.020534\n",
      "batch 4832: loss 0.013747\n",
      "batch 4833: loss 0.084727\n",
      "batch 4834: loss 0.014671\n",
      "batch 4835: loss 0.013052\n",
      "batch 4836: loss 0.085044\n",
      "batch 4837: loss 0.031412\n",
      "batch 4838: loss 0.114299\n",
      "batch 4839: loss 0.078663\n",
      "batch 4840: loss 0.063183\n",
      "batch 4841: loss 0.080718\n",
      "batch 4842: loss 0.052776\n",
      "batch 4843: loss 0.094372\n",
      "batch 4844: loss 0.112977\n",
      "batch 4845: loss 0.013406\n",
      "batch 4846: loss 0.025259\n",
      "batch 4847: loss 0.074615\n",
      "batch 4848: loss 0.066894\n",
      "batch 4849: loss 0.226526\n",
      "batch 4850: loss 0.038122\n",
      "batch 4851: loss 0.036749\n",
      "batch 4852: loss 0.033937\n",
      "batch 4853: loss 0.018734\n",
      "batch 4854: loss 0.134101\n",
      "batch 4855: loss 0.055557\n",
      "batch 4856: loss 0.162514\n",
      "batch 4857: loss 0.029399\n",
      "batch 4858: loss 0.018325\n",
      "batch 4859: loss 0.254489\n",
      "batch 4860: loss 0.171740\n",
      "batch 4861: loss 0.058767\n",
      "batch 4862: loss 0.003768\n",
      "batch 4863: loss 0.081695\n",
      "batch 4864: loss 0.037150\n",
      "batch 4865: loss 0.006835\n",
      "batch 4866: loss 0.161421\n",
      "batch 4867: loss 0.054920\n",
      "batch 4868: loss 0.013741\n",
      "batch 4869: loss 0.031122\n",
      "batch 4870: loss 0.067167\n",
      "batch 4871: loss 0.034863\n",
      "batch 4872: loss 0.035901\n",
      "batch 4873: loss 0.106671\n",
      "batch 4874: loss 0.133018\n",
      "batch 4875: loss 0.041970\n",
      "batch 4876: loss 0.005510\n",
      "batch 4877: loss 0.151571\n",
      "batch 4878: loss 0.041049\n",
      "batch 4879: loss 0.075230\n",
      "batch 4880: loss 0.141867\n",
      "batch 4881: loss 0.090525\n",
      "batch 4882: loss 0.019823\n",
      "batch 4883: loss 0.056530\n",
      "batch 4884: loss 0.042841\n",
      "batch 4885: loss 0.010165\n",
      "batch 4886: loss 0.042347\n",
      "batch 4887: loss 0.129526\n",
      "batch 4888: loss 0.043781\n",
      "batch 4889: loss 0.087840\n",
      "batch 4890: loss 0.007415\n",
      "batch 4891: loss 0.026485\n",
      "batch 4892: loss 0.171510\n",
      "batch 4893: loss 0.025432\n",
      "batch 4894: loss 0.021244\n",
      "batch 4895: loss 0.035154\n",
      "batch 4896: loss 0.006562\n",
      "batch 4897: loss 0.160483\n",
      "batch 4898: loss 0.007624\n",
      "batch 4899: loss 0.111457\n",
      "batch 4900: loss 0.106617\n",
      "batch 4901: loss 0.009589\n",
      "batch 4902: loss 0.071837\n",
      "batch 4903: loss 0.045458\n",
      "batch 4904: loss 0.055596\n",
      "batch 4905: loss 0.025388\n",
      "batch 4906: loss 0.111908\n",
      "batch 4907: loss 0.028245\n",
      "batch 4908: loss 0.104393\n",
      "batch 4909: loss 0.031317\n",
      "batch 4910: loss 0.007155\n",
      "batch 4911: loss 0.061037\n",
      "batch 4912: loss 0.041516\n",
      "batch 4913: loss 0.185815\n",
      "batch 4914: loss 0.080417\n",
      "batch 4915: loss 0.032966\n",
      "batch 4916: loss 0.051173\n",
      "batch 4917: loss 0.092815\n",
      "batch 4918: loss 0.027071\n",
      "batch 4919: loss 0.033244\n",
      "batch 4920: loss 0.076330\n",
      "batch 4921: loss 0.009468\n",
      "batch 4922: loss 0.021272\n",
      "batch 4923: loss 0.005602\n",
      "batch 4924: loss 0.074265\n",
      "batch 4925: loss 0.110992\n",
      "batch 4926: loss 0.051434\n",
      "batch 4927: loss 0.040650\n",
      "batch 4928: loss 0.012003\n",
      "batch 4929: loss 0.096504\n",
      "batch 4930: loss 0.039106\n",
      "batch 4931: loss 0.111797\n",
      "batch 4932: loss 0.056554\n",
      "batch 4933: loss 0.034870\n",
      "batch 4934: loss 0.075722\n",
      "batch 4935: loss 0.047362\n",
      "batch 4936: loss 0.134795\n",
      "batch 4937: loss 0.071526\n",
      "batch 4938: loss 0.075097\n",
      "batch 4939: loss 0.026592\n",
      "batch 4940: loss 0.007216\n",
      "batch 4941: loss 0.103659\n",
      "batch 4942: loss 0.057891\n",
      "batch 4943: loss 0.081969\n",
      "batch 4944: loss 0.018472\n",
      "batch 4945: loss 0.030660\n",
      "batch 4946: loss 0.008626\n",
      "batch 4947: loss 0.044941\n",
      "batch 4948: loss 0.015115\n",
      "batch 4949: loss 0.021159\n",
      "batch 4950: loss 0.017416\n",
      "batch 4951: loss 0.061078\n",
      "batch 4952: loss 0.041820\n",
      "batch 4953: loss 0.047530\n",
      "batch 4954: loss 0.092001\n",
      "batch 4955: loss 0.070218\n",
      "batch 4956: loss 0.021606\n",
      "batch 4957: loss 0.038575\n",
      "batch 4958: loss 0.038595\n",
      "batch 4959: loss 0.020811\n",
      "batch 4960: loss 0.098975\n",
      "batch 4961: loss 0.185050\n",
      "batch 4962: loss 0.028519\n",
      "batch 4963: loss 0.053580\n",
      "batch 4964: loss 0.146685\n",
      "batch 4965: loss 0.061998\n",
      "batch 4966: loss 0.006161\n",
      "batch 4967: loss 0.160680\n",
      "batch 4968: loss 0.133506\n",
      "batch 4969: loss 0.016852\n",
      "batch 4970: loss 0.021137\n",
      "batch 4971: loss 0.304283\n",
      "batch 4972: loss 0.010558\n",
      "batch 4973: loss 0.129911\n",
      "batch 4974: loss 0.046274\n",
      "batch 4975: loss 0.052580\n",
      "batch 4976: loss 0.054043\n",
      "batch 4977: loss 0.014354\n",
      "batch 4978: loss 0.086243\n",
      "batch 4979: loss 0.043596\n",
      "batch 4980: loss 0.079326\n",
      "batch 4981: loss 0.057847\n",
      "batch 4982: loss 0.060405\n",
      "batch 4983: loss 0.156167\n",
      "batch 4984: loss 0.066557\n",
      "batch 4985: loss 0.024030\n",
      "batch 4986: loss 0.045063\n",
      "batch 4987: loss 0.018580\n",
      "batch 4988: loss 0.145736\n",
      "batch 4989: loss 0.069224\n",
      "batch 4990: loss 0.054434\n",
      "batch 4991: loss 0.028021\n",
      "batch 4992: loss 0.052713\n",
      "batch 4993: loss 0.008020\n",
      "batch 4994: loss 0.007297\n",
      "batch 4995: loss 0.069259\n",
      "batch 4996: loss 0.027352\n",
      "batch 4997: loss 0.006052\n",
      "batch 4998: loss 0.087548\n",
      "batch 4999: loss 0.014230\n",
      "batch 5000: loss 0.016059\n",
      "batch 5001: loss 0.036226\n",
      "batch 5002: loss 0.067686\n",
      "batch 5003: loss 0.100351\n",
      "batch 5004: loss 0.029410\n",
      "batch 5005: loss 0.021636\n",
      "batch 5006: loss 0.136757\n",
      "batch 5007: loss 0.027122\n",
      "batch 5008: loss 0.093706\n",
      "batch 5009: loss 0.063404\n",
      "batch 5010: loss 0.054457\n",
      "batch 5011: loss 0.029993\n",
      "batch 5012: loss 0.077493\n",
      "batch 5013: loss 0.016193\n",
      "batch 5014: loss 0.013727\n",
      "batch 5015: loss 0.073260\n",
      "batch 5016: loss 0.048692\n",
      "batch 5017: loss 0.074087\n",
      "batch 5018: loss 0.012302\n",
      "batch 5019: loss 0.096196\n",
      "batch 5020: loss 0.015984\n",
      "batch 5021: loss 0.063531\n",
      "batch 5022: loss 0.031242\n",
      "batch 5023: loss 0.077543\n",
      "batch 5024: loss 0.024084\n",
      "batch 5025: loss 0.012992\n",
      "batch 5026: loss 0.053144\n",
      "batch 5027: loss 0.032924\n",
      "batch 5028: loss 0.038967\n",
      "batch 5029: loss 0.030275\n",
      "batch 5030: loss 0.092178\n",
      "batch 5031: loss 0.058518\n",
      "batch 5032: loss 0.109427\n",
      "batch 5033: loss 0.188849\n",
      "batch 5034: loss 0.168559\n",
      "batch 5035: loss 0.152094\n",
      "batch 5036: loss 0.007948\n",
      "batch 5037: loss 0.018500\n",
      "batch 5038: loss 0.049494\n",
      "batch 5039: loss 0.032196\n",
      "batch 5040: loss 0.058641\n",
      "batch 5041: loss 0.017928\n",
      "batch 5042: loss 0.048093\n",
      "batch 5043: loss 0.165169\n",
      "batch 5044: loss 0.143883\n",
      "batch 5045: loss 0.019184\n",
      "batch 5046: loss 0.334484\n",
      "batch 5047: loss 0.015893\n",
      "batch 5048: loss 0.009823\n",
      "batch 5049: loss 0.041564\n",
      "batch 5050: loss 0.015474\n",
      "batch 5051: loss 0.011930\n",
      "batch 5052: loss 0.019475\n",
      "batch 5053: loss 0.009583\n",
      "batch 5054: loss 0.026559\n",
      "batch 5055: loss 0.112657\n",
      "batch 5056: loss 0.056025\n",
      "batch 5057: loss 0.034099\n",
      "batch 5058: loss 0.046655\n",
      "batch 5059: loss 0.020505\n",
      "batch 5060: loss 0.063362\n",
      "batch 5061: loss 0.116030\n",
      "batch 5062: loss 0.024246\n",
      "batch 5063: loss 0.019389\n",
      "batch 5064: loss 0.022226\n",
      "batch 5065: loss 0.097779\n",
      "batch 5066: loss 0.030643\n",
      "batch 5067: loss 0.059449\n",
      "batch 5068: loss 0.048398\n",
      "batch 5069: loss 0.238440\n",
      "batch 5070: loss 0.028656\n",
      "batch 5071: loss 0.020109\n",
      "batch 5072: loss 0.014638\n",
      "batch 5073: loss 0.013798\n",
      "batch 5074: loss 0.082477\n",
      "batch 5075: loss 0.052007\n",
      "batch 5076: loss 0.086757\n",
      "batch 5077: loss 0.016620\n",
      "batch 5078: loss 0.061881\n",
      "batch 5079: loss 0.103760\n",
      "batch 5080: loss 0.050435\n",
      "batch 5081: loss 0.006892\n",
      "batch 5082: loss 0.043423\n",
      "batch 5083: loss 0.037229\n",
      "batch 5084: loss 0.010730\n",
      "batch 5085: loss 0.013359\n",
      "batch 5086: loss 0.105484\n",
      "batch 5087: loss 0.071122\n",
      "batch 5088: loss 0.063811\n",
      "batch 5089: loss 0.032336\n",
      "batch 5090: loss 0.136348\n",
      "batch 5091: loss 0.024592\n",
      "batch 5092: loss 0.180119\n",
      "batch 5093: loss 0.087314\n",
      "batch 5094: loss 0.090703\n",
      "batch 5095: loss 0.015786\n",
      "batch 5096: loss 0.019319\n",
      "batch 5097: loss 0.052206\n",
      "batch 5098: loss 0.041105\n",
      "batch 5099: loss 0.072777\n",
      "batch 5100: loss 0.071570\n",
      "batch 5101: loss 0.025380\n",
      "batch 5102: loss 0.017156\n",
      "batch 5103: loss 0.092197\n",
      "batch 5104: loss 0.055229\n",
      "batch 5105: loss 0.003795\n",
      "batch 5106: loss 0.017364\n",
      "batch 5107: loss 0.010646\n",
      "batch 5108: loss 0.040124\n",
      "batch 5109: loss 0.118326\n",
      "batch 5110: loss 0.015735\n",
      "batch 5111: loss 0.014200\n",
      "batch 5112: loss 0.032574\n",
      "batch 5113: loss 0.097248\n",
      "batch 5114: loss 0.088036\n",
      "batch 5115: loss 0.099598\n",
      "batch 5116: loss 0.099660\n",
      "batch 5117: loss 0.005542\n",
      "batch 5118: loss 0.054362\n",
      "batch 5119: loss 0.018324\n",
      "batch 5120: loss 0.041863\n",
      "batch 5121: loss 0.013726\n",
      "batch 5122: loss 0.142946\n",
      "batch 5123: loss 0.026924\n",
      "batch 5124: loss 0.004675\n",
      "batch 5125: loss 0.071120\n",
      "batch 5126: loss 0.162701\n",
      "batch 5127: loss 0.028387\n",
      "batch 5128: loss 0.149689\n",
      "batch 5129: loss 0.011464\n",
      "batch 5130: loss 0.024037\n",
      "batch 5131: loss 0.037684\n",
      "batch 5132: loss 0.008932\n",
      "batch 5133: loss 0.102899\n",
      "batch 5134: loss 0.074263\n",
      "batch 5135: loss 0.036116\n",
      "batch 5136: loss 0.016435\n",
      "batch 5137: loss 0.007045\n",
      "batch 5138: loss 0.025805\n",
      "batch 5139: loss 0.035191\n",
      "batch 5140: loss 0.099327\n",
      "batch 5141: loss 0.271588\n",
      "batch 5142: loss 0.034649\n",
      "batch 5143: loss 0.052070\n",
      "batch 5144: loss 0.036227\n",
      "batch 5145: loss 0.020235\n",
      "batch 5146: loss 0.104237\n",
      "batch 5147: loss 0.076398\n",
      "batch 5148: loss 0.177182\n",
      "batch 5149: loss 0.060717\n",
      "batch 5150: loss 0.111843\n",
      "batch 5151: loss 0.108528\n",
      "batch 5152: loss 0.068777\n",
      "batch 5153: loss 0.059622\n",
      "batch 5154: loss 0.085340\n",
      "batch 5155: loss 0.043584\n",
      "batch 5156: loss 0.113420\n",
      "batch 5157: loss 0.021315\n",
      "batch 5158: loss 0.018854\n",
      "batch 5159: loss 0.044259\n",
      "batch 5160: loss 0.137918\n",
      "batch 5161: loss 0.005808\n",
      "batch 5162: loss 0.035132\n",
      "batch 5163: loss 0.025734\n",
      "batch 5164: loss 0.028760\n",
      "batch 5165: loss 0.178771\n",
      "batch 5166: loss 0.124255\n",
      "batch 5167: loss 0.049000\n",
      "batch 5168: loss 0.021575\n",
      "batch 5169: loss 0.034562\n",
      "batch 5170: loss 0.010421\n",
      "batch 5171: loss 0.078477\n",
      "batch 5172: loss 0.011916\n",
      "batch 5173: loss 0.170573\n",
      "batch 5174: loss 0.055677\n",
      "batch 5175: loss 0.067763\n",
      "batch 5176: loss 0.026750\n",
      "batch 5177: loss 0.013398\n",
      "batch 5178: loss 0.136029\n",
      "batch 5179: loss 0.086831\n",
      "batch 5180: loss 0.015339\n",
      "batch 5181: loss 0.024415\n",
      "batch 5182: loss 0.025289\n",
      "batch 5183: loss 0.051518\n",
      "batch 5184: loss 0.040161\n",
      "batch 5185: loss 0.006640\n",
      "batch 5186: loss 0.052402\n",
      "batch 5187: loss 0.060799\n",
      "batch 5188: loss 0.089187\n",
      "batch 5189: loss 0.090557\n",
      "batch 5190: loss 0.091896\n",
      "batch 5191: loss 0.053248\n",
      "batch 5192: loss 0.024120\n",
      "batch 5193: loss 0.108272\n",
      "batch 5194: loss 0.011942\n",
      "batch 5195: loss 0.057250\n",
      "batch 5196: loss 0.056375\n",
      "batch 5197: loss 0.015144\n",
      "batch 5198: loss 0.119859\n",
      "batch 5199: loss 0.033564\n",
      "batch 5200: loss 0.150602\n",
      "batch 5201: loss 0.034110\n",
      "batch 5202: loss 0.012601\n",
      "batch 5203: loss 0.079395\n",
      "batch 5204: loss 0.130872\n",
      "batch 5205: loss 0.016953\n",
      "batch 5206: loss 0.025476\n",
      "batch 5207: loss 0.095409\n",
      "batch 5208: loss 0.032002\n",
      "batch 5209: loss 0.031674\n",
      "batch 5210: loss 0.008849\n",
      "batch 5211: loss 0.077452\n",
      "batch 5212: loss 0.140399\n",
      "batch 5213: loss 0.060920\n",
      "batch 5214: loss 0.020221\n",
      "batch 5215: loss 0.175076\n",
      "batch 5216: loss 0.026247\n",
      "batch 5217: loss 0.104769\n",
      "batch 5218: loss 0.020094\n",
      "batch 5219: loss 0.035987\n",
      "batch 5220: loss 0.133230\n",
      "batch 5221: loss 0.030470\n",
      "batch 5222: loss 0.033611\n",
      "batch 5223: loss 0.059366\n",
      "batch 5224: loss 0.218605\n",
      "batch 5225: loss 0.032606\n",
      "batch 5226: loss 0.026033\n",
      "batch 5227: loss 0.036660\n",
      "batch 5228: loss 0.049787\n",
      "batch 5229: loss 0.217555\n",
      "batch 5230: loss 0.047119\n",
      "batch 5231: loss 0.009168\n",
      "batch 5232: loss 0.011968\n",
      "batch 5233: loss 0.014598\n",
      "batch 5234: loss 0.019870\n",
      "batch 5235: loss 0.036991\n",
      "batch 5236: loss 0.073009\n",
      "batch 5237: loss 0.021155\n",
      "batch 5238: loss 0.105297\n",
      "batch 5239: loss 0.050161\n",
      "batch 5240: loss 0.058052\n",
      "batch 5241: loss 0.047773\n",
      "batch 5242: loss 0.053172\n",
      "batch 5243: loss 0.074552\n",
      "batch 5244: loss 0.054066\n",
      "batch 5245: loss 0.029213\n",
      "batch 5246: loss 0.083141\n",
      "batch 5247: loss 0.026872\n",
      "batch 5248: loss 0.050588\n",
      "batch 5249: loss 0.099933\n",
      "batch 5250: loss 0.014014\n",
      "batch 5251: loss 0.026511\n",
      "batch 5252: loss 0.074194\n",
      "batch 5253: loss 0.035303\n",
      "batch 5254: loss 0.022115\n",
      "batch 5255: loss 0.092372\n",
      "batch 5256: loss 0.102542\n",
      "batch 5257: loss 0.054596\n",
      "batch 5258: loss 0.154329\n",
      "batch 5259: loss 0.093871\n",
      "batch 5260: loss 0.074524\n",
      "batch 5261: loss 0.024207\n",
      "batch 5262: loss 0.050080\n",
      "batch 5263: loss 0.048788\n",
      "batch 5264: loss 0.023848\n",
      "batch 5265: loss 0.155567\n",
      "batch 5266: loss 0.031315\n",
      "batch 5267: loss 0.029012\n",
      "batch 5268: loss 0.090845\n",
      "batch 5269: loss 0.025663\n",
      "batch 5270: loss 0.010687\n",
      "batch 5271: loss 0.033162\n",
      "batch 5272: loss 0.002884\n",
      "batch 5273: loss 0.039547\n",
      "batch 5274: loss 0.051065\n",
      "batch 5275: loss 0.017954\n",
      "batch 5276: loss 0.023341\n",
      "batch 5277: loss 0.016238\n",
      "batch 5278: loss 0.031619\n",
      "batch 5279: loss 0.022314\n",
      "batch 5280: loss 0.025463\n",
      "batch 5281: loss 0.035551\n",
      "batch 5282: loss 0.030825\n",
      "batch 5283: loss 0.053726\n",
      "batch 5284: loss 0.113304\n",
      "batch 5285: loss 0.031052\n",
      "batch 5286: loss 0.035194\n",
      "batch 5287: loss 0.105464\n",
      "batch 5288: loss 0.031584\n",
      "batch 5289: loss 0.093987\n",
      "batch 5290: loss 0.008331\n",
      "batch 5291: loss 0.034176\n",
      "batch 5292: loss 0.059393\n",
      "batch 5293: loss 0.024706\n",
      "batch 5294: loss 0.107088\n",
      "batch 5295: loss 0.123178\n",
      "batch 5296: loss 0.048868\n",
      "batch 5297: loss 0.025392\n",
      "batch 5298: loss 0.035456\n",
      "batch 5299: loss 0.038224\n",
      "batch 5300: loss 0.053800\n",
      "batch 5301: loss 0.007898\n",
      "batch 5302: loss 0.090380\n",
      "batch 5303: loss 0.004874\n",
      "batch 5304: loss 0.026172\n",
      "batch 5305: loss 0.058695\n",
      "batch 5306: loss 0.029362\n",
      "batch 5307: loss 0.041965\n",
      "batch 5308: loss 0.022267\n",
      "batch 5309: loss 0.030214\n",
      "batch 5310: loss 0.016131\n",
      "batch 5311: loss 0.032502\n",
      "batch 5312: loss 0.023165\n",
      "batch 5313: loss 0.123913\n",
      "batch 5314: loss 0.025231\n",
      "batch 5315: loss 0.045314\n",
      "batch 5316: loss 0.116400\n",
      "batch 5317: loss 0.087681\n",
      "batch 5318: loss 0.063347\n",
      "batch 5319: loss 0.097031\n",
      "batch 5320: loss 0.009558\n",
      "batch 5321: loss 0.033707\n",
      "batch 5322: loss 0.029160\n",
      "batch 5323: loss 0.032004\n",
      "batch 5324: loss 0.016087\n",
      "batch 5325: loss 0.042635\n",
      "batch 5326: loss 0.032522\n",
      "batch 5327: loss 0.049202\n",
      "batch 5328: loss 0.165861\n",
      "batch 5329: loss 0.068152\n",
      "batch 5330: loss 0.024573\n",
      "batch 5331: loss 0.127393\n",
      "batch 5332: loss 0.025079\n",
      "batch 5333: loss 0.174374\n",
      "batch 5334: loss 0.033433\n",
      "batch 5335: loss 0.023750\n",
      "batch 5336: loss 0.021185\n",
      "batch 5337: loss 0.106000\n",
      "batch 5338: loss 0.031598\n",
      "batch 5339: loss 0.166128\n",
      "batch 5340: loss 0.113119\n",
      "batch 5341: loss 0.129467\n",
      "batch 5342: loss 0.075374\n",
      "batch 5343: loss 0.093642\n",
      "batch 5344: loss 0.032063\n",
      "batch 5345: loss 0.043799\n",
      "batch 5346: loss 0.043226\n",
      "batch 5347: loss 0.041538\n",
      "batch 5348: loss 0.029988\n",
      "batch 5349: loss 0.064877\n",
      "batch 5350: loss 0.030243\n",
      "batch 5351: loss 0.071027\n",
      "batch 5352: loss 0.011660\n",
      "batch 5353: loss 0.016927\n",
      "batch 5354: loss 0.177927\n",
      "batch 5355: loss 0.002625\n",
      "batch 5356: loss 0.047337\n",
      "batch 5357: loss 0.011998\n",
      "batch 5358: loss 0.039318\n",
      "batch 5359: loss 0.074167\n",
      "batch 5360: loss 0.029245\n",
      "batch 5361: loss 0.069218\n",
      "batch 5362: loss 0.043246\n",
      "batch 5363: loss 0.004158\n",
      "batch 5364: loss 0.089223\n",
      "batch 5365: loss 0.011204\n",
      "batch 5366: loss 0.009778\n",
      "batch 5367: loss 0.058034\n",
      "batch 5368: loss 0.039313\n",
      "batch 5369: loss 0.041034\n",
      "batch 5370: loss 0.041056\n",
      "batch 5371: loss 0.045211\n",
      "batch 5372: loss 0.018483\n",
      "batch 5373: loss 0.041729\n",
      "batch 5374: loss 0.020540\n",
      "batch 5375: loss 0.117316\n",
      "batch 5376: loss 0.042270\n",
      "batch 5377: loss 0.010729\n",
      "batch 5378: loss 0.037722\n",
      "batch 5379: loss 0.114994\n",
      "batch 5380: loss 0.043610\n",
      "batch 5381: loss 0.242136\n",
      "batch 5382: loss 0.036820\n",
      "batch 5383: loss 0.039990\n",
      "batch 5384: loss 0.052851\n",
      "batch 5385: loss 0.033259\n",
      "batch 5386: loss 0.187425\n",
      "batch 5387: loss 0.093462\n",
      "batch 5388: loss 0.034478\n",
      "batch 5389: loss 0.064280\n",
      "batch 5390: loss 0.102078\n",
      "batch 5391: loss 0.022004\n",
      "batch 5392: loss 0.035969\n",
      "batch 5393: loss 0.080129\n",
      "batch 5394: loss 0.049327\n",
      "batch 5395: loss 0.151073\n",
      "batch 5396: loss 0.048314\n",
      "batch 5397: loss 0.012018\n",
      "batch 5398: loss 0.108582\n",
      "batch 5399: loss 0.073521\n",
      "batch 5400: loss 0.169100\n",
      "batch 5401: loss 0.098428\n",
      "batch 5402: loss 0.036269\n",
      "batch 5403: loss 0.032897\n",
      "batch 5404: loss 0.026810\n",
      "batch 5405: loss 0.047747\n",
      "batch 5406: loss 0.033024\n",
      "batch 5407: loss 0.025914\n",
      "batch 5408: loss 0.116748\n",
      "batch 5409: loss 0.044731\n",
      "batch 5410: loss 0.093550\n",
      "batch 5411: loss 0.067281\n",
      "batch 5412: loss 0.067935\n",
      "batch 5413: loss 0.066665\n",
      "batch 5414: loss 0.018284\n",
      "batch 5415: loss 0.046904\n",
      "batch 5416: loss 0.014272\n",
      "batch 5417: loss 0.036704\n",
      "batch 5418: loss 0.010500\n",
      "batch 5419: loss 0.064734\n",
      "batch 5420: loss 0.029561\n",
      "batch 5421: loss 0.193215\n",
      "batch 5422: loss 0.061294\n",
      "batch 5423: loss 0.015348\n",
      "batch 5424: loss 0.103061\n",
      "batch 5425: loss 0.124015\n",
      "batch 5426: loss 0.093436\n",
      "batch 5427: loss 0.155723\n",
      "batch 5428: loss 0.053363\n",
      "batch 5429: loss 0.046525\n",
      "batch 5430: loss 0.125854\n",
      "batch 5431: loss 0.098191\n",
      "batch 5432: loss 0.010548\n",
      "batch 5433: loss 0.015872\n",
      "batch 5434: loss 0.033249\n",
      "batch 5435: loss 0.030165\n",
      "batch 5436: loss 0.023064\n",
      "batch 5437: loss 0.009845\n",
      "batch 5438: loss 0.089188\n",
      "batch 5439: loss 0.012515\n",
      "batch 5440: loss 0.027033\n",
      "batch 5441: loss 0.008066\n",
      "batch 5442: loss 0.038764\n",
      "batch 5443: loss 0.048565\n",
      "batch 5444: loss 0.038403\n",
      "batch 5445: loss 0.078331\n",
      "batch 5446: loss 0.009373\n",
      "batch 5447: loss 0.123699\n",
      "batch 5448: loss 0.072091\n",
      "batch 5449: loss 0.055009\n",
      "batch 5450: loss 0.050637\n",
      "batch 5451: loss 0.075482\n",
      "batch 5452: loss 0.009177\n",
      "batch 5453: loss 0.044271\n",
      "batch 5454: loss 0.006572\n",
      "batch 5455: loss 0.014996\n",
      "batch 5456: loss 0.059047\n",
      "batch 5457: loss 0.018784\n",
      "batch 5458: loss 0.067376\n",
      "batch 5459: loss 0.097655\n",
      "batch 5460: loss 0.031410\n",
      "batch 5461: loss 0.047263\n",
      "batch 5462: loss 0.051147\n",
      "batch 5463: loss 0.023826\n",
      "batch 5464: loss 0.050473\n",
      "batch 5465: loss 0.028833\n",
      "batch 5466: loss 0.021092\n",
      "batch 5467: loss 0.114749\n",
      "batch 5468: loss 0.010318\n",
      "batch 5469: loss 0.081974\n",
      "batch 5470: loss 0.036324\n",
      "batch 5471: loss 0.131918\n",
      "batch 5472: loss 0.058599\n",
      "batch 5473: loss 0.144423\n",
      "batch 5474: loss 0.037915\n",
      "batch 5475: loss 0.011696\n",
      "batch 5476: loss 0.033378\n",
      "batch 5477: loss 0.163927\n",
      "batch 5478: loss 0.032211\n",
      "batch 5479: loss 0.005504\n",
      "batch 5480: loss 0.120032\n",
      "batch 5481: loss 0.040529\n",
      "batch 5482: loss 0.092934\n",
      "batch 5483: loss 0.073190\n",
      "batch 5484: loss 0.105618\n",
      "batch 5485: loss 0.057929\n",
      "batch 5486: loss 0.101675\n",
      "batch 5487: loss 0.030469\n",
      "batch 5488: loss 0.085992\n",
      "batch 5489: loss 0.017961\n",
      "batch 5490: loss 0.055727\n",
      "batch 5491: loss 0.053551\n",
      "batch 5492: loss 0.156251\n",
      "batch 5493: loss 0.037092\n",
      "batch 5494: loss 0.040392\n",
      "batch 5495: loss 0.007640\n",
      "batch 5496: loss 0.015786\n",
      "batch 5497: loss 0.012856\n",
      "batch 5498: loss 0.040194\n",
      "batch 5499: loss 0.024244\n",
      "batch 5500: loss 0.054486\n",
      "batch 5501: loss 0.010003\n",
      "batch 5502: loss 0.046364\n",
      "batch 5503: loss 0.046272\n",
      "batch 5504: loss 0.103415\n",
      "batch 5505: loss 0.043335\n",
      "batch 5506: loss 0.229996\n",
      "batch 5507: loss 0.080404\n",
      "batch 5508: loss 0.173691\n",
      "batch 5509: loss 0.025193\n",
      "batch 5510: loss 0.028263\n",
      "batch 5511: loss 0.019639\n",
      "batch 5512: loss 0.017192\n",
      "batch 5513: loss 0.057077\n",
      "batch 5514: loss 0.054022\n",
      "batch 5515: loss 0.086940\n",
      "batch 5516: loss 0.012355\n",
      "batch 5517: loss 0.013173\n",
      "batch 5518: loss 0.022203\n",
      "batch 5519: loss 0.026604\n",
      "batch 5520: loss 0.075644\n",
      "batch 5521: loss 0.031714\n",
      "batch 5522: loss 0.038138\n",
      "batch 5523: loss 0.098981\n",
      "batch 5524: loss 0.041261\n",
      "batch 5525: loss 0.009915\n",
      "batch 5526: loss 0.038012\n",
      "batch 5527: loss 0.024016\n",
      "batch 5528: loss 0.026234\n",
      "batch 5529: loss 0.065559\n",
      "batch 5530: loss 0.221682\n",
      "batch 5531: loss 0.044935\n",
      "batch 5532: loss 0.093286\n",
      "batch 5533: loss 0.051989\n",
      "batch 5534: loss 0.030034\n",
      "batch 5535: loss 0.047449\n",
      "batch 5536: loss 0.029002\n",
      "batch 5537: loss 0.042506\n",
      "batch 5538: loss 0.060437\n",
      "batch 5539: loss 0.056075\n",
      "batch 5540: loss 0.053965\n",
      "batch 5541: loss 0.073183\n",
      "batch 5542: loss 0.012857\n",
      "batch 5543: loss 0.060020\n",
      "batch 5544: loss 0.054441\n",
      "batch 5545: loss 0.025435\n",
      "batch 5546: loss 0.032074\n",
      "batch 5547: loss 0.022995\n",
      "batch 5548: loss 0.075964\n",
      "batch 5549: loss 0.141833\n",
      "batch 5550: loss 0.292640\n",
      "batch 5551: loss 0.015949\n",
      "batch 5552: loss 0.020894\n",
      "batch 5553: loss 0.045880\n",
      "batch 5554: loss 0.076991\n",
      "batch 5555: loss 0.102472\n",
      "batch 5556: loss 0.084240\n",
      "batch 5557: loss 0.026303\n",
      "batch 5558: loss 0.028067\n",
      "batch 5559: loss 0.011380\n",
      "batch 5560: loss 0.119724\n",
      "batch 5561: loss 0.040143\n",
      "batch 5562: loss 0.069776\n",
      "batch 5563: loss 0.047392\n",
      "batch 5564: loss 0.104703\n",
      "batch 5565: loss 0.031289\n",
      "batch 5566: loss 0.002087\n",
      "batch 5567: loss 0.053506\n",
      "batch 5568: loss 0.040611\n",
      "batch 5569: loss 0.199404\n",
      "batch 5570: loss 0.191580\n",
      "batch 5571: loss 0.030192\n",
      "batch 5572: loss 0.013929\n",
      "batch 5573: loss 0.025965\n",
      "batch 5574: loss 0.028059\n",
      "batch 5575: loss 0.129197\n",
      "batch 5576: loss 0.036147\n",
      "batch 5577: loss 0.047322\n",
      "batch 5578: loss 0.093860\n",
      "batch 5579: loss 0.050610\n",
      "batch 5580: loss 0.027790\n",
      "batch 5581: loss 0.103575\n",
      "batch 5582: loss 0.016586\n",
      "batch 5583: loss 0.005986\n",
      "batch 5584: loss 0.039801\n",
      "batch 5585: loss 0.057484\n",
      "batch 5586: loss 0.029206\n",
      "batch 5587: loss 0.081485\n",
      "batch 5588: loss 0.024898\n",
      "batch 5589: loss 0.024372\n",
      "batch 5590: loss 0.102248\n",
      "batch 5591: loss 0.007508\n",
      "batch 5592: loss 0.141319\n",
      "batch 5593: loss 0.016759\n",
      "batch 5594: loss 0.010794\n",
      "batch 5595: loss 0.037805\n",
      "batch 5596: loss 0.118351\n",
      "batch 5597: loss 0.109024\n",
      "batch 5598: loss 0.004831\n",
      "batch 5599: loss 0.162343\n",
      "batch 5600: loss 0.038396\n",
      "batch 5601: loss 0.088499\n",
      "batch 5602: loss 0.051630\n",
      "batch 5603: loss 0.007906\n",
      "batch 5604: loss 0.010580\n",
      "batch 5605: loss 0.041913\n",
      "batch 5606: loss 0.121135\n",
      "batch 5607: loss 0.031884\n",
      "batch 5608: loss 0.045432\n",
      "batch 5609: loss 0.093160\n",
      "batch 5610: loss 0.014658\n",
      "batch 5611: loss 0.026280\n",
      "batch 5612: loss 0.033499\n",
      "batch 5613: loss 0.020703\n",
      "batch 5614: loss 0.012308\n",
      "batch 5615: loss 0.104624\n",
      "batch 5616: loss 0.010801\n",
      "batch 5617: loss 0.045293\n",
      "batch 5618: loss 0.052181\n",
      "batch 5619: loss 0.011802\n",
      "batch 5620: loss 0.049304\n",
      "batch 5621: loss 0.015916\n",
      "batch 5622: loss 0.123754\n",
      "batch 5623: loss 0.099323\n",
      "batch 5624: loss 0.066834\n",
      "batch 5625: loss 0.078996\n",
      "batch 5626: loss 0.014278\n",
      "batch 5627: loss 0.048001\n",
      "batch 5628: loss 0.058595\n",
      "batch 5629: loss 0.027981\n",
      "batch 5630: loss 0.080005\n",
      "batch 5631: loss 0.038529\n",
      "batch 5632: loss 0.035877\n",
      "batch 5633: loss 0.031795\n",
      "batch 5634: loss 0.107763\n",
      "batch 5635: loss 0.088936\n",
      "batch 5636: loss 0.115082\n",
      "batch 5637: loss 0.089811\n",
      "batch 5638: loss 0.113988\n",
      "batch 5639: loss 0.032842\n",
      "batch 5640: loss 0.029497\n",
      "batch 5641: loss 0.067358\n",
      "batch 5642: loss 0.072442\n",
      "batch 5643: loss 0.045766\n",
      "batch 5644: loss 0.025991\n",
      "batch 5645: loss 0.021331\n",
      "batch 5646: loss 0.051608\n",
      "batch 5647: loss 0.029313\n",
      "batch 5648: loss 0.029477\n",
      "batch 5649: loss 0.023068\n",
      "batch 5650: loss 0.123850\n",
      "batch 5651: loss 0.053380\n",
      "batch 5652: loss 0.125462\n",
      "batch 5653: loss 0.189815\n",
      "batch 5654: loss 0.045998\n",
      "batch 5655: loss 0.127946\n",
      "batch 5656: loss 0.078832\n",
      "batch 5657: loss 0.016884\n",
      "batch 5658: loss 0.094160\n",
      "batch 5659: loss 0.069197\n",
      "batch 5660: loss 0.041933\n",
      "batch 5661: loss 0.016771\n",
      "batch 5662: loss 0.104479\n",
      "batch 5663: loss 0.040822\n",
      "batch 5664: loss 0.036885\n",
      "batch 5665: loss 0.032292\n",
      "batch 5666: loss 0.046871\n",
      "batch 5667: loss 0.049710\n",
      "batch 5668: loss 0.006210\n",
      "batch 5669: loss 0.023397\n",
      "batch 5670: loss 0.040188\n",
      "batch 5671: loss 0.043805\n",
      "batch 5672: loss 0.018190\n",
      "batch 5673: loss 0.180306\n",
      "batch 5674: loss 0.046539\n",
      "batch 5675: loss 0.039394\n",
      "batch 5676: loss 0.019963\n",
      "batch 5677: loss 0.008685\n",
      "batch 5678: loss 0.056032\n",
      "batch 5679: loss 0.078656\n",
      "batch 5680: loss 0.133860\n",
      "batch 5681: loss 0.063722\n",
      "batch 5682: loss 0.125872\n",
      "batch 5683: loss 0.044700\n",
      "batch 5684: loss 0.096098\n",
      "batch 5685: loss 0.022047\n",
      "batch 5686: loss 0.090938\n",
      "batch 5687: loss 0.040059\n",
      "batch 5688: loss 0.068078\n",
      "batch 5689: loss 0.046548\n",
      "batch 5690: loss 0.007174\n",
      "batch 5691: loss 0.027219\n",
      "batch 5692: loss 0.008931\n",
      "batch 5693: loss 0.016052\n",
      "batch 5694: loss 0.034498\n",
      "batch 5695: loss 0.007421\n",
      "batch 5696: loss 0.046508\n",
      "batch 5697: loss 0.113219\n",
      "batch 5698: loss 0.047957\n",
      "batch 5699: loss 0.023139\n",
      "batch 5700: loss 0.020592\n",
      "batch 5701: loss 0.022389\n",
      "batch 5702: loss 0.006725\n",
      "batch 5703: loss 0.036058\n",
      "batch 5704: loss 0.051030\n",
      "batch 5705: loss 0.067582\n",
      "batch 5706: loss 0.034085\n",
      "batch 5707: loss 0.153560\n",
      "batch 5708: loss 0.056219\n",
      "batch 5709: loss 0.074701\n",
      "batch 5710: loss 0.019818\n",
      "batch 5711: loss 0.018334\n",
      "batch 5712: loss 0.039318\n",
      "batch 5713: loss 0.027740\n",
      "batch 5714: loss 0.130852\n",
      "batch 5715: loss 0.013960\n",
      "batch 5716: loss 0.020768\n",
      "batch 5717: loss 0.060108\n",
      "batch 5718: loss 0.011544\n",
      "batch 5719: loss 0.066927\n",
      "batch 5720: loss 0.123105\n",
      "batch 5721: loss 0.015445\n",
      "batch 5722: loss 0.035820\n",
      "batch 5723: loss 0.012137\n",
      "batch 5724: loss 0.045142\n",
      "batch 5725: loss 0.047655\n",
      "batch 5726: loss 0.016953\n",
      "batch 5727: loss 0.007686\n",
      "batch 5728: loss 0.028717\n",
      "batch 5729: loss 0.026462\n",
      "batch 5730: loss 0.037372\n",
      "batch 5731: loss 0.019586\n",
      "batch 5732: loss 0.093187\n",
      "batch 5733: loss 0.058903\n",
      "batch 5734: loss 0.012678\n",
      "batch 5735: loss 0.101810\n",
      "batch 5736: loss 0.057495\n",
      "batch 5737: loss 0.038147\n",
      "batch 5738: loss 0.029849\n",
      "batch 5739: loss 0.012363\n",
      "batch 5740: loss 0.116207\n",
      "batch 5741: loss 0.008717\n",
      "batch 5742: loss 0.009788\n",
      "batch 5743: loss 0.071559\n",
      "batch 5744: loss 0.037698\n",
      "batch 5745: loss 0.019390\n",
      "batch 5746: loss 0.013953\n",
      "batch 5747: loss 0.003766\n",
      "batch 5748: loss 0.004949\n",
      "batch 5749: loss 0.020987\n",
      "batch 5750: loss 0.051327\n",
      "batch 5751: loss 0.048306\n",
      "batch 5752: loss 0.014069\n",
      "batch 5753: loss 0.069790\n",
      "batch 5754: loss 0.052669\n",
      "batch 5755: loss 0.045848\n",
      "batch 5756: loss 0.106804\n",
      "batch 5757: loss 0.016656\n",
      "batch 5758: loss 0.085813\n",
      "batch 5759: loss 0.008569\n",
      "batch 5760: loss 0.022068\n",
      "batch 5761: loss 0.056014\n",
      "batch 5762: loss 0.072346\n",
      "batch 5763: loss 0.030253\n",
      "batch 5764: loss 0.111709\n",
      "batch 5765: loss 0.014261\n",
      "batch 5766: loss 0.016564\n",
      "batch 5767: loss 0.014095\n",
      "batch 5768: loss 0.117590\n",
      "batch 5769: loss 0.043062\n",
      "batch 5770: loss 0.037051\n",
      "batch 5771: loss 0.017629\n",
      "batch 5772: loss 0.039633\n",
      "batch 5773: loss 0.025869\n",
      "batch 5774: loss 0.012277\n",
      "batch 5775: loss 0.037738\n",
      "batch 5776: loss 0.043234\n",
      "batch 5777: loss 0.088470\n",
      "batch 5778: loss 0.054903\n",
      "batch 5779: loss 0.018302\n",
      "batch 5780: loss 0.042069\n",
      "batch 5781: loss 0.028542\n",
      "batch 5782: loss 0.015514\n",
      "batch 5783: loss 0.057065\n",
      "batch 5784: loss 0.011109\n",
      "batch 5785: loss 0.094954\n",
      "batch 5786: loss 0.022189\n",
      "batch 5787: loss 0.014905\n",
      "batch 5788: loss 0.006192\n",
      "batch 5789: loss 0.002451\n",
      "batch 5790: loss 0.117921\n",
      "batch 5791: loss 0.062752\n",
      "batch 5792: loss 0.029710\n",
      "batch 5793: loss 0.182289\n",
      "batch 5794: loss 0.033857\n",
      "batch 5795: loss 0.045495\n",
      "batch 5796: loss 0.041232\n",
      "batch 5797: loss 0.107141\n",
      "batch 5798: loss 0.098592\n",
      "batch 5799: loss 0.018845\n",
      "batch 5800: loss 0.140240\n",
      "batch 5801: loss 0.034155\n",
      "batch 5802: loss 0.013184\n",
      "batch 5803: loss 0.005719\n",
      "batch 5804: loss 0.053746\n",
      "batch 5805: loss 0.076214\n",
      "batch 5806: loss 0.022051\n",
      "batch 5807: loss 0.103541\n",
      "batch 5808: loss 0.205353\n",
      "batch 5809: loss 0.059440\n",
      "batch 5810: loss 0.024152\n",
      "batch 5811: loss 0.009121\n",
      "batch 5812: loss 0.023586\n",
      "batch 5813: loss 0.040948\n",
      "batch 5814: loss 0.036645\n",
      "batch 5815: loss 0.062427\n",
      "batch 5816: loss 0.053615\n",
      "batch 5817: loss 0.023441\n",
      "batch 5818: loss 0.065117\n",
      "batch 5819: loss 0.020976\n",
      "batch 5820: loss 0.031363\n",
      "batch 5821: loss 0.011182\n",
      "batch 5822: loss 0.011836\n",
      "batch 5823: loss 0.021710\n",
      "batch 5824: loss 0.053769\n",
      "batch 5825: loss 0.077297\n",
      "batch 5826: loss 0.109778\n",
      "batch 5827: loss 0.048670\n",
      "batch 5828: loss 0.018196\n",
      "batch 5829: loss 0.014276\n",
      "batch 5830: loss 0.066189\n",
      "batch 5831: loss 0.032401\n",
      "batch 5832: loss 0.062558\n",
      "batch 5833: loss 0.009573\n",
      "batch 5834: loss 0.005215\n",
      "batch 5835: loss 0.106299\n",
      "batch 5836: loss 0.054766\n",
      "batch 5837: loss 0.037515\n",
      "batch 5838: loss 0.011370\n",
      "batch 5839: loss 0.022113\n",
      "batch 5840: loss 0.033640\n",
      "batch 5841: loss 0.021029\n",
      "batch 5842: loss 0.084092\n",
      "batch 5843: loss 0.050716\n",
      "batch 5844: loss 0.040099\n",
      "batch 5845: loss 0.110757\n",
      "batch 5846: loss 0.027593\n",
      "batch 5847: loss 0.042432\n",
      "batch 5848: loss 0.174964\n",
      "batch 5849: loss 0.062797\n",
      "batch 5850: loss 0.017402\n",
      "batch 5851: loss 0.023391\n",
      "batch 5852: loss 0.083755\n",
      "batch 5853: loss 0.102265\n",
      "batch 5854: loss 0.097861\n",
      "batch 5855: loss 0.106997\n",
      "batch 5856: loss 0.038254\n",
      "batch 5857: loss 0.090285\n",
      "batch 5858: loss 0.010183\n",
      "batch 5859: loss 0.052074\n",
      "batch 5860: loss 0.038508\n",
      "batch 5861: loss 0.049606\n",
      "batch 5862: loss 0.071391\n",
      "batch 5863: loss 0.144744\n",
      "batch 5864: loss 0.012593\n",
      "batch 5865: loss 0.029870\n",
      "batch 5866: loss 0.154395\n",
      "batch 5867: loss 0.186732\n",
      "batch 5868: loss 0.088170\n",
      "batch 5869: loss 0.022098\n",
      "batch 5870: loss 0.006603\n",
      "batch 5871: loss 0.021085\n",
      "batch 5872: loss 0.076368\n",
      "batch 5873: loss 0.058632\n",
      "batch 5874: loss 0.040499\n",
      "batch 5875: loss 0.034480\n",
      "batch 5876: loss 0.028254\n",
      "batch 5877: loss 0.022334\n",
      "batch 5878: loss 0.047726\n",
      "batch 5879: loss 0.049029\n",
      "batch 5880: loss 0.056585\n",
      "batch 5881: loss 0.053755\n",
      "batch 5882: loss 0.031961\n",
      "batch 5883: loss 0.024119\n",
      "batch 5884: loss 0.007723\n",
      "batch 5885: loss 0.009110\n",
      "batch 5886: loss 0.090063\n",
      "batch 5887: loss 0.146426\n",
      "batch 5888: loss 0.021715\n",
      "batch 5889: loss 0.051789\n",
      "batch 5890: loss 0.080180\n",
      "batch 5891: loss 0.060882\n",
      "batch 5892: loss 0.011107\n",
      "batch 5893: loss 0.064260\n",
      "batch 5894: loss 0.144592\n",
      "batch 5895: loss 0.090444\n",
      "batch 5896: loss 0.010483\n",
      "batch 5897: loss 0.053585\n",
      "batch 5898: loss 0.077591\n",
      "batch 5899: loss 0.036975\n",
      "batch 5900: loss 0.039896\n",
      "batch 5901: loss 0.083972\n",
      "batch 5902: loss 0.047534\n",
      "batch 5903: loss 0.034992\n",
      "batch 5904: loss 0.110425\n",
      "batch 5905: loss 0.009338\n",
      "batch 5906: loss 0.081741\n",
      "batch 5907: loss 0.037959\n",
      "batch 5908: loss 0.013685\n",
      "batch 5909: loss 0.026154\n",
      "batch 5910: loss 0.170637\n",
      "batch 5911: loss 0.017057\n",
      "batch 5912: loss 0.024771\n",
      "batch 5913: loss 0.048488\n",
      "batch 5914: loss 0.073258\n",
      "batch 5915: loss 0.039970\n",
      "batch 5916: loss 0.049572\n",
      "batch 5917: loss 0.033495\n",
      "batch 5918: loss 0.004533\n",
      "batch 5919: loss 0.015076\n",
      "batch 5920: loss 0.078605\n",
      "batch 5921: loss 0.013939\n",
      "batch 5922: loss 0.054214\n",
      "batch 5923: loss 0.019028\n",
      "batch 5924: loss 0.015363\n",
      "batch 5925: loss 0.016822\n",
      "batch 5926: loss 0.069266\n",
      "batch 5927: loss 0.025207\n",
      "batch 5928: loss 0.025251\n",
      "batch 5929: loss 0.236687\n",
      "batch 5930: loss 0.015386\n",
      "batch 5931: loss 0.010578\n",
      "batch 5932: loss 0.023421\n",
      "batch 5933: loss 0.009625\n",
      "batch 5934: loss 0.046505\n",
      "batch 5935: loss 0.024619\n",
      "batch 5936: loss 0.034317\n",
      "batch 5937: loss 0.096592\n",
      "batch 5938: loss 0.065550\n",
      "batch 5939: loss 0.077917\n",
      "batch 5940: loss 0.118376\n",
      "batch 5941: loss 0.059953\n",
      "batch 5942: loss 0.106300\n",
      "batch 5943: loss 0.041464\n",
      "batch 5944: loss 0.024408\n",
      "batch 5945: loss 0.008771\n",
      "batch 5946: loss 0.016680\n",
      "batch 5947: loss 0.151035\n",
      "batch 5948: loss 0.138939\n",
      "batch 5949: loss 0.061709\n",
      "batch 5950: loss 0.088284\n",
      "batch 5951: loss 0.022011\n",
      "batch 5952: loss 0.039137\n",
      "batch 5953: loss 0.124319\n",
      "batch 5954: loss 0.079929\n",
      "batch 5955: loss 0.056782\n",
      "batch 5956: loss 0.028682\n",
      "batch 5957: loss 0.041593\n",
      "batch 5958: loss 0.005499\n",
      "batch 5959: loss 0.014898\n",
      "batch 5960: loss 0.056422\n",
      "batch 5961: loss 0.015607\n",
      "batch 5962: loss 0.085012\n",
      "batch 5963: loss 0.035939\n",
      "batch 5964: loss 0.086011\n",
      "batch 5965: loss 0.005982\n",
      "batch 5966: loss 0.039395\n",
      "batch 5967: loss 0.057786\n",
      "batch 5968: loss 0.027464\n",
      "batch 5969: loss 0.019206\n",
      "batch 5970: loss 0.153646\n",
      "batch 5971: loss 0.034442\n",
      "batch 5972: loss 0.011177\n",
      "batch 5973: loss 0.027551\n",
      "batch 5974: loss 0.037720\n",
      "batch 5975: loss 0.010605\n",
      "batch 5976: loss 0.069280\n",
      "batch 5977: loss 0.052245\n",
      "batch 5978: loss 0.008847\n",
      "batch 5979: loss 0.036710\n",
      "batch 5980: loss 0.103466\n",
      "batch 5981: loss 0.004402\n",
      "batch 5982: loss 0.135139\n",
      "batch 5983: loss 0.058798\n",
      "batch 5984: loss 0.066972\n",
      "batch 5985: loss 0.064944\n",
      "batch 5986: loss 0.017226\n",
      "batch 5987: loss 0.024041\n",
      "batch 5988: loss 0.019944\n",
      "batch 5989: loss 0.033105\n",
      "batch 5990: loss 0.011027\n",
      "batch 5991: loss 0.033253\n",
      "batch 5992: loss 0.107244\n",
      "batch 5993: loss 0.033065\n",
      "batch 5994: loss 0.008994\n",
      "batch 5995: loss 0.048364\n",
      "batch 5996: loss 0.032806\n",
      "batch 5997: loss 0.053558\n",
      "batch 5998: loss 0.024356\n",
      "batch 5999: loss 0.107096\n",
      "test accuracy: 0.973000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1L2(0.01,0.05))\n",
    "#         self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "#        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L2(0.01))\n",
    "\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout(TensorFlow版本)\n",
    "類似負重訓練的效果，在training時將部分neuron移除，test時再補回來!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.456346\n",
      "batch 1: loss 2.320688\n",
      "batch 2: loss 2.200954\n",
      "batch 3: loss 2.174162\n",
      "batch 4: loss 2.087512\n",
      "batch 5: loss 2.018888\n",
      "batch 6: loss 1.932891\n",
      "batch 7: loss 1.857764\n",
      "batch 8: loss 1.706555\n",
      "batch 9: loss 1.727158\n",
      "batch 10: loss 1.660357\n",
      "batch 11: loss 1.626040\n",
      "batch 12: loss 1.563582\n",
      "batch 13: loss 1.467975\n",
      "batch 14: loss 1.393619\n",
      "batch 15: loss 1.441103\n",
      "batch 16: loss 1.382273\n",
      "batch 17: loss 1.405621\n",
      "batch 18: loss 1.331125\n",
      "batch 19: loss 1.237661\n",
      "batch 20: loss 1.198734\n",
      "batch 21: loss 1.167284\n",
      "batch 22: loss 1.263037\n",
      "batch 23: loss 1.148620\n",
      "batch 24: loss 1.137599\n",
      "batch 25: loss 1.130811\n",
      "batch 26: loss 1.054866\n",
      "batch 27: loss 1.044715\n",
      "batch 28: loss 1.105528\n",
      "batch 29: loss 1.040915\n",
      "batch 30: loss 1.029762\n",
      "batch 31: loss 1.098210\n",
      "batch 32: loss 0.927978\n",
      "batch 33: loss 1.096447\n",
      "batch 34: loss 0.885126\n",
      "batch 35: loss 1.023054\n",
      "batch 36: loss 0.890171\n",
      "batch 37: loss 1.030002\n",
      "batch 38: loss 0.970649\n",
      "batch 39: loss 0.881382\n",
      "batch 40: loss 0.911616\n",
      "batch 41: loss 0.937124\n",
      "batch 42: loss 0.923900\n",
      "batch 43: loss 0.920669\n",
      "batch 44: loss 0.922779\n",
      "batch 45: loss 0.845668\n",
      "batch 46: loss 0.909483\n",
      "batch 47: loss 0.931679\n",
      "batch 48: loss 0.956809\n",
      "batch 49: loss 0.905209\n",
      "batch 50: loss 0.946668\n",
      "batch 51: loss 0.781925\n",
      "batch 52: loss 0.775657\n",
      "batch 53: loss 0.853281\n",
      "batch 54: loss 0.889986\n",
      "batch 55: loss 0.778264\n",
      "batch 56: loss 0.752644\n",
      "batch 57: loss 0.816924\n",
      "batch 58: loss 0.962931\n",
      "batch 59: loss 0.926298\n",
      "batch 60: loss 0.785760\n",
      "batch 61: loss 0.828692\n",
      "batch 62: loss 0.852342\n",
      "batch 63: loss 0.894498\n",
      "batch 64: loss 0.855582\n",
      "batch 65: loss 0.835797\n",
      "batch 66: loss 0.867731\n",
      "batch 67: loss 0.724202\n",
      "batch 68: loss 0.693663\n",
      "batch 69: loss 0.851293\n",
      "batch 70: loss 0.654872\n",
      "batch 71: loss 0.744852\n",
      "batch 72: loss 0.787590\n",
      "batch 73: loss 0.727724\n",
      "batch 74: loss 0.778332\n",
      "batch 75: loss 0.725743\n",
      "batch 76: loss 0.668168\n",
      "batch 77: loss 0.728658\n",
      "batch 78: loss 0.744723\n",
      "batch 79: loss 0.699564\n",
      "batch 80: loss 0.707020\n",
      "batch 81: loss 0.764840\n",
      "batch 82: loss 0.768681\n",
      "batch 83: loss 0.767315\n",
      "batch 84: loss 0.780606\n",
      "batch 85: loss 0.780894\n",
      "batch 86: loss 0.577164\n",
      "batch 87: loss 0.690546\n",
      "batch 88: loss 0.710287\n",
      "batch 89: loss 0.677192\n",
      "batch 90: loss 0.718425\n",
      "batch 91: loss 0.785516\n",
      "batch 92: loss 0.730615\n",
      "batch 93: loss 0.841635\n",
      "batch 94: loss 0.632712\n",
      "batch 95: loss 0.622777\n",
      "batch 96: loss 0.736930\n",
      "batch 97: loss 0.717465\n",
      "batch 98: loss 0.740791\n",
      "batch 99: loss 0.626099\n",
      "batch 100: loss 0.691671\n",
      "batch 101: loss 0.689354\n",
      "batch 102: loss 0.710761\n",
      "batch 103: loss 0.775951\n",
      "batch 104: loss 0.608717\n",
      "batch 105: loss 0.642021\n",
      "batch 106: loss 0.743395\n",
      "batch 107: loss 0.652798\n",
      "batch 108: loss 0.767762\n",
      "batch 109: loss 0.630849\n",
      "batch 110: loss 0.667596\n",
      "batch 111: loss 0.765444\n",
      "batch 112: loss 0.629167\n",
      "batch 113: loss 0.764856\n",
      "batch 114: loss 0.692600\n",
      "batch 115: loss 0.619584\n",
      "batch 116: loss 0.638201\n",
      "batch 117: loss 0.621692\n",
      "batch 118: loss 0.743173\n",
      "batch 119: loss 0.566340\n",
      "batch 120: loss 0.586329\n",
      "batch 121: loss 0.619675\n",
      "batch 122: loss 0.654832\n",
      "batch 123: loss 0.579846\n",
      "batch 124: loss 0.601791\n",
      "batch 125: loss 0.676230\n",
      "batch 126: loss 0.618898\n",
      "batch 127: loss 0.695704\n",
      "batch 128: loss 0.645460\n",
      "batch 129: loss 0.574893\n",
      "batch 130: loss 0.589489\n",
      "batch 131: loss 0.564645\n",
      "batch 132: loss 0.608008\n",
      "batch 133: loss 0.525730\n",
      "batch 134: loss 0.536931\n",
      "batch 135: loss 0.560744\n",
      "batch 136: loss 0.685490\n",
      "batch 137: loss 0.605067\n",
      "batch 138: loss 0.633551\n",
      "batch 139: loss 0.659834\n",
      "batch 140: loss 0.564636\n",
      "batch 141: loss 0.575687\n",
      "batch 142: loss 0.616922\n",
      "batch 143: loss 0.540905\n",
      "batch 144: loss 0.653339\n",
      "batch 145: loss 0.615583\n",
      "batch 146: loss 0.631024\n",
      "batch 147: loss 0.607596\n",
      "batch 148: loss 0.652801\n",
      "batch 149: loss 0.663253\n",
      "batch 150: loss 0.632022\n",
      "batch 151: loss 0.591979\n",
      "batch 152: loss 0.594675\n",
      "batch 153: loss 0.660354\n",
      "batch 154: loss 0.695901\n",
      "batch 155: loss 0.621883\n",
      "batch 156: loss 0.569445\n",
      "batch 157: loss 0.648864\n",
      "batch 158: loss 0.625551\n",
      "batch 159: loss 0.553812\n",
      "batch 160: loss 0.655557\n",
      "batch 161: loss 0.634651\n",
      "batch 162: loss 0.602094\n",
      "batch 163: loss 0.594886\n",
      "batch 164: loss 0.738722\n",
      "batch 165: loss 0.578068\n",
      "batch 166: loss 0.649010\n",
      "batch 167: loss 0.468836\n",
      "batch 168: loss 0.742326\n",
      "batch 169: loss 0.581334\n",
      "batch 170: loss 0.534521\n",
      "batch 171: loss 0.577955\n",
      "batch 172: loss 0.663758\n",
      "batch 173: loss 0.538451\n",
      "batch 174: loss 0.521338\n",
      "batch 175: loss 0.585807\n",
      "batch 176: loss 0.567371\n",
      "batch 177: loss 0.620420\n",
      "batch 178: loss 0.616954\n",
      "batch 179: loss 0.502600\n",
      "batch 180: loss 0.590922\n",
      "batch 181: loss 0.624981\n",
      "batch 182: loss 0.564327\n",
      "batch 183: loss 0.592301\n",
      "batch 184: loss 0.550224\n",
      "batch 185: loss 0.615125\n",
      "batch 186: loss 0.730189\n",
      "batch 187: loss 0.528860\n",
      "batch 188: loss 0.505282\n",
      "batch 189: loss 0.571574\n",
      "batch 190: loss 0.602564\n",
      "batch 191: loss 0.650895\n",
      "batch 192: loss 0.586809\n",
      "batch 193: loss 0.523873\n",
      "batch 194: loss 0.562414\n",
      "batch 195: loss 0.636772\n",
      "batch 196: loss 0.586153\n",
      "batch 197: loss 0.551849\n",
      "batch 198: loss 0.600511\n",
      "batch 199: loss 0.529167\n",
      "batch 200: loss 0.691762\n",
      "batch 201: loss 0.588848\n",
      "batch 202: loss 0.647473\n",
      "batch 203: loss 0.552159\n",
      "batch 204: loss 0.483571\n",
      "batch 205: loss 0.610975\n",
      "batch 206: loss 0.517246\n",
      "batch 207: loss 0.618116\n",
      "batch 208: loss 0.531052\n",
      "batch 209: loss 0.558087\n",
      "batch 210: loss 0.624547\n",
      "batch 211: loss 0.582232\n",
      "batch 212: loss 0.585633\n",
      "batch 213: loss 0.600117\n",
      "batch 214: loss 0.522293\n",
      "batch 215: loss 0.564654\n",
      "batch 216: loss 0.604247\n",
      "batch 217: loss 0.589080\n",
      "batch 218: loss 0.595612\n",
      "batch 219: loss 0.570421\n",
      "batch 220: loss 0.601969\n",
      "batch 221: loss 0.595366\n",
      "batch 222: loss 0.577881\n",
      "batch 223: loss 0.514397\n",
      "batch 224: loss 0.605708\n",
      "batch 225: loss 0.518314\n",
      "batch 226: loss 0.497267\n",
      "batch 227: loss 0.577950\n",
      "batch 228: loss 0.578161\n",
      "batch 229: loss 0.544250\n",
      "batch 230: loss 0.553564\n",
      "batch 231: loss 0.567016\n",
      "batch 232: loss 0.560737\n",
      "batch 233: loss 0.486696\n",
      "batch 234: loss 0.522358\n",
      "batch 235: loss 0.549106\n",
      "batch 236: loss 0.537391\n",
      "batch 237: loss 0.636645\n",
      "batch 238: loss 0.529772\n",
      "batch 239: loss 0.629233\n",
      "batch 240: loss 0.550917\n",
      "batch 241: loss 0.510345\n",
      "batch 242: loss 0.432057\n",
      "batch 243: loss 0.559646\n",
      "batch 244: loss 0.509800\n",
      "batch 245: loss 0.514846\n",
      "batch 246: loss 0.425310\n",
      "batch 247: loss 0.481789\n",
      "batch 248: loss 0.583861\n",
      "batch 249: loss 0.610466\n",
      "batch 250: loss 0.499815\n",
      "batch 251: loss 0.521737\n",
      "batch 252: loss 0.461261\n",
      "batch 253: loss 0.534243\n",
      "batch 254: loss 0.501026\n",
      "batch 255: loss 0.547791\n",
      "batch 256: loss 0.666536\n",
      "batch 257: loss 0.488673\n",
      "batch 258: loss 0.579386\n",
      "batch 259: loss 0.523478\n",
      "batch 260: loss 0.547198\n",
      "batch 261: loss 0.577658\n",
      "batch 262: loss 0.542107\n",
      "batch 263: loss 0.533451\n",
      "batch 264: loss 0.530134\n",
      "batch 265: loss 0.488219\n",
      "batch 266: loss 0.549255\n",
      "batch 267: loss 0.508812\n",
      "batch 268: loss 0.568969\n",
      "batch 269: loss 0.582696\n",
      "batch 270: loss 0.559806\n",
      "batch 271: loss 0.455920\n",
      "batch 272: loss 0.667127\n",
      "batch 273: loss 0.556062\n",
      "batch 274: loss 0.615315\n",
      "batch 275: loss 0.545353\n",
      "batch 276: loss 0.557662\n",
      "batch 277: loss 0.471951\n",
      "batch 278: loss 0.646362\n",
      "batch 279: loss 0.585418\n",
      "batch 280: loss 0.580256\n",
      "batch 281: loss 0.552352\n",
      "batch 282: loss 0.560963\n",
      "batch 283: loss 0.598009\n",
      "batch 284: loss 0.589110\n",
      "batch 285: loss 0.507771\n",
      "batch 286: loss 0.511845\n",
      "batch 287: loss 0.444610\n",
      "batch 288: loss 0.543970\n",
      "batch 289: loss 0.544492\n",
      "batch 290: loss 0.494403\n",
      "batch 291: loss 0.471722\n",
      "batch 292: loss 0.524478\n",
      "batch 293: loss 0.510703\n",
      "batch 294: loss 0.566465\n",
      "batch 295: loss 0.607104\n",
      "batch 296: loss 0.465609\n",
      "batch 297: loss 0.516126\n",
      "batch 298: loss 0.489413\n",
      "batch 299: loss 0.446328\n",
      "batch 300: loss 0.498002\n",
      "batch 301: loss 0.621035\n",
      "batch 302: loss 0.494623\n",
      "batch 303: loss 0.635104\n",
      "batch 304: loss 0.568767\n",
      "batch 305: loss 0.493687\n",
      "batch 306: loss 0.514293\n",
      "batch 307: loss 0.565210\n",
      "batch 308: loss 0.458641\n",
      "batch 309: loss 0.526484\n",
      "batch 310: loss 0.529604\n",
      "batch 311: loss 0.535496\n",
      "batch 312: loss 0.502847\n",
      "batch 313: loss 0.615979\n",
      "batch 314: loss 0.522190\n",
      "batch 315: loss 0.560373\n",
      "batch 316: loss 0.528952\n",
      "batch 317: loss 0.449318\n",
      "batch 318: loss 0.536758\n",
      "batch 319: loss 0.481091\n",
      "batch 320: loss 0.438869\n",
      "batch 321: loss 0.454843\n",
      "batch 322: loss 0.479491\n",
      "batch 323: loss 0.525120\n",
      "batch 324: loss 0.539429\n",
      "batch 325: loss 0.509221\n",
      "batch 326: loss 0.530711\n",
      "batch 327: loss 0.558891\n",
      "batch 328: loss 0.563053\n",
      "batch 329: loss 0.564076\n",
      "batch 330: loss 0.468448\n",
      "batch 331: loss 0.469562\n",
      "batch 332: loss 0.534971\n",
      "batch 333: loss 0.537781\n",
      "batch 334: loss 0.451684\n",
      "batch 335: loss 0.449756\n",
      "batch 336: loss 0.574305\n",
      "batch 337: loss 0.448022\n",
      "batch 338: loss 0.466370\n",
      "batch 339: loss 0.443010\n",
      "batch 340: loss 0.450337\n",
      "batch 341: loss 0.438076\n",
      "batch 342: loss 0.428375\n",
      "batch 343: loss 0.522186\n",
      "batch 344: loss 0.473058\n",
      "batch 345: loss 0.482614\n",
      "batch 346: loss 0.476621\n",
      "batch 347: loss 0.521961\n",
      "batch 348: loss 0.537064\n",
      "batch 349: loss 0.511948\n",
      "batch 350: loss 0.472326\n",
      "batch 351: loss 0.460474\n",
      "batch 352: loss 0.479705\n",
      "batch 353: loss 0.506733\n",
      "batch 354: loss 0.499580\n",
      "batch 355: loss 0.412020\n",
      "batch 356: loss 0.514617\n",
      "batch 357: loss 0.581447\n",
      "batch 358: loss 0.558811\n",
      "batch 359: loss 0.527297\n",
      "batch 360: loss 0.434913\n",
      "batch 361: loss 0.447248\n",
      "batch 362: loss 0.476501\n",
      "batch 363: loss 0.511964\n",
      "batch 364: loss 0.443060\n",
      "batch 365: loss 0.557655\n",
      "batch 366: loss 0.427473\n",
      "batch 367: loss 0.530787\n",
      "batch 368: loss 0.501558\n",
      "batch 369: loss 0.527092\n",
      "batch 370: loss 0.547750\n",
      "batch 371: loss 0.474522\n",
      "batch 372: loss 0.583439\n",
      "batch 373: loss 0.354248\n",
      "batch 374: loss 0.497245\n",
      "batch 375: loss 0.527218\n",
      "batch 376: loss 0.508488\n",
      "batch 377: loss 0.411346\n",
      "batch 378: loss 0.510897\n",
      "batch 379: loss 0.518163\n",
      "batch 380: loss 0.402116\n",
      "batch 381: loss 0.537534\n",
      "batch 382: loss 0.410878\n",
      "batch 383: loss 0.540385\n",
      "batch 384: loss 0.557661\n",
      "batch 385: loss 0.444472\n",
      "batch 386: loss 0.556341\n",
      "batch 387: loss 0.588483\n",
      "batch 388: loss 0.479848\n",
      "batch 389: loss 0.408498\n",
      "batch 390: loss 0.396661\n",
      "batch 391: loss 0.565481\n",
      "batch 392: loss 0.522547\n",
      "batch 393: loss 0.346074\n",
      "batch 394: loss 0.481584\n",
      "batch 395: loss 0.459943\n",
      "batch 396: loss 0.399130\n",
      "batch 397: loss 0.482155\n",
      "batch 398: loss 0.397839\n",
      "batch 399: loss 0.459603\n",
      "batch 400: loss 0.559281\n",
      "batch 401: loss 0.493656\n",
      "batch 402: loss 0.480987\n",
      "batch 403: loss 0.462220\n",
      "batch 404: loss 0.434327\n",
      "batch 405: loss 0.483680\n",
      "batch 406: loss 0.490069\n",
      "batch 407: loss 0.373316\n",
      "batch 408: loss 0.481317\n",
      "batch 409: loss 0.528841\n",
      "batch 410: loss 0.461366\n",
      "batch 411: loss 0.577159\n",
      "batch 412: loss 0.392122\n",
      "batch 413: loss 0.494716\n",
      "batch 414: loss 0.452073\n",
      "batch 415: loss 0.464502\n",
      "batch 416: loss 0.510322\n",
      "batch 417: loss 0.464473\n",
      "batch 418: loss 0.404795\n",
      "batch 419: loss 0.500164\n",
      "batch 420: loss 0.471038\n",
      "batch 421: loss 0.492968\n",
      "batch 422: loss 0.451490\n",
      "batch 423: loss 0.452030\n",
      "batch 424: loss 0.500953\n",
      "batch 425: loss 0.493552\n",
      "batch 426: loss 0.407115\n",
      "batch 427: loss 0.501680\n",
      "batch 428: loss 0.405547\n",
      "batch 429: loss 0.463372\n",
      "batch 430: loss 0.539890\n",
      "batch 431: loss 0.456681\n",
      "batch 432: loss 0.459208\n",
      "batch 433: loss 0.518201\n",
      "batch 434: loss 0.504097\n",
      "batch 435: loss 0.484878\n",
      "batch 436: loss 0.447601\n",
      "batch 437: loss 0.503078\n",
      "batch 438: loss 0.500354\n",
      "batch 439: loss 0.414253\n",
      "batch 440: loss 0.451195\n",
      "batch 441: loss 0.472269\n",
      "batch 442: loss 0.475980\n",
      "batch 443: loss 0.420286\n",
      "batch 444: loss 0.498636\n",
      "batch 445: loss 0.471507\n",
      "batch 446: loss 0.458445\n",
      "batch 447: loss 0.524858\n",
      "batch 448: loss 0.592479\n",
      "batch 449: loss 0.460836\n",
      "batch 450: loss 0.462416\n",
      "batch 451: loss 0.489777\n",
      "batch 452: loss 0.564804\n",
      "batch 453: loss 0.443273\n",
      "batch 454: loss 0.417313\n",
      "batch 455: loss 0.460314\n",
      "batch 456: loss 0.515769\n",
      "batch 457: loss 0.505507\n",
      "batch 458: loss 0.447549\n",
      "batch 459: loss 0.471015\n",
      "batch 460: loss 0.531259\n",
      "batch 461: loss 0.359289\n",
      "batch 462: loss 0.481444\n",
      "batch 463: loss 0.543578\n",
      "batch 464: loss 0.533042\n",
      "batch 465: loss 0.415601\n",
      "batch 466: loss 0.422334\n",
      "batch 467: loss 0.477892\n",
      "batch 468: loss 0.411872\n",
      "batch 469: loss 0.424916\n",
      "batch 470: loss 0.416179\n",
      "batch 471: loss 0.437295\n",
      "batch 472: loss 0.407772\n",
      "batch 473: loss 0.404905\n",
      "batch 474: loss 0.420080\n",
      "batch 475: loss 0.510067\n",
      "batch 476: loss 0.457308\n",
      "batch 477: loss 0.483395\n",
      "batch 478: loss 0.446538\n",
      "batch 479: loss 0.527727\n",
      "batch 480: loss 0.469915\n",
      "batch 481: loss 0.426657\n",
      "batch 482: loss 0.423935\n",
      "batch 483: loss 0.459244\n",
      "batch 484: loss 0.459085\n",
      "batch 485: loss 0.588928\n",
      "batch 486: loss 0.418828\n",
      "batch 487: loss 0.525812\n",
      "batch 488: loss 0.457583\n",
      "batch 489: loss 0.463832\n",
      "batch 490: loss 0.400322\n",
      "batch 491: loss 0.579892\n",
      "batch 492: loss 0.455178\n",
      "batch 493: loss 0.419722\n",
      "batch 494: loss 0.553877\n",
      "batch 495: loss 0.482509\n",
      "batch 496: loss 0.548820\n",
      "batch 497: loss 0.386320\n",
      "batch 498: loss 0.383802\n",
      "batch 499: loss 0.539737\n",
      "batch 500: loss 0.476290\n",
      "batch 501: loss 0.482277\n",
      "batch 502: loss 0.476612\n",
      "batch 503: loss 0.473680\n",
      "batch 504: loss 0.481343\n",
      "batch 505: loss 0.441667\n",
      "batch 506: loss 0.456090\n",
      "batch 507: loss 0.455480\n",
      "batch 508: loss 0.451697\n",
      "batch 509: loss 0.467673\n",
      "batch 510: loss 0.476550\n",
      "batch 511: loss 0.445095\n",
      "batch 512: loss 0.541187\n",
      "batch 513: loss 0.526072\n",
      "batch 514: loss 0.369859\n",
      "batch 515: loss 0.422558\n",
      "batch 516: loss 0.425407\n",
      "batch 517: loss 0.588268\n",
      "batch 518: loss 0.518519\n",
      "batch 519: loss 0.398060\n",
      "batch 520: loss 0.498962\n",
      "batch 521: loss 0.468072\n",
      "batch 522: loss 0.370891\n",
      "batch 523: loss 0.399435\n",
      "batch 524: loss 0.346187\n",
      "batch 525: loss 0.562094\n",
      "batch 526: loss 0.474637\n",
      "batch 527: loss 0.588161\n",
      "batch 528: loss 0.457333\n",
      "batch 529: loss 0.507490\n",
      "batch 530: loss 0.484244\n",
      "batch 531: loss 0.372606\n",
      "batch 532: loss 0.465050\n",
      "batch 533: loss 0.396366\n",
      "batch 534: loss 0.428212\n",
      "batch 535: loss 0.393067\n",
      "batch 536: loss 0.392802\n",
      "batch 537: loss 0.416848\n",
      "batch 538: loss 0.380043\n",
      "batch 539: loss 0.518812\n",
      "batch 540: loss 0.434410\n",
      "batch 541: loss 0.375361\n",
      "batch 542: loss 0.419407\n",
      "batch 543: loss 0.383046\n",
      "batch 544: loss 0.408501\n",
      "batch 545: loss 0.463180\n",
      "batch 546: loss 0.437181\n",
      "batch 547: loss 0.434872\n",
      "batch 548: loss 0.380418\n",
      "batch 549: loss 0.516837\n",
      "batch 550: loss 0.335990\n",
      "batch 551: loss 0.420347\n",
      "batch 552: loss 0.384760\n",
      "batch 553: loss 0.375045\n",
      "batch 554: loss 0.365651\n",
      "batch 555: loss 0.490182\n",
      "batch 556: loss 0.382474\n",
      "batch 557: loss 0.413233\n",
      "batch 558: loss 0.342494\n",
      "batch 559: loss 0.384056\n",
      "batch 560: loss 0.364143\n",
      "batch 561: loss 0.357984\n",
      "batch 562: loss 0.390832\n",
      "batch 563: loss 0.511612\n",
      "batch 564: loss 0.362766\n",
      "batch 565: loss 0.371527\n",
      "batch 566: loss 0.451921\n",
      "batch 567: loss 0.376321\n",
      "batch 568: loss 0.409918\n",
      "batch 569: loss 0.414685\n",
      "batch 570: loss 0.526316\n",
      "batch 571: loss 0.381564\n",
      "batch 572: loss 0.418997\n",
      "batch 573: loss 0.528011\n",
      "batch 574: loss 0.385280\n",
      "batch 575: loss 0.425184\n",
      "batch 576: loss 0.374542\n",
      "batch 577: loss 0.397404\n",
      "batch 578: loss 0.467083\n",
      "batch 579: loss 0.374328\n",
      "batch 580: loss 0.496490\n",
      "batch 581: loss 0.358967\n",
      "batch 582: loss 0.381759\n",
      "batch 583: loss 0.459119\n",
      "batch 584: loss 0.426276\n",
      "batch 585: loss 0.554488\n",
      "batch 586: loss 0.423989\n",
      "batch 587: loss 0.462394\n",
      "batch 588: loss 0.442021\n",
      "batch 589: loss 0.437579\n",
      "batch 590: loss 0.442460\n",
      "batch 591: loss 0.412133\n",
      "batch 592: loss 0.442278\n",
      "batch 593: loss 0.389939\n",
      "batch 594: loss 0.489614\n",
      "batch 595: loss 0.471041\n",
      "batch 596: loss 0.399051\n",
      "batch 597: loss 0.330580\n",
      "batch 598: loss 0.355828\n",
      "batch 599: loss 0.458259\n",
      "batch 600: loss 0.415879\n",
      "batch 601: loss 0.359942\n",
      "batch 602: loss 0.393345\n",
      "batch 603: loss 0.423140\n",
      "batch 604: loss 0.507853\n",
      "batch 605: loss 0.395241\n",
      "batch 606: loss 0.293130\n",
      "batch 607: loss 0.484649\n",
      "batch 608: loss 0.474606\n",
      "batch 609: loss 0.455993\n",
      "batch 610: loss 0.447466\n",
      "batch 611: loss 0.368992\n",
      "batch 612: loss 0.413094\n",
      "batch 613: loss 0.425952\n",
      "batch 614: loss 0.368573\n",
      "batch 615: loss 0.408733\n",
      "batch 616: loss 0.421419\n",
      "batch 617: loss 0.387953\n",
      "batch 618: loss 0.429082\n",
      "batch 619: loss 0.448070\n",
      "batch 620: loss 0.394300\n",
      "batch 621: loss 0.372317\n",
      "batch 622: loss 0.545317\n",
      "batch 623: loss 0.457593\n",
      "batch 624: loss 0.437876\n",
      "batch 625: loss 0.444134\n",
      "batch 626: loss 0.459693\n",
      "batch 627: loss 0.428278\n",
      "batch 628: loss 0.380063\n",
      "batch 629: loss 0.399067\n",
      "batch 630: loss 0.411457\n",
      "batch 631: loss 0.427480\n",
      "batch 632: loss 0.373902\n",
      "batch 633: loss 0.468926\n",
      "batch 634: loss 0.468463\n",
      "batch 635: loss 0.355292\n",
      "batch 636: loss 0.497378\n",
      "batch 637: loss 0.441736\n",
      "batch 638: loss 0.389671\n",
      "batch 639: loss 0.423833\n",
      "batch 640: loss 0.424476\n",
      "batch 641: loss 0.411903\n",
      "batch 642: loss 0.420947\n",
      "batch 643: loss 0.374934\n",
      "batch 644: loss 0.419438\n",
      "batch 645: loss 0.420231\n",
      "batch 646: loss 0.502137\n",
      "batch 647: loss 0.427389\n",
      "batch 648: loss 0.502705\n",
      "batch 649: loss 0.484294\n",
      "batch 650: loss 0.360108\n",
      "batch 651: loss 0.447762\n",
      "batch 652: loss 0.435890\n",
      "batch 653: loss 0.474021\n",
      "batch 654: loss 0.318121\n",
      "batch 655: loss 0.433997\n",
      "batch 656: loss 0.424356\n",
      "batch 657: loss 0.507929\n",
      "batch 658: loss 0.383667\n",
      "batch 659: loss 0.365444\n",
      "batch 660: loss 0.410087\n",
      "batch 661: loss 0.410155\n",
      "batch 662: loss 0.418633\n",
      "batch 663: loss 0.414686\n",
      "batch 664: loss 0.447849\n",
      "batch 665: loss 0.409984\n",
      "batch 666: loss 0.462835\n",
      "batch 667: loss 0.424180\n",
      "batch 668: loss 0.436654\n",
      "batch 669: loss 0.370070\n",
      "batch 670: loss 0.372974\n",
      "batch 671: loss 0.461759\n",
      "batch 672: loss 0.368862\n",
      "batch 673: loss 0.355813\n",
      "batch 674: loss 0.445597\n",
      "batch 675: loss 0.478820\n",
      "batch 676: loss 0.420201\n",
      "batch 677: loss 0.329874\n",
      "batch 678: loss 0.498306\n",
      "batch 679: loss 0.378552\n",
      "batch 680: loss 0.386485\n",
      "batch 681: loss 0.483615\n",
      "batch 682: loss 0.421162\n",
      "batch 683: loss 0.441086\n",
      "batch 684: loss 0.378389\n",
      "batch 685: loss 0.373597\n",
      "batch 686: loss 0.357807\n",
      "batch 687: loss 0.444515\n",
      "batch 688: loss 0.452011\n",
      "batch 689: loss 0.464490\n",
      "batch 690: loss 0.387269\n",
      "batch 691: loss 0.319971\n",
      "batch 692: loss 0.430970\n",
      "batch 693: loss 0.479422\n",
      "batch 694: loss 0.446063\n",
      "batch 695: loss 0.386044\n",
      "batch 696: loss 0.461664\n",
      "batch 697: loss 0.378976\n",
      "batch 698: loss 0.480002\n",
      "batch 699: loss 0.399900\n",
      "batch 700: loss 0.399094\n",
      "batch 701: loss 0.375998\n",
      "batch 702: loss 0.350960\n",
      "batch 703: loss 0.423216\n",
      "batch 704: loss 0.373005\n",
      "batch 705: loss 0.519505\n",
      "batch 706: loss 0.384480\n",
      "batch 707: loss 0.374677\n",
      "batch 708: loss 0.417779\n",
      "batch 709: loss 0.353897\n",
      "batch 710: loss 0.347148\n",
      "batch 711: loss 0.414929\n",
      "batch 712: loss 0.438565\n",
      "batch 713: loss 0.430140\n",
      "batch 714: loss 0.453846\n",
      "batch 715: loss 0.420432\n",
      "batch 716: loss 0.309282\n",
      "batch 717: loss 0.390530\n",
      "batch 718: loss 0.426536\n",
      "batch 719: loss 0.423802\n",
      "batch 720: loss 0.419403\n",
      "batch 721: loss 0.421525\n",
      "batch 722: loss 0.477294\n",
      "batch 723: loss 0.428095\n",
      "batch 724: loss 0.485844\n",
      "batch 725: loss 0.480922\n",
      "batch 726: loss 0.354379\n",
      "batch 727: loss 0.319615\n",
      "batch 728: loss 0.389754\n",
      "batch 729: loss 0.426893\n",
      "batch 730: loss 0.492419\n",
      "batch 731: loss 0.493529\n",
      "batch 732: loss 0.536340\n",
      "batch 733: loss 0.510166\n",
      "batch 734: loss 0.374238\n",
      "batch 735: loss 0.452421\n",
      "batch 736: loss 0.349323\n",
      "batch 737: loss 0.370977\n",
      "batch 738: loss 0.421568\n",
      "batch 739: loss 0.369154\n",
      "batch 740: loss 0.413744\n",
      "batch 741: loss 0.460091\n",
      "batch 742: loss 0.374852\n",
      "batch 743: loss 0.434283\n",
      "batch 744: loss 0.341560\n",
      "batch 745: loss 0.358036\n",
      "batch 746: loss 0.407112\n",
      "batch 747: loss 0.331136\n",
      "batch 748: loss 0.387159\n",
      "batch 749: loss 0.396000\n",
      "batch 750: loss 0.364042\n",
      "batch 751: loss 0.482224\n",
      "batch 752: loss 0.345969\n",
      "batch 753: loss 0.428370\n",
      "batch 754: loss 0.347250\n",
      "batch 755: loss 0.404112\n",
      "batch 756: loss 0.422398\n",
      "batch 757: loss 0.423124\n",
      "batch 758: loss 0.380117\n",
      "batch 759: loss 0.488399\n",
      "batch 760: loss 0.405523\n",
      "batch 761: loss 0.398431\n",
      "batch 762: loss 0.379096\n",
      "batch 763: loss 0.421143\n",
      "batch 764: loss 0.312014\n",
      "batch 765: loss 0.417940\n",
      "batch 766: loss 0.420670\n",
      "batch 767: loss 0.412471\n",
      "batch 768: loss 0.331957\n",
      "batch 769: loss 0.387544\n",
      "batch 770: loss 0.429115\n",
      "batch 771: loss 0.367082\n",
      "batch 772: loss 0.496043\n",
      "batch 773: loss 0.483982\n",
      "batch 774: loss 0.415079\n",
      "batch 775: loss 0.528576\n",
      "batch 776: loss 0.392977\n",
      "batch 777: loss 0.395657\n",
      "batch 778: loss 0.386698\n",
      "batch 779: loss 0.377763\n",
      "batch 780: loss 0.331998\n",
      "batch 781: loss 0.344913\n",
      "batch 782: loss 0.563151\n",
      "batch 783: loss 0.433899\n",
      "batch 784: loss 0.510139\n",
      "batch 785: loss 0.540390\n",
      "batch 786: loss 0.365408\n",
      "batch 787: loss 0.453594\n",
      "batch 788: loss 0.435196\n",
      "batch 789: loss 0.371684\n",
      "batch 790: loss 0.399839\n",
      "batch 791: loss 0.379955\n",
      "batch 792: loss 0.439983\n",
      "batch 793: loss 0.359089\n",
      "batch 794: loss 0.410357\n",
      "batch 795: loss 0.417896\n",
      "batch 796: loss 0.414295\n",
      "batch 797: loss 0.387171\n",
      "batch 798: loss 0.348122\n",
      "batch 799: loss 0.307934\n",
      "batch 800: loss 0.468669\n",
      "batch 801: loss 0.361451\n",
      "batch 802: loss 0.483335\n",
      "batch 803: loss 0.450048\n",
      "batch 804: loss 0.494861\n",
      "batch 805: loss 0.337861\n",
      "batch 806: loss 0.259416\n",
      "batch 807: loss 0.410947\n",
      "batch 808: loss 0.438815\n",
      "batch 809: loss 0.319198\n",
      "batch 810: loss 0.415606\n",
      "batch 811: loss 0.339896\n",
      "batch 812: loss 0.358597\n",
      "batch 813: loss 0.434911\n",
      "batch 814: loss 0.438210\n",
      "batch 815: loss 0.360709\n",
      "batch 816: loss 0.376053\n",
      "batch 817: loss 0.363105\n",
      "batch 818: loss 0.557402\n",
      "batch 819: loss 0.402126\n",
      "batch 820: loss 0.385605\n",
      "batch 821: loss 0.291761\n",
      "batch 822: loss 0.402133\n",
      "batch 823: loss 0.440956\n",
      "batch 824: loss 0.429777\n",
      "batch 825: loss 0.471514\n",
      "batch 826: loss 0.355047\n",
      "batch 827: loss 0.356513\n",
      "batch 828: loss 0.363833\n",
      "batch 829: loss 0.367609\n",
      "batch 830: loss 0.459585\n",
      "batch 831: loss 0.512899\n",
      "batch 832: loss 0.444572\n",
      "batch 833: loss 0.467258\n",
      "batch 834: loss 0.399010\n",
      "batch 835: loss 0.443150\n",
      "batch 836: loss 0.489431\n",
      "batch 837: loss 0.448922\n",
      "batch 838: loss 0.375397\n",
      "batch 839: loss 0.400367\n",
      "batch 840: loss 0.399291\n",
      "batch 841: loss 0.450492\n",
      "batch 842: loss 0.343900\n",
      "batch 843: loss 0.394671\n",
      "batch 844: loss 0.437602\n",
      "batch 845: loss 0.427627\n",
      "batch 846: loss 0.357014\n",
      "batch 847: loss 0.334423\n",
      "batch 848: loss 0.381331\n",
      "batch 849: loss 0.472904\n",
      "batch 850: loss 0.356540\n",
      "batch 851: loss 0.444601\n",
      "batch 852: loss 0.448120\n",
      "batch 853: loss 0.469024\n",
      "batch 854: loss 0.408265\n",
      "batch 855: loss 0.444014\n",
      "batch 856: loss 0.369146\n",
      "batch 857: loss 0.452678\n",
      "batch 858: loss 0.449562\n",
      "batch 859: loss 0.357578\n",
      "batch 860: loss 0.373160\n",
      "batch 861: loss 0.344880\n",
      "batch 862: loss 0.329591\n",
      "batch 863: loss 0.367222\n",
      "batch 864: loss 0.373915\n",
      "batch 865: loss 0.455155\n",
      "batch 866: loss 0.374749\n",
      "batch 867: loss 0.400696\n",
      "batch 868: loss 0.387721\n",
      "batch 869: loss 0.414549\n",
      "batch 870: loss 0.366708\n",
      "batch 871: loss 0.425530\n",
      "batch 872: loss 0.388474\n",
      "batch 873: loss 0.419411\n",
      "batch 874: loss 0.403941\n",
      "batch 875: loss 0.437822\n",
      "batch 876: loss 0.410271\n",
      "batch 877: loss 0.429961\n",
      "batch 878: loss 0.412800\n",
      "batch 879: loss 0.401616\n",
      "batch 880: loss 0.392760\n",
      "batch 881: loss 0.390913\n",
      "batch 882: loss 0.325808\n",
      "batch 883: loss 0.380257\n",
      "batch 884: loss 0.381020\n",
      "batch 885: loss 0.372810\n",
      "batch 886: loss 0.376542\n",
      "batch 887: loss 0.363668\n",
      "batch 888: loss 0.333402\n",
      "batch 889: loss 0.296077\n",
      "batch 890: loss 0.333959\n",
      "batch 891: loss 0.385626\n",
      "batch 892: loss 0.465533\n",
      "batch 893: loss 0.412649\n",
      "batch 894: loss 0.369923\n",
      "batch 895: loss 0.376675\n",
      "batch 896: loss 0.337279\n",
      "batch 897: loss 0.497618\n",
      "batch 898: loss 0.353568\n",
      "batch 899: loss 0.420774\n",
      "batch 900: loss 0.370467\n",
      "batch 901: loss 0.418202\n",
      "batch 902: loss 0.393922\n",
      "batch 903: loss 0.442706\n",
      "batch 904: loss 0.454743\n",
      "batch 905: loss 0.442765\n",
      "batch 906: loss 0.327240\n",
      "batch 907: loss 0.375152\n",
      "batch 908: loss 0.420599\n",
      "batch 909: loss 0.333338\n",
      "batch 910: loss 0.331710\n",
      "batch 911: loss 0.342025\n",
      "batch 912: loss 0.372619\n",
      "batch 913: loss 0.371872\n",
      "batch 914: loss 0.370996\n",
      "batch 915: loss 0.322896\n",
      "batch 916: loss 0.349475\n",
      "batch 917: loss 0.358451\n",
      "batch 918: loss 0.375329\n",
      "batch 919: loss 0.374306\n",
      "batch 920: loss 0.380711\n",
      "batch 921: loss 0.356043\n",
      "batch 922: loss 0.332036\n",
      "batch 923: loss 0.379731\n",
      "batch 924: loss 0.423052\n",
      "batch 925: loss 0.407966\n",
      "batch 926: loss 0.432823\n",
      "batch 927: loss 0.399770\n",
      "batch 928: loss 0.375176\n",
      "batch 929: loss 0.358383\n",
      "batch 930: loss 0.479479\n",
      "batch 931: loss 0.418293\n",
      "batch 932: loss 0.351594\n",
      "batch 933: loss 0.298568\n",
      "batch 934: loss 0.522197\n",
      "batch 935: loss 0.395583\n",
      "batch 936: loss 0.451418\n",
      "batch 937: loss 0.415350\n",
      "batch 938: loss 0.386025\n",
      "batch 939: loss 0.443399\n",
      "batch 940: loss 0.429751\n",
      "batch 941: loss 0.408809\n",
      "batch 942: loss 0.386886\n",
      "batch 943: loss 0.351788\n",
      "batch 944: loss 0.369345\n",
      "batch 945: loss 0.401058\n",
      "batch 946: loss 0.373113\n",
      "batch 947: loss 0.501530\n",
      "batch 948: loss 0.437702\n",
      "batch 949: loss 0.302353\n",
      "batch 950: loss 0.410536\n",
      "batch 951: loss 0.424006\n",
      "batch 952: loss 0.333366\n",
      "batch 953: loss 0.305630\n",
      "batch 954: loss 0.396984\n",
      "batch 955: loss 0.389989\n",
      "batch 956: loss 0.279432\n",
      "batch 957: loss 0.409634\n",
      "batch 958: loss 0.427914\n",
      "batch 959: loss 0.397471\n",
      "batch 960: loss 0.358935\n",
      "batch 961: loss 0.360312\n",
      "batch 962: loss 0.352277\n",
      "batch 963: loss 0.396921\n",
      "batch 964: loss 0.358005\n",
      "batch 965: loss 0.424106\n",
      "batch 966: loss 0.334745\n",
      "batch 967: loss 0.459688\n",
      "batch 968: loss 0.318708\n",
      "batch 969: loss 0.359994\n",
      "batch 970: loss 0.411913\n",
      "batch 971: loss 0.241719\n",
      "batch 972: loss 0.345679\n",
      "batch 973: loss 0.379239\n",
      "batch 974: loss 0.491813\n",
      "batch 975: loss 0.351390\n",
      "batch 976: loss 0.350862\n",
      "batch 977: loss 0.422096\n",
      "batch 978: loss 0.346977\n",
      "batch 979: loss 0.382514\n",
      "batch 980: loss 0.326126\n",
      "batch 981: loss 0.365399\n",
      "batch 982: loss 0.408785\n",
      "batch 983: loss 0.287197\n",
      "batch 984: loss 0.406886\n",
      "batch 985: loss 0.449107\n",
      "batch 986: loss 0.401990\n",
      "batch 987: loss 0.338309\n",
      "batch 988: loss 0.358561\n",
      "batch 989: loss 0.487675\n",
      "batch 990: loss 0.323261\n",
      "batch 991: loss 0.374182\n",
      "batch 992: loss 0.382819\n",
      "batch 993: loss 0.448237\n",
      "batch 994: loss 0.329569\n",
      "batch 995: loss 0.327480\n",
      "batch 996: loss 0.354624\n",
      "batch 997: loss 0.272289\n",
      "batch 998: loss 0.408216\n",
      "batch 999: loss 0.378531\n",
      "batch 1000: loss 0.465687\n",
      "batch 1001: loss 0.379283\n",
      "batch 1002: loss 0.401343\n",
      "batch 1003: loss 0.413048\n",
      "batch 1004: loss 0.414614\n",
      "batch 1005: loss 0.422961\n",
      "batch 1006: loss 0.363677\n",
      "batch 1007: loss 0.353511\n",
      "batch 1008: loss 0.326022\n",
      "batch 1009: loss 0.312212\n",
      "batch 1010: loss 0.483233\n",
      "batch 1011: loss 0.321043\n",
      "batch 1012: loss 0.410730\n",
      "batch 1013: loss 0.357190\n",
      "batch 1014: loss 0.349900\n",
      "batch 1015: loss 0.318086\n",
      "batch 1016: loss 0.336077\n",
      "batch 1017: loss 0.305767\n",
      "batch 1018: loss 0.355375\n",
      "batch 1019: loss 0.336203\n",
      "batch 1020: loss 0.425437\n",
      "batch 1021: loss 0.354776\n",
      "batch 1022: loss 0.441719\n",
      "batch 1023: loss 0.372939\n",
      "batch 1024: loss 0.349206\n",
      "batch 1025: loss 0.382374\n",
      "batch 1026: loss 0.304824\n",
      "batch 1027: loss 0.375975\n",
      "batch 1028: loss 0.385314\n",
      "batch 1029: loss 0.360932\n",
      "batch 1030: loss 0.443948\n",
      "batch 1031: loss 0.406505\n",
      "batch 1032: loss 0.487936\n",
      "batch 1033: loss 0.376652\n",
      "batch 1034: loss 0.420948\n",
      "batch 1035: loss 0.332367\n",
      "batch 1036: loss 0.360747\n",
      "batch 1037: loss 0.426460\n",
      "batch 1038: loss 0.368658\n",
      "batch 1039: loss 0.372814\n",
      "batch 1040: loss 0.381737\n",
      "batch 1041: loss 0.325137\n",
      "batch 1042: loss 0.385878\n",
      "batch 1043: loss 0.392120\n",
      "batch 1044: loss 0.449165\n",
      "batch 1045: loss 0.360461\n",
      "batch 1046: loss 0.361190\n",
      "batch 1047: loss 0.357179\n",
      "batch 1048: loss 0.463008\n",
      "batch 1049: loss 0.335534\n",
      "batch 1050: loss 0.386920\n",
      "batch 1051: loss 0.487479\n",
      "batch 1052: loss 0.361160\n",
      "batch 1053: loss 0.367090\n",
      "batch 1054: loss 0.356841\n",
      "batch 1055: loss 0.344634\n",
      "batch 1056: loss 0.304351\n",
      "batch 1057: loss 0.390325\n",
      "batch 1058: loss 0.373456\n",
      "batch 1059: loss 0.414895\n",
      "batch 1060: loss 0.394456\n",
      "batch 1061: loss 0.331605\n",
      "batch 1062: loss 0.306652\n",
      "batch 1063: loss 0.370065\n",
      "batch 1064: loss 0.322130\n",
      "batch 1065: loss 0.353024\n",
      "batch 1066: loss 0.318122\n",
      "batch 1067: loss 0.357322\n",
      "batch 1068: loss 0.497803\n",
      "batch 1069: loss 0.348140\n",
      "batch 1070: loss 0.354900\n",
      "batch 1071: loss 0.420855\n",
      "batch 1072: loss 0.374382\n",
      "batch 1073: loss 0.355590\n",
      "batch 1074: loss 0.305587\n",
      "batch 1075: loss 0.324859\n",
      "batch 1076: loss 0.378344\n",
      "batch 1077: loss 0.363589\n",
      "batch 1078: loss 0.434152\n",
      "batch 1079: loss 0.414562\n",
      "batch 1080: loss 0.293676\n",
      "batch 1081: loss 0.319348\n",
      "batch 1082: loss 0.369219\n",
      "batch 1083: loss 0.323119\n",
      "batch 1084: loss 0.307996\n",
      "batch 1085: loss 0.401843\n",
      "batch 1086: loss 0.328320\n",
      "batch 1087: loss 0.427517\n",
      "batch 1088: loss 0.320922\n",
      "batch 1089: loss 0.431335\n",
      "batch 1090: loss 0.338237\n",
      "batch 1091: loss 0.403696\n",
      "batch 1092: loss 0.454532\n",
      "batch 1093: loss 0.414839\n",
      "batch 1094: loss 0.386283\n",
      "batch 1095: loss 0.393223\n",
      "batch 1096: loss 0.376036\n",
      "batch 1097: loss 0.352666\n",
      "batch 1098: loss 0.344507\n",
      "batch 1099: loss 0.398824\n",
      "batch 1100: loss 0.452075\n",
      "batch 1101: loss 0.362432\n",
      "batch 1102: loss 0.379974\n",
      "batch 1103: loss 0.384566\n",
      "batch 1104: loss 0.336423\n",
      "batch 1105: loss 0.403162\n",
      "batch 1106: loss 0.280575\n",
      "batch 1107: loss 0.422125\n",
      "batch 1108: loss 0.407267\n",
      "batch 1109: loss 0.411510\n",
      "batch 1110: loss 0.271618\n",
      "batch 1111: loss 0.392288\n",
      "batch 1112: loss 0.346203\n",
      "batch 1113: loss 0.468101\n",
      "batch 1114: loss 0.419217\n",
      "batch 1115: loss 0.431610\n",
      "batch 1116: loss 0.329530\n",
      "batch 1117: loss 0.426347\n",
      "batch 1118: loss 0.347285\n",
      "batch 1119: loss 0.339960\n",
      "batch 1120: loss 0.354281\n",
      "batch 1121: loss 0.347219\n",
      "batch 1122: loss 0.337885\n",
      "batch 1123: loss 0.350536\n",
      "batch 1124: loss 0.344441\n",
      "batch 1125: loss 0.369295\n",
      "batch 1126: loss 0.286635\n",
      "batch 1127: loss 0.344328\n",
      "batch 1128: loss 0.309252\n",
      "batch 1129: loss 0.315876\n",
      "batch 1130: loss 0.350247\n",
      "batch 1131: loss 0.287567\n",
      "batch 1132: loss 0.301099\n",
      "batch 1133: loss 0.346748\n",
      "batch 1134: loss 0.400190\n",
      "batch 1135: loss 0.434892\n",
      "batch 1136: loss 0.304913\n",
      "batch 1137: loss 0.390725\n",
      "batch 1138: loss 0.390892\n",
      "batch 1139: loss 0.362938\n",
      "batch 1140: loss 0.471579\n",
      "batch 1141: loss 0.365059\n",
      "batch 1142: loss 0.300319\n",
      "batch 1143: loss 0.426374\n",
      "batch 1144: loss 0.368747\n",
      "batch 1145: loss 0.393119\n",
      "batch 1146: loss 0.264269\n",
      "batch 1147: loss 0.295756\n",
      "batch 1148: loss 0.467305\n",
      "batch 1149: loss 0.372652\n",
      "batch 1150: loss 0.364119\n",
      "batch 1151: loss 0.394745\n",
      "batch 1152: loss 0.402475\n",
      "batch 1153: loss 0.354408\n",
      "batch 1154: loss 0.361304\n",
      "batch 1155: loss 0.345097\n",
      "batch 1156: loss 0.442731\n",
      "batch 1157: loss 0.315383\n",
      "batch 1158: loss 0.329179\n",
      "batch 1159: loss 0.356727\n",
      "batch 1160: loss 0.316443\n",
      "batch 1161: loss 0.357626\n",
      "batch 1162: loss 0.397927\n",
      "batch 1163: loss 0.367211\n",
      "batch 1164: loss 0.329462\n",
      "batch 1165: loss 0.316903\n",
      "batch 1166: loss 0.323720\n",
      "batch 1167: loss 0.449393\n",
      "batch 1168: loss 0.336645\n",
      "batch 1169: loss 0.449301\n",
      "test accuracy: 0.961939\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten层将除第一维（batch_size）以外的维度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu, kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.dropout_layer = tf.keras.layers.Dropout(rate=0.2)\n",
    "\n",
    "    def call(self, inputs, training=None):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dropout_layer(x, training=training)\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.dropout_layer(x, training=training)\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "# model.compile\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X, training=True) # 這裡一定要加上 training = true代表訓練預測時使用負重訓練\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    \n",
    "    \n",
    "\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout(Keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 59,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 0s - loss: 14.1817 - accuracy: 0.3163 - val_loss: 9.7888 - val_accuracy: 0.4633\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 8.7625 - accuracy: 0.5245 - val_loss: 7.5016 - val_accuracy: 0.6594\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 6.6568 - accuracy: 0.6931 - val_loss: 5.6912 - val_accuracy: 0.8068\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 5.0692 - accuracy: 0.8050 - val_loss: 4.3518 - val_accuracy: 0.8623\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 3.8899 - accuracy: 0.8499 - val_loss: 3.3255 - val_accuracy: 0.8871\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 3.0141 - accuracy: 0.8778 - val_loss: 2.6012 - val_accuracy: 0.8973\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 2.3885 - accuracy: 0.8901 - val_loss: 2.0736 - val_accuracy: 0.9047\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 1.9300 - accuracy: 0.8965 - val_loss: 1.6778 - val_accuracy: 0.9108\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 1.5942 - accuracy: 0.8996 - val_loss: 1.3969 - val_accuracy: 0.9160\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 1.3507 - accuracy: 0.9006 - val_loss: 1.1873 - val_accuracy: 0.9177\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 1.1610 - accuracy: 0.9067 - val_loss: 1.0232 - val_accuracy: 0.9194\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 1.0261 - accuracy: 0.9053 - val_loss: 0.9001 - val_accuracy: 0.9224\n",
      "Epoch 13/100\n",
      "165/165 - 1s - loss: 0.9265 - accuracy: 0.9076 - val_loss: 0.8203 - val_accuracy: 0.9229\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.8451 - accuracy: 0.9106 - val_loss: 0.7562 - val_accuracy: 0.9226\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.7921 - accuracy: 0.9109 - val_loss: 0.7041 - val_accuracy: 0.9251\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.7452 - accuracy: 0.9116 - val_loss: 0.6655 - val_accuracy: 0.9271\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.7087 - accuracy: 0.9122 - val_loss: 0.6463 - val_accuracy: 0.9230\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.6897 - accuracy: 0.9131 - val_loss: 0.6134 - val_accuracy: 0.9278\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.6669 - accuracy: 0.9141 - val_loss: 0.6054 - val_accuracy: 0.9262\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.6586 - accuracy: 0.9139 - val_loss: 0.5865 - val_accuracy: 0.9278\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.6358 - accuracy: 0.9161 - val_loss: 0.5725 - val_accuracy: 0.9277\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.6308 - accuracy: 0.9152 - val_loss: 0.5648 - val_accuracy: 0.9284\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.6157 - accuracy: 0.9175 - val_loss: 0.5606 - val_accuracy: 0.9270\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.6105 - accuracy: 0.9155 - val_loss: 0.5491 - val_accuracy: 0.9287\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.5979 - accuracy: 0.9168 - val_loss: 0.5461 - val_accuracy: 0.9298\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.5948 - accuracy: 0.9171 - val_loss: 0.5411 - val_accuracy: 0.9294\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.5897 - accuracy: 0.9173 - val_loss: 0.5352 - val_accuracy: 0.9313\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.5829 - accuracy: 0.9187 - val_loss: 0.5293 - val_accuracy: 0.9303\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.5759 - accuracy: 0.9200 - val_loss: 0.5245 - val_accuracy: 0.9292\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.5671 - accuracy: 0.9209 - val_loss: 0.5227 - val_accuracy: 0.9298\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9213 - val_loss: 0.5199 - val_accuracy: 0.9303\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.5645 - accuracy: 0.9203 - val_loss: 0.5171 - val_accuracy: 0.9290\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.5604 - accuracy: 0.9198 - val_loss: 0.5185 - val_accuracy: 0.9292\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.5582 - accuracy: 0.9210 - val_loss: 0.5112 - val_accuracy: 0.9312\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.5570 - accuracy: 0.9205 - val_loss: 0.5140 - val_accuracy: 0.9309\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.5487 - accuracy: 0.9221 - val_loss: 0.5092 - val_accuracy: 0.9297\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.5438 - accuracy: 0.9223 - val_loss: 0.5055 - val_accuracy: 0.9309\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.5444 - accuracy: 0.9219 - val_loss: 0.5001 - val_accuracy: 0.9326\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.5390 - accuracy: 0.9240 - val_loss: 0.4971 - val_accuracy: 0.9332\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.5379 - accuracy: 0.9242 - val_loss: 0.4976 - val_accuracy: 0.9321\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.5374 - accuracy: 0.9237 - val_loss: 0.4995 - val_accuracy: 0.9306\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.5348 - accuracy: 0.9245 - val_loss: 0.4930 - val_accuracy: 0.9322\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.5343 - accuracy: 0.9242 - val_loss: 0.4905 - val_accuracy: 0.9332\n",
      "Epoch 44/100\n",
      "165/165 - 1s - loss: 0.5264 - accuracy: 0.9253 - val_loss: 0.4931 - val_accuracy: 0.9322\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.5281 - accuracy: 0.9244 - val_loss: 0.4850 - val_accuracy: 0.9335\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.5234 - accuracy: 0.9256 - val_loss: 0.4875 - val_accuracy: 0.9323\n",
      "Epoch 47/100\n",
      "165/165 - 1s - loss: 0.5276 - accuracy: 0.9245 - val_loss: 0.4881 - val_accuracy: 0.9334\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.5199 - accuracy: 0.9250 - val_loss: 0.4839 - val_accuracy: 0.9316\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.5149 - accuracy: 0.9263 - val_loss: 0.4797 - val_accuracy: 0.9343\n",
      "Epoch 50/100\n",
      "165/165 - 1s - loss: 0.5153 - accuracy: 0.9250 - val_loss: 0.4886 - val_accuracy: 0.9321\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.5123 - accuracy: 0.9273 - val_loss: 0.4800 - val_accuracy: 0.9329\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.5116 - accuracy: 0.9266 - val_loss: 0.4745 - val_accuracy: 0.9337\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.5094 - accuracy: 0.9259 - val_loss: 0.4779 - val_accuracy: 0.9329\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.5077 - accuracy: 0.9267 - val_loss: 0.4780 - val_accuracy: 0.9316\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.5067 - accuracy: 0.9287 - val_loss: 0.4776 - val_accuracy: 0.9331\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9278 - val_loss: 0.4728 - val_accuracy: 0.9337\n",
      "Epoch 57/100\n",
      "165/165 - 1s - loss: 0.5076 - accuracy: 0.9266 - val_loss: 0.4782 - val_accuracy: 0.9333\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.5035 - accuracy: 0.9289 - val_loss: 0.4702 - val_accuracy: 0.9336\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.4998 - accuracy: 0.9287 - val_loss: 0.4792 - val_accuracy: 0.9318\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.4975 - accuracy: 0.9294 - val_loss: 0.4742 - val_accuracy: 0.9326\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.5011 - accuracy: 0.9300 - val_loss: 0.4796 - val_accuracy: 0.9316\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.4969 - accuracy: 0.9286 - val_loss: 0.4752 - val_accuracy: 0.9314\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9288 - val_loss: 0.4718 - val_accuracy: 0.9326\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.4955 - accuracy: 0.9271 - val_loss: 0.4777 - val_accuracy: 0.9327\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.4984 - accuracy: 0.9280 - val_loss: 0.4702 - val_accuracy: 0.9345\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.4913 - accuracy: 0.9291 - val_loss: 0.4693 - val_accuracy: 0.9334\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.4889 - accuracy: 0.9300 - val_loss: 0.4777 - val_accuracy: 0.9301\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.4939 - accuracy: 0.9278 - val_loss: 0.4670 - val_accuracy: 0.9341\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.4850 - accuracy: 0.9293 - val_loss: 0.4667 - val_accuracy: 0.9334\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.4867 - accuracy: 0.9314 - val_loss: 0.4743 - val_accuracy: 0.9331\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.4890 - accuracy: 0.9289 - val_loss: 0.4623 - val_accuracy: 0.9336\n",
      "Epoch 72/100\n",
      "165/165 - 1s - loss: 0.4828 - accuracy: 0.9304 - val_loss: 0.4732 - val_accuracy: 0.9339\n",
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.4846 - accuracy: 0.9300 - val_loss: 0.4667 - val_accuracy: 0.9334\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.4848 - accuracy: 0.9302 - val_loss: 0.4707 - val_accuracy: 0.9324\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9306 - val_loss: 0.4600 - val_accuracy: 0.9340\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.4789 - accuracy: 0.9309 - val_loss: 0.4571 - val_accuracy: 0.9346\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.4783 - accuracy: 0.9298 - val_loss: 0.4627 - val_accuracy: 0.9338\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.4753 - accuracy: 0.9305 - val_loss: 0.4604 - val_accuracy: 0.9335\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.4816 - accuracy: 0.9292 - val_loss: 0.4573 - val_accuracy: 0.9334\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9320 - val_loss: 0.4720 - val_accuracy: 0.9319\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.4755 - accuracy: 0.9318 - val_loss: 0.4676 - val_accuracy: 0.9329\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.4739 - accuracy: 0.9315 - val_loss: 0.4653 - val_accuracy: 0.9318\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.4750 - accuracy: 0.9308 - val_loss: 0.4587 - val_accuracy: 0.9337\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.4709 - accuracy: 0.9312 - val_loss: 0.4622 - val_accuracy: 0.9342\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.4746 - accuracy: 0.9322 - val_loss: 0.4605 - val_accuracy: 0.9322\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9324 - val_loss: 0.4633 - val_accuracy: 0.9346\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.4698 - accuracy: 0.9318 - val_loss: 0.4550 - val_accuracy: 0.9338\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.4642 - accuracy: 0.9352 - val_loss: 0.4486 - val_accuracy: 0.9373\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.4582 - accuracy: 0.9360 - val_loss: 0.4470 - val_accuracy: 0.9382\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.4615 - accuracy: 0.9343 - val_loss: 0.4427 - val_accuracy: 0.9377\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.4569 - accuracy: 0.9354 - val_loss: 0.4481 - val_accuracy: 0.9366\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.4549 - accuracy: 0.9365 - val_loss: 0.4426 - val_accuracy: 0.9384\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.4567 - accuracy: 0.9356 - val_loss: 0.4424 - val_accuracy: 0.9369\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.4518 - accuracy: 0.9362 - val_loss: 0.4480 - val_accuracy: 0.9379\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.4573 - accuracy: 0.9364 - val_loss: 0.4424 - val_accuracy: 0.9392\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.4529 - accuracy: 0.9360 - val_loss: 0.4448 - val_accuracy: 0.9369\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.4499 - accuracy: 0.9362 - val_loss: 0.4388 - val_accuracy: 0.9394\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.4468 - accuracy: 0.9372 - val_loss: 0.4426 - val_accuracy: 0.9377\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.4513 - accuracy: 0.9356 - val_loss: 0.4489 - val_accuracy: 0.9382\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.4455 - accuracy: 0.9381 - val_loss: 0.4421 - val_accuracy: 0.9362\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(784,)),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L1(0.01)),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(0.01))\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['training', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 601us/step - loss: 0.4262 - accuracy: 0.9404\n",
      "[0.42615756392478943, 0.9404000043869019]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization(TensorFlow版本)\n",
    "透過將每一個batch的資料正規化來縮短模型訓練時間"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.395871\n",
      "batch 1: loss 2.302521\n",
      "batch 2: loss 2.223227\n",
      "batch 3: loss 2.127664\n",
      "batch 4: loss 2.082026\n",
      "batch 5: loss 2.010781\n",
      "batch 6: loss 1.953032\n",
      "batch 7: loss 1.892682\n",
      "batch 8: loss 1.830975\n",
      "batch 9: loss 1.719758\n",
      "batch 10: loss 1.677893\n",
      "batch 11: loss 1.581999\n",
      "batch 12: loss 1.517661\n",
      "batch 13: loss 1.482351\n",
      "batch 14: loss 1.403175\n",
      "batch 15: loss 1.358136\n",
      "batch 16: loss 1.222202\n",
      "batch 17: loss 1.243408\n",
      "batch 18: loss 1.194086\n",
      "batch 19: loss 1.110593\n",
      "batch 20: loss 1.021773\n",
      "batch 21: loss 0.942443\n",
      "batch 22: loss 0.911492\n",
      "batch 23: loss 0.880218\n",
      "batch 24: loss 0.804583\n",
      "batch 25: loss 0.768039\n",
      "batch 26: loss 0.785902\n",
      "batch 27: loss 0.724358\n",
      "batch 28: loss 0.699926\n",
      "batch 29: loss 0.649268\n",
      "batch 30: loss 0.610541\n",
      "batch 31: loss 0.590848\n",
      "batch 32: loss 0.602088\n",
      "batch 33: loss 0.553420\n",
      "batch 34: loss 0.570544\n",
      "batch 35: loss 0.587716\n",
      "batch 36: loss 0.547053\n",
      "batch 37: loss 0.570801\n",
      "batch 38: loss 0.541263\n",
      "batch 39: loss 0.543303\n",
      "batch 40: loss 0.511966\n",
      "batch 41: loss 0.480443\n",
      "batch 42: loss 0.459338\n",
      "batch 43: loss 0.456228\n",
      "batch 44: loss 0.493893\n",
      "batch 45: loss 0.478905\n",
      "batch 46: loss 0.407915\n",
      "batch 47: loss 0.460229\n",
      "batch 48: loss 0.452885\n",
      "batch 49: loss 0.443677\n",
      "batch 50: loss 0.431070\n",
      "batch 51: loss 0.402176\n",
      "batch 52: loss 0.460926\n",
      "batch 53: loss 0.427145\n",
      "batch 54: loss 0.420839\n",
      "batch 55: loss 0.423288\n",
      "batch 56: loss 0.392523\n",
      "batch 57: loss 0.375156\n",
      "batch 58: loss 0.393117\n",
      "batch 59: loss 0.384371\n",
      "batch 60: loss 0.371423\n",
      "batch 61: loss 0.398796\n",
      "batch 62: loss 0.386292\n",
      "batch 63: loss 0.396710\n",
      "batch 64: loss 0.386143\n",
      "batch 65: loss 0.417796\n",
      "batch 66: loss 0.344399\n",
      "batch 67: loss 0.384053\n",
      "batch 68: loss 0.348121\n",
      "batch 69: loss 0.373084\n",
      "batch 70: loss 0.407333\n",
      "batch 71: loss 0.354201\n",
      "batch 72: loss 0.349270\n",
      "batch 73: loss 0.325483\n",
      "batch 74: loss 0.350455\n",
      "batch 75: loss 0.383837\n",
      "batch 76: loss 0.322372\n",
      "batch 77: loss 0.345053\n",
      "batch 78: loss 0.333755\n",
      "batch 79: loss 0.300634\n",
      "batch 80: loss 0.364523\n",
      "batch 81: loss 0.301152\n",
      "batch 82: loss 0.373795\n",
      "batch 83: loss 0.307607\n",
      "batch 84: loss 0.336133\n",
      "batch 85: loss 0.321614\n",
      "batch 86: loss 0.354595\n",
      "batch 87: loss 0.371244\n",
      "batch 88: loss 0.340039\n",
      "batch 89: loss 0.326758\n",
      "batch 90: loss 0.319692\n",
      "batch 91: loss 0.318233\n",
      "batch 92: loss 0.290001\n",
      "batch 93: loss 0.287864\n",
      "batch 94: loss 0.297477\n",
      "batch 95: loss 0.318509\n",
      "batch 96: loss 0.328466\n",
      "batch 97: loss 0.315088\n",
      "batch 98: loss 0.301933\n",
      "batch 99: loss 0.322340\n",
      "batch 100: loss 0.294147\n",
      "batch 101: loss 0.329137\n",
      "batch 102: loss 0.318309\n",
      "batch 103: loss 0.295231\n",
      "batch 104: loss 0.301700\n",
      "batch 105: loss 0.335583\n",
      "batch 106: loss 0.309289\n",
      "batch 107: loss 0.338678\n",
      "batch 108: loss 0.334796\n",
      "batch 109: loss 0.315106\n",
      "batch 110: loss 0.306316\n",
      "batch 111: loss 0.319466\n",
      "batch 112: loss 0.282578\n",
      "batch 113: loss 0.307298\n",
      "batch 114: loss 0.304969\n",
      "batch 115: loss 0.293954\n",
      "batch 116: loss 0.311902\n",
      "batch 117: loss 0.290350\n",
      "batch 118: loss 0.265605\n",
      "batch 119: loss 0.273305\n",
      "batch 120: loss 0.277005\n",
      "batch 121: loss 0.303520\n",
      "batch 122: loss 0.291574\n",
      "batch 123: loss 0.269237\n",
      "batch 124: loss 0.267254\n",
      "batch 125: loss 0.262400\n",
      "batch 126: loss 0.295292\n",
      "batch 127: loss 0.305832\n",
      "batch 128: loss 0.316331\n",
      "batch 129: loss 0.254420\n",
      "batch 130: loss 0.274555\n",
      "batch 131: loss 0.270258\n",
      "batch 132: loss 0.301042\n",
      "batch 133: loss 0.290342\n",
      "batch 134: loss 0.287707\n",
      "batch 135: loss 0.305810\n",
      "batch 136: loss 0.283544\n",
      "batch 137: loss 0.261826\n",
      "batch 138: loss 0.307725\n",
      "batch 139: loss 0.285658\n",
      "batch 140: loss 0.303536\n",
      "batch 141: loss 0.278576\n",
      "batch 142: loss 0.278145\n",
      "batch 143: loss 0.299287\n",
      "batch 144: loss 0.260034\n",
      "batch 145: loss 0.277047\n",
      "batch 146: loss 0.313596\n",
      "batch 147: loss 0.262001\n",
      "batch 148: loss 0.243808\n",
      "batch 149: loss 0.290044\n",
      "batch 150: loss 0.285346\n",
      "batch 151: loss 0.234701\n",
      "batch 152: loss 0.252644\n",
      "batch 153: loss 0.248686\n",
      "batch 154: loss 0.283697\n",
      "batch 155: loss 0.322151\n",
      "batch 156: loss 0.246379\n",
      "batch 157: loss 0.271421\n",
      "batch 158: loss 0.275010\n",
      "batch 159: loss 0.224847\n",
      "batch 160: loss 0.274099\n",
      "batch 161: loss 0.236618\n",
      "batch 162: loss 0.289178\n",
      "batch 163: loss 0.309952\n",
      "batch 164: loss 0.274812\n",
      "batch 165: loss 0.276938\n",
      "batch 166: loss 0.271889\n",
      "batch 167: loss 0.277662\n",
      "batch 168: loss 0.255136\n",
      "batch 169: loss 0.254084\n",
      "batch 170: loss 0.308528\n",
      "batch 171: loss 0.291556\n",
      "batch 172: loss 0.269903\n",
      "batch 173: loss 0.238076\n",
      "batch 174: loss 0.252666\n",
      "batch 175: loss 0.245389\n",
      "batch 176: loss 0.240502\n",
      "batch 177: loss 0.201639\n",
      "batch 178: loss 0.242383\n",
      "batch 179: loss 0.263700\n",
      "batch 180: loss 0.262498\n",
      "batch 181: loss 0.236075\n",
      "batch 182: loss 0.235341\n",
      "batch 183: loss 0.230550\n",
      "batch 184: loss 0.226118\n",
      "batch 185: loss 0.279357\n",
      "batch 186: loss 0.246040\n",
      "batch 187: loss 0.251954\n",
      "batch 188: loss 0.249165\n",
      "batch 189: loss 0.249130\n",
      "batch 190: loss 0.296653\n",
      "batch 191: loss 0.262396\n",
      "batch 192: loss 0.211352\n",
      "batch 193: loss 0.245191\n",
      "batch 194: loss 0.252495\n",
      "batch 195: loss 0.240762\n",
      "batch 196: loss 0.271061\n",
      "batch 197: loss 0.256170\n",
      "batch 198: loss 0.270625\n",
      "batch 199: loss 0.262907\n",
      "batch 200: loss 0.245638\n",
      "batch 201: loss 0.260787\n",
      "batch 202: loss 0.242953\n",
      "batch 203: loss 0.281707\n",
      "batch 204: loss 0.246451\n",
      "batch 205: loss 0.217752\n",
      "batch 206: loss 0.236371\n",
      "batch 207: loss 0.278844\n",
      "batch 208: loss 0.220445\n",
      "batch 209: loss 0.256022\n",
      "batch 210: loss 0.238423\n",
      "batch 211: loss 0.255439\n",
      "batch 212: loss 0.241087\n",
      "batch 213: loss 0.285587\n",
      "batch 214: loss 0.230529\n",
      "batch 215: loss 0.204795\n",
      "batch 216: loss 0.259563\n",
      "batch 217: loss 0.266391\n",
      "batch 218: loss 0.228562\n",
      "batch 219: loss 0.265632\n",
      "batch 220: loss 0.246009\n",
      "batch 221: loss 0.212537\n",
      "batch 222: loss 0.238537\n",
      "batch 223: loss 0.267754\n",
      "batch 224: loss 0.216679\n",
      "batch 225: loss 0.283271\n",
      "batch 226: loss 0.247846\n",
      "batch 227: loss 0.204072\n",
      "batch 228: loss 0.229214\n",
      "batch 229: loss 0.204013\n",
      "batch 230: loss 0.249167\n",
      "batch 231: loss 0.266510\n",
      "batch 232: loss 0.235478\n",
      "batch 233: loss 0.227869\n",
      "batch 234: loss 0.217476\n",
      "batch 235: loss 0.229067\n",
      "batch 236: loss 0.254162\n",
      "batch 237: loss 0.235742\n",
      "batch 238: loss 0.236314\n",
      "batch 239: loss 0.246631\n",
      "batch 240: loss 0.214194\n",
      "batch 241: loss 0.223150\n",
      "batch 242: loss 0.233451\n",
      "batch 243: loss 0.244585\n",
      "batch 244: loss 0.193062\n",
      "batch 245: loss 0.215817\n",
      "batch 246: loss 0.217867\n",
      "batch 247: loss 0.202764\n",
      "batch 248: loss 0.265996\n",
      "batch 249: loss 0.212988\n",
      "batch 250: loss 0.234969\n",
      "batch 251: loss 0.241901\n",
      "batch 252: loss 0.271046\n",
      "batch 253: loss 0.230895\n",
      "batch 254: loss 0.275338\n",
      "batch 255: loss 0.288774\n",
      "batch 256: loss 0.222712\n",
      "batch 257: loss 0.201342\n",
      "batch 258: loss 0.233245\n",
      "batch 259: loss 0.221035\n",
      "batch 260: loss 0.222363\n",
      "batch 261: loss 0.187967\n",
      "batch 262: loss 0.242912\n",
      "batch 263: loss 0.220077\n",
      "batch 264: loss 0.236692\n",
      "batch 265: loss 0.233128\n",
      "batch 266: loss 0.227481\n",
      "batch 267: loss 0.170328\n",
      "batch 268: loss 0.210498\n",
      "batch 269: loss 0.182006\n",
      "batch 270: loss 0.234633\n",
      "batch 271: loss 0.232164\n",
      "batch 272: loss 0.188884\n",
      "batch 273: loss 0.181317\n",
      "batch 274: loss 0.233423\n",
      "batch 275: loss 0.169424\n",
      "batch 276: loss 0.234360\n",
      "batch 277: loss 0.194796\n",
      "batch 278: loss 0.211834\n",
      "batch 279: loss 0.214875\n",
      "batch 280: loss 0.169863\n",
      "batch 281: loss 0.249782\n",
      "batch 282: loss 0.194659\n",
      "batch 283: loss 0.233114\n",
      "batch 284: loss 0.241509\n",
      "batch 285: loss 0.202995\n",
      "batch 286: loss 0.209868\n",
      "batch 287: loss 0.221284\n",
      "batch 288: loss 0.220094\n",
      "batch 289: loss 0.196181\n",
      "batch 290: loss 0.226332\n",
      "batch 291: loss 0.183376\n",
      "batch 292: loss 0.237367\n",
      "batch 293: loss 0.191565\n",
      "batch 294: loss 0.181346\n",
      "batch 295: loss 0.222528\n",
      "batch 296: loss 0.219471\n",
      "batch 297: loss 0.206688\n",
      "batch 298: loss 0.198317\n",
      "batch 299: loss 0.192314\n",
      "test accuracy: 0.938400\n"
     ]
    }
   ],
   "source": [
    "# 增加訓練速度 (丟進激活函數之前先做Batch Normalization，簡稱BN層)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]\n",
    "    \n",
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.dense1 = tf.keras.layers.Dense(units=100)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac1 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "        \n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.ac2 = tf.keras.layers.LeakyReLU(alpha=0.2)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.bn1(x)\n",
    "        x = self.ac1(x)\n",
    "        \n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        x = self.bn2(x)\n",
    "        x = self.ac2(x)\n",
    "\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output\n",
    "\n",
    "num_epochs = 5\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.trainable_variables))\n",
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)   (60000,)\n",
      "(10000, 784)   (10000,)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,910\n",
      "Trainable params: 79,710\n",
      "Non-trainable params: 200\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "165/165 - 0s - loss: 0.4803 - accuracy: 0.8596 - val_loss: 0.2738 - val_accuracy: 0.9233\n",
      "Epoch 2/100\n",
      "165/165 - 0s - loss: 0.2487 - accuracy: 0.9308 - val_loss: 0.2292 - val_accuracy: 0.9365\n",
      "Epoch 3/100\n",
      "165/165 - 0s - loss: 0.2034 - accuracy: 0.9422 - val_loss: 0.2060 - val_accuracy: 0.9437\n",
      "Epoch 4/100\n",
      "165/165 - 0s - loss: 0.1736 - accuracy: 0.9520 - val_loss: 0.1930 - val_accuracy: 0.9462\n",
      "Epoch 5/100\n",
      "165/165 - 0s - loss: 0.1503 - accuracy: 0.9592 - val_loss: 0.1771 - val_accuracy: 0.9517\n",
      "Epoch 6/100\n",
      "165/165 - 0s - loss: 0.1330 - accuracy: 0.9640 - val_loss: 0.1665 - val_accuracy: 0.9521\n",
      "Epoch 7/100\n",
      "165/165 - 0s - loss: 0.1192 - accuracy: 0.9675 - val_loss: 0.1618 - val_accuracy: 0.9536\n",
      "Epoch 8/100\n",
      "165/165 - 0s - loss: 0.1060 - accuracy: 0.9709 - val_loss: 0.1590 - val_accuracy: 0.9556\n",
      "Epoch 9/100\n",
      "165/165 - 0s - loss: 0.0973 - accuracy: 0.9731 - val_loss: 0.1460 - val_accuracy: 0.9591\n",
      "Epoch 10/100\n",
      "165/165 - 0s - loss: 0.0867 - accuracy: 0.9762 - val_loss: 0.1449 - val_accuracy: 0.9585\n",
      "Epoch 11/100\n",
      "165/165 - 0s - loss: 0.0828 - accuracy: 0.9772 - val_loss: 0.1465 - val_accuracy: 0.9588\n",
      "Epoch 12/100\n",
      "165/165 - 0s - loss: 0.0735 - accuracy: 0.9802 - val_loss: 0.1387 - val_accuracy: 0.9607\n",
      "Epoch 13/100\n",
      "165/165 - 0s - loss: 0.0673 - accuracy: 0.9818 - val_loss: 0.1393 - val_accuracy: 0.9604\n",
      "Epoch 14/100\n",
      "165/165 - 0s - loss: 0.0581 - accuracy: 0.9855 - val_loss: 0.1380 - val_accuracy: 0.9618\n",
      "Epoch 15/100\n",
      "165/165 - 0s - loss: 0.0529 - accuracy: 0.9863 - val_loss: 0.1456 - val_accuracy: 0.9590\n",
      "Epoch 16/100\n",
      "165/165 - 0s - loss: 0.0479 - accuracy: 0.9884 - val_loss: 0.1312 - val_accuracy: 0.9632\n",
      "Epoch 17/100\n",
      "165/165 - 0s - loss: 0.0442 - accuracy: 0.9896 - val_loss: 0.1414 - val_accuracy: 0.9600\n",
      "Epoch 18/100\n",
      "165/165 - 0s - loss: 0.0423 - accuracy: 0.9901 - val_loss: 0.1320 - val_accuracy: 0.9637\n",
      "Epoch 19/100\n",
      "165/165 - 0s - loss: 0.0372 - accuracy: 0.9909 - val_loss: 0.1377 - val_accuracy: 0.9608\n",
      "Epoch 20/100\n",
      "165/165 - 0s - loss: 0.0373 - accuracy: 0.9910 - val_loss: 0.1379 - val_accuracy: 0.9623\n",
      "Epoch 21/100\n",
      "165/165 - 0s - loss: 0.0316 - accuracy: 0.9933 - val_loss: 0.1411 - val_accuracy: 0.9611\n",
      "Epoch 22/100\n",
      "165/165 - 0s - loss: 0.0395 - accuracy: 0.9892 - val_loss: 0.1370 - val_accuracy: 0.9637\n",
      "Epoch 23/100\n",
      "165/165 - 0s - loss: 0.0325 - accuracy: 0.9919 - val_loss: 0.1341 - val_accuracy: 0.9649\n",
      "Epoch 24/100\n",
      "165/165 - 0s - loss: 0.0291 - accuracy: 0.9931 - val_loss: 0.1337 - val_accuracy: 0.9637\n",
      "Epoch 25/100\n",
      "165/165 - 0s - loss: 0.0271 - accuracy: 0.9939 - val_loss: 0.1275 - val_accuracy: 0.9662\n",
      "Epoch 26/100\n",
      "165/165 - 0s - loss: 0.0202 - accuracy: 0.9965 - val_loss: 0.1295 - val_accuracy: 0.9644\n",
      "Epoch 27/100\n",
      "165/165 - 0s - loss: 0.0259 - accuracy: 0.9939 - val_loss: 0.1338 - val_accuracy: 0.9656\n",
      "Epoch 28/100\n",
      "165/165 - 0s - loss: 0.0321 - accuracy: 0.9917 - val_loss: 0.1338 - val_accuracy: 0.9651\n",
      "Epoch 29/100\n",
      "165/165 - 0s - loss: 0.0218 - accuracy: 0.9955 - val_loss: 0.1384 - val_accuracy: 0.9632\n",
      "Epoch 30/100\n",
      "165/165 - 0s - loss: 0.0209 - accuracy: 0.9956 - val_loss: 0.1287 - val_accuracy: 0.9663\n",
      "Epoch 31/100\n",
      "165/165 - 0s - loss: 0.0141 - accuracy: 0.9982 - val_loss: 0.1331 - val_accuracy: 0.9654\n",
      "Epoch 32/100\n",
      "165/165 - 0s - loss: 0.0160 - accuracy: 0.9972 - val_loss: 0.1338 - val_accuracy: 0.9651\n",
      "Epoch 33/100\n",
      "165/165 - 0s - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.1391 - val_accuracy: 0.9641\n",
      "Epoch 34/100\n",
      "165/165 - 0s - loss: 0.0227 - accuracy: 0.9938 - val_loss: 0.1390 - val_accuracy: 0.9651\n",
      "Epoch 35/100\n",
      "165/165 - 0s - loss: 0.0138 - accuracy: 0.9977 - val_loss: 0.1400 - val_accuracy: 0.9642\n",
      "Epoch 36/100\n",
      "165/165 - 0s - loss: 0.0104 - accuracy: 0.9989 - val_loss: 0.1328 - val_accuracy: 0.9677\n",
      "Epoch 37/100\n",
      "165/165 - 0s - loss: 0.0103 - accuracy: 0.9989 - val_loss: 0.1437 - val_accuracy: 0.9630\n",
      "Epoch 38/100\n",
      "165/165 - 0s - loss: 0.0131 - accuracy: 0.9977 - val_loss: 0.1386 - val_accuracy: 0.9667\n",
      "Epoch 39/100\n",
      "165/165 - 0s - loss: 0.0099 - accuracy: 0.9987 - val_loss: 0.1383 - val_accuracy: 0.9666\n",
      "Epoch 40/100\n",
      "165/165 - 0s - loss: 0.0091 - accuracy: 0.9989 - val_loss: 0.1468 - val_accuracy: 0.9634\n",
      "Epoch 41/100\n",
      "165/165 - 0s - loss: 0.0258 - accuracy: 0.9933 - val_loss: 0.1528 - val_accuracy: 0.9633\n",
      "Epoch 42/100\n",
      "165/165 - 0s - loss: 0.0155 - accuracy: 0.9961 - val_loss: 0.1447 - val_accuracy: 0.9652\n",
      "Epoch 43/100\n",
      "165/165 - 0s - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.1449 - val_accuracy: 0.9638\n",
      "Epoch 44/100\n",
      "165/165 - 0s - loss: 0.0136 - accuracy: 0.9967 - val_loss: 0.1420 - val_accuracy: 0.9662\n",
      "Epoch 45/100\n",
      "165/165 - 0s - loss: 0.0066 - accuracy: 0.9995 - val_loss: 0.1444 - val_accuracy: 0.9661\n",
      "Epoch 46/100\n",
      "165/165 - 0s - loss: 0.0173 - accuracy: 0.9953 - val_loss: 0.1617 - val_accuracy: 0.9617\n",
      "Epoch 47/100\n",
      "165/165 - 0s - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.1428 - val_accuracy: 0.9659\n",
      "Epoch 48/100\n",
      "165/165 - 0s - loss: 0.0107 - accuracy: 0.9979 - val_loss: 0.1449 - val_accuracy: 0.9664\n",
      "Epoch 49/100\n",
      "165/165 - 0s - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.1428 - val_accuracy: 0.9669\n",
      "Epoch 50/100\n",
      "165/165 - 0s - loss: 0.0107 - accuracy: 0.9982 - val_loss: 0.1438 - val_accuracy: 0.9656\n",
      "Epoch 51/100\n",
      "165/165 - 0s - loss: 0.0129 - accuracy: 0.9972 - val_loss: 0.1429 - val_accuracy: 0.9674\n",
      "Epoch 52/100\n",
      "165/165 - 0s - loss: 0.0055 - accuracy: 0.9995 - val_loss: 0.1444 - val_accuracy: 0.9670\n",
      "Epoch 53/100\n",
      "165/165 - 0s - loss: 0.0096 - accuracy: 0.9981 - val_loss: 0.1413 - val_accuracy: 0.9672\n",
      "Epoch 54/100\n",
      "165/165 - 0s - loss: 0.0047 - accuracy: 0.9997 - val_loss: 0.1505 - val_accuracy: 0.9660\n",
      "Epoch 55/100\n",
      "165/165 - 0s - loss: 0.0109 - accuracy: 0.9980 - val_loss: 0.1489 - val_accuracy: 0.9666\n",
      "Epoch 56/100\n",
      "165/165 - 0s - loss: 0.0045 - accuracy: 0.9997 - val_loss: 0.1441 - val_accuracy: 0.9679\n",
      "Epoch 57/100\n",
      "165/165 - 0s - loss: 0.0037 - accuracy: 0.9998 - val_loss: 0.1433 - val_accuracy: 0.9679\n",
      "Epoch 58/100\n",
      "165/165 - 0s - loss: 0.0032 - accuracy: 0.9999 - val_loss: 0.1445 - val_accuracy: 0.9667\n",
      "Epoch 59/100\n",
      "165/165 - 0s - loss: 0.0057 - accuracy: 0.9992 - val_loss: 0.1521 - val_accuracy: 0.9658\n",
      "Epoch 60/100\n",
      "165/165 - 0s - loss: 0.0166 - accuracy: 0.9953 - val_loss: 0.1578 - val_accuracy: 0.9636\n",
      "Epoch 61/100\n",
      "165/165 - 0s - loss: 0.0131 - accuracy: 0.9965 - val_loss: 0.1517 - val_accuracy: 0.9655\n",
      "Epoch 62/100\n",
      "165/165 - 0s - loss: 0.0047 - accuracy: 0.9994 - val_loss: 0.1471 - val_accuracy: 0.9683\n",
      "Epoch 63/100\n",
      "165/165 - 0s - loss: 0.0039 - accuracy: 0.9997 - val_loss: 0.1593 - val_accuracy: 0.9645\n",
      "Epoch 64/100\n",
      "165/165 - 0s - loss: 0.0165 - accuracy: 0.9956 - val_loss: 0.1483 - val_accuracy: 0.9666\n",
      "Epoch 65/100\n",
      "165/165 - 0s - loss: 0.0058 - accuracy: 0.9991 - val_loss: 0.1655 - val_accuracy: 0.9624\n",
      "Epoch 66/100\n",
      "165/165 - 0s - loss: 0.0131 - accuracy: 0.9963 - val_loss: 0.1517 - val_accuracy: 0.9667\n",
      "Epoch 67/100\n",
      "165/165 - 0s - loss: 0.0038 - accuracy: 0.9998 - val_loss: 0.1499 - val_accuracy: 0.9672\n",
      "Epoch 68/100\n",
      "165/165 - 0s - loss: 0.0029 - accuracy: 0.9999 - val_loss: 0.1511 - val_accuracy: 0.9677\n",
      "Epoch 69/100\n",
      "165/165 - 0s - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.1481 - val_accuracy: 0.9688\n",
      "Epoch 70/100\n",
      "165/165 - 0s - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.1501 - val_accuracy: 0.9680\n",
      "Epoch 71/100\n",
      "165/165 - 0s - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.1611 - val_accuracy: 0.9663\n",
      "Epoch 72/100\n",
      "165/165 - 0s - loss: 0.0130 - accuracy: 0.9963 - val_loss: 0.1614 - val_accuracy: 0.9659\n",
      "Epoch 73/100\n",
      "165/165 - 0s - loss: 0.0042 - accuracy: 0.9994 - val_loss: 0.1540 - val_accuracy: 0.9668\n",
      "Epoch 74/100\n",
      "165/165 - 0s - loss: 0.0051 - accuracy: 0.9992 - val_loss: 0.1560 - val_accuracy: 0.9673\n",
      "Epoch 75/100\n",
      "165/165 - 0s - loss: 0.0024 - accuracy: 0.9999 - val_loss: 0.1488 - val_accuracy: 0.9684\n",
      "Epoch 76/100\n",
      "165/165 - 0s - loss: 0.0102 - accuracy: 0.9973 - val_loss: 0.1588 - val_accuracy: 0.9677\n",
      "Epoch 77/100\n",
      "165/165 - 0s - loss: 0.0031 - accuracy: 0.9997 - val_loss: 0.1584 - val_accuracy: 0.9661\n",
      "Epoch 78/100\n",
      "165/165 - 0s - loss: 0.0113 - accuracy: 0.9971 - val_loss: 0.1660 - val_accuracy: 0.9649\n",
      "Epoch 79/100\n",
      "165/165 - 0s - loss: 0.0133 - accuracy: 0.9960 - val_loss: 0.1584 - val_accuracy: 0.9670\n",
      "Epoch 80/100\n",
      "165/165 - 0s - loss: 0.0080 - accuracy: 0.9981 - val_loss: 0.1550 - val_accuracy: 0.9672\n",
      "Epoch 81/100\n",
      "165/165 - 0s - loss: 0.0089 - accuracy: 0.9975 - val_loss: 0.1525 - val_accuracy: 0.9681\n",
      "Epoch 82/100\n",
      "165/165 - 0s - loss: 0.0025 - accuracy: 0.9998 - val_loss: 0.1539 - val_accuracy: 0.9687\n",
      "Epoch 83/100\n",
      "165/165 - 0s - loss: 0.0021 - accuracy: 0.9999 - val_loss: 0.1480 - val_accuracy: 0.9678\n",
      "Epoch 84/100\n",
      "165/165 - 0s - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.1489 - val_accuracy: 0.9693\n",
      "Epoch 85/100\n",
      "165/165 - 0s - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.1500 - val_accuracy: 0.9686\n",
      "Epoch 86/100\n",
      "165/165 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9672\n",
      "Epoch 87/100\n",
      "165/165 - 0s - loss: 0.0089 - accuracy: 0.9981 - val_loss: 0.1670 - val_accuracy: 0.9672\n",
      "Epoch 88/100\n",
      "165/165 - 0s - loss: 0.0026 - accuracy: 0.9998 - val_loss: 0.1561 - val_accuracy: 0.9686\n",
      "Epoch 89/100\n",
      "165/165 - 0s - loss: 0.0059 - accuracy: 0.9988 - val_loss: 0.1608 - val_accuracy: 0.9669\n",
      "Epoch 90/100\n",
      "165/165 - 0s - loss: 0.0029 - accuracy: 0.9997 - val_loss: 0.1560 - val_accuracy: 0.9681\n",
      "Epoch 91/100\n",
      "165/165 - 0s - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.1543 - val_accuracy: 0.9689\n",
      "Epoch 92/100\n",
      "165/165 - 0s - loss: 0.0017 - accuracy: 0.9999 - val_loss: 0.1587 - val_accuracy: 0.9683\n",
      "Epoch 93/100\n",
      "165/165 - 0s - loss: 0.0053 - accuracy: 0.9989 - val_loss: 0.1660 - val_accuracy: 0.9662\n",
      "Epoch 94/100\n",
      "165/165 - 0s - loss: 0.0020 - accuracy: 0.9998 - val_loss: 0.1630 - val_accuracy: 0.9666\n",
      "Epoch 95/100\n",
      "165/165 - 0s - loss: 0.0074 - accuracy: 0.9986 - val_loss: 0.1725 - val_accuracy: 0.9673\n",
      "Epoch 96/100\n",
      "165/165 - 0s - loss: 0.0107 - accuracy: 0.9969 - val_loss: 0.1694 - val_accuracy: 0.9656\n",
      "Epoch 97/100\n",
      "165/165 - 0s - loss: 0.0023 - accuracy: 0.9998 - val_loss: 0.1560 - val_accuracy: 0.9692\n",
      "Epoch 98/100\n",
      "165/165 - 0s - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1593 - val_accuracy: 0.9683\n",
      "Epoch 99/100\n",
      "165/165 - 0s - loss: 0.0062 - accuracy: 0.9984 - val_loss: 0.1690 - val_accuracy: 0.9670\n",
      "Epoch 100/100\n",
      "165/165 - 0s - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.1587 - val_accuracy: 0.9682\n",
      "313/313 [==============================] - 0s 395us/step - loss: 0.1250 - accuracy: 0.9717\n",
      "[0.12498364597558975, 0.9717000126838684]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape([x_train.shape[0], -1])\n",
    "x_test = x_test.reshape([x_test.shape[0], -1])\n",
    "print(x_train.shape, ' ', y_train.shape)\n",
    "print(x_test.shape, ' ', y_test.shape)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(100, input_shape=(784,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.LeakyReLU(),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "#keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "#keras.optimizers.Adam(learning_rate=0.01)\n",
    "#keras.optimizers.RMSprop(learning_rate=0.01)\n",
    "\n",
    "# provide labels as one_hot representation => tf.keras.losses.CategoricalCrossentropy\n",
    "# provide labels as integers => tf.keras.losses.SparseCategoricalCrossentropy \n",
    "model.compile(optimizer=keras.optimizers.Adam(),\n",
    "             loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "             metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=256, epochs=100, validation_split=0.3, verbose=2)\n",
    "\n",
    "result = model.evaluate(x_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
